<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>kerntools: R tools for kernel methods • kerntools</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../apple-touch-icon-60x60.png">
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.4.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.4.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="kerntools: R tools for kernel methods">
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">kerntools</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">1.0.2</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="active nav-item"><a class="nav-link" href="../articles/kerntools.html">Get started</a></li>
<li class="nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="nav-item"><a class="nav-link" href="../news/index.html">Changelog</a></li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json">
</form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/elies-ramon/kerntools/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="../logo.png" class="logo" alt=""><h1>kerntools: R tools for kernel methods</h1>
                        <h4 data-toc-skip class="author">Elies
Ramon</h4>
            
      
      <small class="dont-index">Source: <a href="https://github.com/elies-ramon/kerntools/blob/master/vignettes/kerntools.Rmd" class="external-link"><code>vignettes/kerntools.Rmd</code></a></small>
      <div class="d-none name"><code>kerntools.Rmd</code></div>
    </div>

    
    
<div class="section level2">
<h2 id="purpose">Purpose<a class="anchor" aria-label="anchor" href="#purpose"></a>
</h2>
<p><strong>kerntools</strong> provides R tools for working with a family
of Machine Learning methods called kernel methods. This package
implements several kernel functions for treating nonnegative and real
vectors, real matrices, categorical and ordinal variables, sets, and
strings. Several tools for studying the resulting kernel matrix or to
compare two kernel matrices are available. These diagnostic tools may be
used to infer the kernel(s) matrix(ces) suitability in model training.
<code>kerntools</code> also provides functions for extracting the
feature importance of Support Vector Machines (SVMs) or displaying
customizable kernel Principal Components Analysis (PCA) plots. For
convenience, widespread model’s performance measures and feature
importance visualization are available for the user.</p>
</div>
<div class="section level2">
<h2 id="loading">Loading<a class="anchor" aria-label="anchor" href="#loading"></a>
</h2>
<p>Once the package is installed, it can be loaded anytime typing:</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/elies-ramon/kerntools" class="external-link">kerntools</a></span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="package-overview">Package Overview<a class="anchor" aria-label="anchor" href="#package-overview"></a>
</h2>
<p>In this section several examples will be used to illustrate a typical
workflow of <code>kerntools</code>.</p>
<div class="section level3">
<h3 id="a-simple-example">A simple example<a class="anchor" aria-label="anchor" href="#a-simple-example"></a>
</h3>
<p>Let’s suppose we are working with the well known <a href="https://en.wikipedia.org/wiki/Iris_flower_data_set" class="external-link">iris
dataset</a>. This dataset contains sepal and petal measurements for
<em>N = 150</em> iris flowers. These flowers belong to three different
species: <em>Iris setosa</em>, <em>Iris versicolor</em>, and <em>Iris
virginica</em>. There are <em>D = 4</em> numeric variables (also called
“features”) that are measured: <code>Sepal.Length</code>,
<code>Sepal.Width</code>, <code>Petal.Length</code> and
<code>Petal.Width</code>.</p>
<p>We first standardize these four variables so they have mean 0 and
standard deviation 1. This places them on a common ground. Then, we
compute our first kernel: the linear kernel.</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">iris_std</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/scale.html" class="external-link">scale</a></span><span class="op">(</span><span class="va">iris</span><span class="op">[</span>,<span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span> <span class="st">"Sepal.Length"</span>,<span class="st">"Sepal.Width"</span>,<span class="st">"Petal.Length"</span>, <span class="st">"Petal.Width"</span><span class="op">)</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="va">KL</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/Linear.html">Linear</a></span><span class="op">(</span><span class="va">iris_std</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/dim.html" class="external-link">dim</a></span><span class="op">(</span><span class="va">KL</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 150 150</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/class.html" class="external-link">class</a></span><span class="op">(</span><span class="va">KL</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] "matrix" "array"</span></span></code></pre></div>
<p>The linear kernel is simply the pairwise inner product between all
<em>N</em> samples (in our case: flower measurements). The result is
“stored” on a matrix (the kernel matrix) that has dimensions
<em>NxN</em>. Furthermore, it is always symmetric and positive
semi-definite (PSD). To check the kernel between two samples (for
instance, 32 and 106), we can type either <code>KL[32,106]</code> or
<code>KL[106,32]</code>. It should be noted that kernel matrices
generated by <code>kerntools</code> have class “matrix”, “array”. In
fact, to keep things simple, no function in this package requires any
special classes/objects not present in base R.</p>
<p>Next, we examine this kernel matrix further. Although it is not
excessively large, it is large enough to make simply typing
<code>KL</code> in R unpractical. Instead, we may summarize its values
using a histogram:</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/histK.html">histK</a></span><span class="op">(</span><span class="va">KL</span>, vn <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span></code></pre></div>
<p><img src="kerntools_files/figure-html/unnamed-chunk-3-1.png" width="672"></p>
<p>This “almost-gaussian” shape is due to the features’ scaling. Further
parameters, including color, can be passed to <code><a href="../reference/histK.html">histK()</a></code> (for
more info, check the documentation of <code><a href="https://rdrr.io/r/graphics/hist.html" class="external-link">graphics::hist()</a></code>).
The von Neumann entropy shown here is optional (in fact, this value can
be computed separately doing <code>vonNeumann(KL)</code>). Entropy
values close to 0 indicate that all kernel matrix elements are very
similar, while values close to 1 indicate a high variability.</p>
<p>Another possibility is to visualize the whole kernel matrix with a
heatmap:</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/heatK.html">heatK</a></span><span class="op">(</span><span class="va">KL</span>,cos.norm <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span></code></pre></div>
<p><img src="kerntools_files/figure-html/unnamed-chunk-4-1.png" width="672"></p>
<p>Here, yellow denotes a high similarity between the samples, while red
denotes that the similarity is low (the colour palette is customizable
via the parameter <code>color</code>). At a glance we see that the first
50 samples (<em>I. setosa</em>) have a higher intra-group similarity.
Also, they are very different from the samples 101-150 (which correspond
to <em>I. virginica</em>). Instead, <em>I. versicolor</em> is kind of
intermediate between these two groups.</p>
<p>To confirm our intuitions about the (dis)similarity among the three
species, we may proceed to a widespread ordination method: the Principal
Components Analysis (PCA). PCAs can be computed from kernel matrices
very easily. In fact, using kernel matrices <em>expands</em> what a PCA
can do, but this will be discussed in further sections. To display a
beautiful PCA plot that colors the samples by species, we do:</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">iris_species</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html" class="external-link">factor</a></span><span class="op">(</span><span class="va">iris</span><span class="op">$</span><span class="va">Species</span><span class="op">)</span></span>
<span><span class="va">kpca</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/kPCA.html">kPCA</a></span><span class="op">(</span><span class="va">KL</span>,plot <span class="op">=</span> <span class="fl">1</span><span class="op">:</span><span class="fl">2</span>,y <span class="op">=</span> <span class="va">iris_species</span><span class="op">)</span></span>
<span><span class="va">kpca</span><span class="op">$</span><span class="va">plot</span></span></code></pre></div>
<p><img src="kerntools_files/figure-html/unnamed-chunk-5-1.png" width="672"></p>
<p>Indeed, we can see that <em>I. setosa</em> and <em>I. virginica</em>
are the most different groups, while <em>I. versicolor</em> and
<em>I.virginica</em> are very close. The colors can be changed if
desired with the <code>kPCA(...,colors)</code> parameter.</p>
<p>After seeing this plot, we can infer that a predictive model from
these data will work well. Although we have a ton of machine learning
methods at our disposal, in this vignette we will stick with the kernel
methods. More specifically, we will use the most famous kernel method:
the Support Vector Machine (SVM).</p>
<p>SVMs are <strong>not</strong> implemented in <code>kerntools</code>.
However, they are in other R packages like <code>kernlab</code> or
<code>e1071</code>. Here we will use the <code><a href="https://rdrr.io/pkg/kernlab/man/ksvm.html" class="external-link">ksvm()</a></code> function of
the former package:</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va">kernlab</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="fl">777</span><span class="op">)</span></span>
<span><span class="co">## Training data</span></span>
<span><span class="va">test_idx</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sample.html" class="external-link">sample</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fl">150</span><span class="op">)</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">30</span><span class="op">]</span> <span class="co"># 20% of samples</span></span>
<span><span class="va">train_idx</span> <span class="op">&lt;-</span> <span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fl">150</span><span class="op">)</span><span class="op">[</span><span class="op">-</span><span class="va">test_idx</span><span class="op">]</span></span>
<span><span class="va">KL_train</span> <span class="op">&lt;-</span> <span class="va">KL</span><span class="op">[</span><span class="va">train_idx</span>,<span class="va">train_idx</span><span class="op">]</span></span>
<span><span class="co">## Model (training data)</span></span>
<span><span class="va">linear_model</span> <span class="op">&lt;-</span> <span class="fu">kernlab</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/kernlab/man/ksvm.html" class="external-link">ksvm</a></span><span class="op">(</span>x<span class="op">=</span><span class="va">KL_train</span>, y<span class="op">=</span><span class="va">iris_species</span><span class="op">[</span><span class="va">train_idx</span><span class="op">]</span>, kernel<span class="op">=</span><span class="st">"matrix"</span><span class="op">)</span></span>
<span><span class="va">linear_model</span></span>
<span><span class="co">#&gt; Support Vector Machine object of class "ksvm" </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; SV type: C-svc  (classification) </span></span>
<span><span class="co">#&gt;  parameter : cost C = 1 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [1] " Kernel matrix used as input."</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Number of Support Vectors : 27 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Objective Function Value : -0.9459 -0.3184 -14.3709 </span></span>
<span><span class="co">#&gt; Training error : 0.025</span></span></code></pre></div>
<p>First and foremost: in prediction models, it is mandatory to have an
additional test set so a honest estimation of the model’s performance
can be computed (we will do this latter). Also, please note that in a
real-world machine learning setting, training data should have been
preprocessed first and then the <em>same</em> exact preprocessing should
have been applied to test data. In our case, the only preprocessing was
standardize the dataset: thus, the mean and standard deviation should
have been computed from training data, and then these values should have
been used for standardize <em>both</em> the training and test sets. That
being said, in order to not interrupt the flow of this vignette, we will
use leave things as they are.</p>
<p>Now returning to our (questionably obtained) model, we have a very
low training error. The support vectors (which are the only samples that
are relevant for us, as the rest are not used to define the SVM
discriminating hyperplane) constitute only the 22% (approx) of samples.
Not bad.</p>
<p>Before jumping to the test set, we may be interested in another
topic: feature importance. This means studying which variables are
considered more important by the model when discriminating between
classes. Feature importance is important for avoiding “black-box
models”: prediction models that we know that work well, but not
<em>why</em>.</p>
<p>Obtaining the importances out of a SVM model can be somewhat
convoluted (this is discussed later in more depth) and sometimes
downright impossible. In our particular case, the only problem is that
we wanted to classify 3 classes (species)… but SVM classifiers are
binary. For discriminating 3 classes, <code>kernlab</code> in fact
builds 3 classifiers: “setosa vs versicolor”, “setosa vs virginica”, and
“versicolor vs virginica”. These 3 classifiers constitute the
<code>linear_model</code> object and the prediction of the class of a
sample is done by a voting scheme. To simplify things, for the features’
importance part, we will focus only on the third classifier: “versicolor
vs virginica”, which we have seen previously that are the two most
related species. The way to go here is to obtain the index of the
Support Vectors in our model, and their coefficients. All that is
gracefully provided by <code>kernlab</code>. Then, we will return to
<code>kerntools</code> and call the function <code><a href="../reference/svm_imp.html">svm_imp()</a></code>:</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">## Third model: Versicolor vs virginica</span></span>
<span><span class="va">sv_index</span> <span class="op">&lt;-</span> <span class="fu">kernlab</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/kernlab/man/ksvm-class.html" class="external-link">alphaindex</a></span><span class="op">(</span><span class="va">linear_model</span><span class="op">)</span><span class="op">[[</span><span class="fl">3</span><span class="op">]</span><span class="op">]</span> <span class="co"># Vector with the SV indices</span></span>
<span><span class="va">sv_coef</span> <span class="op">&lt;-</span> <span class="fu">kernlab</span><span class="fu">::</span><span class="fu">coef</span><span class="op">(</span><span class="va">linear_model</span><span class="op">)</span><span class="op">[[</span><span class="fl">3</span><span class="op">]</span><span class="op">]</span>  <span class="co"># Vector with the SV coefficients</span></span>
<span></span>
<span><span class="va">feat_imp3</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/svm_imp.html">svm_imp</a></span><span class="op">(</span>X<span class="op">=</span><span class="va">iris_std</span><span class="op">[</span><span class="va">train_idx</span>,<span class="op">]</span>,svindx<span class="op">=</span><span class="va">sv_index</span>,coeff<span class="op">=</span><span class="va">sv_coef</span><span class="op">)</span></span>
<span><span class="co">#&gt; Do not use this function if the SVM model was created with the RBF,</span></span>
<span><span class="co">#&gt;           Laplacian, Bray-Curtis, Jaccard/Ruzicka, or Kendall's tau kernels</span></span></code></pre></div>
<p>Note that here we need the data that we used to compute
<code>KL</code>: <code>iris_std.</code> It is <em>very</em> important to
use this version of the dataset, and not any other version with more
pre-processing (or the “original” without pre-processing).
<code><a href="../reference/svm_imp.html">svm_imp()</a></code> has parameters like <code>center</code>,
<code>scale</code> and <code>cos.norm</code> to take this widespread
normalization techniques into account, but it is better to play
safe.</p>
<p>Conveniently, <code>kerntools</code> provides a function to visualize
the features’ importance:</p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/plotImp.html">plotImp</a></span><span class="op">(</span><span class="va">feat_imp3</span>, leftmargin <span class="op">=</span> <span class="fl">7</span>, main<span class="op">=</span><span class="st">"3rd model: versicolor vs virginica"</span>, color<span class="op">=</span><span class="st">"steelblue1"</span><span class="op">)</span></span></code></pre></div>
<p><img src="kerntools_files/figure-html/unnamed-chunk-8-1.png" width="672"></p>
<pre><code><span><span class="co">#&gt; $first_features</span></span>
<span><span class="co">#&gt; [1] "Petal.Length" "Petal.Width"  "Sepal.Width"  "Sepal.Length"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $cumsum</span></span>
<span><span class="co">#&gt; [1] 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $barplot</span></span>
<span><span class="co">#&gt;      [,1]</span></span>
<span><span class="co">#&gt; [1,]  0.7</span></span>
<span><span class="co">#&gt; [2,]  1.9</span></span>
<span><span class="co">#&gt; [3,]  3.1</span></span>
<span><span class="co">#&gt; [4,]  4.3</span></span></code></pre>
<p>As we can see, the model considers the petals most discriminating
that the sepals, and within the petals’ measures, the petal length.</p>
<p>If we return to the PCA, we could also check the weight of each
variable in the first and second PCs (that is, the ones we displayed).
To do so, it comes in handy the <code><a href="../reference/kPCA_imp.html">kPCA_imp()</a></code> function. Again,
this function requires the dataset that generated the <code>KL</code>
matrix:</p>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">loadings</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/kPCA_imp.html">kPCA_imp</a></span><span class="op">(</span><span class="va">iris_std</span><span class="op">)</span></span>
<span><span class="co">#&gt; Do not use this function if the PCA was created with the RBF,</span></span>
<span><span class="co">#&gt;           Laplacian, Bray-Curtis, Jaccard/Ruzicka, or Kendall's tau kernels</span></span>
<span><span class="va">pcs</span> <span class="op">&lt;-</span> <span class="va">loadings</span><span class="op">$</span><span class="va">loadings</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">2</span>,<span class="op">]</span></span>
<span><span class="va">pcs</span></span>
<span><span class="co">#&gt;     Sepal.Length Sepal.Width Petal.Length Petal.Width</span></span>
<span><span class="co">#&gt; PC1    0.5210659  -0.2693474   0.58041310  0.56485654</span></span>
<span><span class="co">#&gt; PC2    0.3774176   0.9232957   0.02449161  0.06694199</span></span></code></pre></div>
<p>It seems that the first component is dominated by Petal Length and
Petal Width but also by Sepal Length. The second component, that plays a
role in discriminating <em>I. versicolor</em> and <em>I.virginica</em>,
is dominated by Sepal Width. The PCA disagrees a bit with the SVM
feature importances, but remember that in the latter we focused only on
the “versicolor vs virginica” problem, while in the former we are
looking at the ordination of the three classes. We may represent the
contributions of the four features for the 1st PC, and to make things
easier we will include the 2nd PC onto the barplot:</p>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/plotImp.html">plotImp</a></span><span class="op">(</span><span class="va">pcs</span><span class="op">[</span><span class="fl">1</span>,<span class="op">]</span>, y<span class="op">=</span><span class="va">pcs</span><span class="op">[</span><span class="fl">2</span>,<span class="op">]</span>, ylegend<span class="op">=</span><span class="st">"PC2"</span>,absolute<span class="op">=</span><span class="cn">FALSE</span>, main<span class="op">=</span><span class="st">"PC1"</span>, leftmargin <span class="op">=</span> <span class="fl">7</span>,  color<span class="op">=</span><span class="st">"rosybrown1"</span><span class="op">)</span></span></code></pre></div>
<p><img src="kerntools_files/figure-html/unnamed-chunk-10-1.png" width="672"></p>
<pre><code><span><span class="co">#&gt; $first_features</span></span>
<span><span class="co">#&gt; [1] "Petal.Length" "Petal.Width"  "Sepal.Length" "Sepal.Width" </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $cumsum</span></span>
<span><span class="co">#&gt; [1] 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $barplot</span></span>
<span><span class="co">#&gt;      [,1]</span></span>
<span><span class="co">#&gt; [1,]  0.7</span></span>
<span><span class="co">#&gt; [2,]  1.9</span></span>
<span><span class="co">#&gt; [3,]  3.1</span></span>
<span><span class="co">#&gt; [4,]  4.3</span></span></code></pre>
<p>We used <code>absolute=FALSE</code> because the contribution of each
variable to the PC is relevant not only in magnitude, but also in sign.
Pink bars represent PC1, while the black line represents PC2 (parameter
<code>y</code>). As we only wanted to see the order and the relative
magnitude, the X axis show the relative contribution (in the
<code><a href="../reference/plotImp.html">plotImp()</a></code> function, <code>relative=TRUE</code> by
default).</p>
<p>With <code>kerntools</code>, we can draw the contributions on the PCA
plot as arrows. We only need the PCA plot (given by <code><a href="../reference/kPCA.html">kPCA()</a></code>)
and the contributions (given by <code><a href="../reference/kPCA_imp.html">kPCA_imp()</a></code>):</p>
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/kPCA_arrows.html">kPCA_arrows</a></span><span class="op">(</span>plot<span class="op">=</span><span class="va">kpca</span><span class="op">$</span><span class="va">plot</span>,contributions<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/t.html" class="external-link">t</a></span><span class="op">(</span><span class="va">pcs</span><span class="op">)</span>,colour<span class="op">=</span><span class="st">"grey15"</span><span class="op">)</span></span></code></pre></div>
<p><img src="kerntools_files/figure-html/unnamed-chunk-11-1.png" width="672"></p>
<p>(Note that the arrows are scaled to match with the original PCA plot.
They are somewhat orientative: their directions are correct, and longer
arrows represent a greater contribution to a PC that shorter arrows;
however, usually the arrows’ magnitudes do not coincide with the actual
magnitudes that can be computed from <code><a href="../reference/kPCA_imp.html">kPCA_imp()</a></code>).</p>
<p>And now, finally, we are going to check the performance in the test
set (considering the 3 classes). To do so, we subset <code>KL</code> to
have a suitable <em>test x training</em> matrix, input the matrix into
our linear_model, and compare with the predicted species with the actual
species:</p>
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">KL_test</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/kernlab/man/as.kernelMatrix.html" class="external-link">as.kernelMatrix</a></span><span class="op">(</span><span class="va">KL</span><span class="op">[</span><span class="va">test_idx</span>,<span class="va">train_idx</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="co">## Prediction (test data)</span></span>
<span><span class="va">pred_class</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict</a></span><span class="op">(</span><span class="va">linear_model</span>,<span class="va">KL_test</span><span class="op">)</span></span>
<span><span class="va">actual_class</span> <span class="op">&lt;-</span> <span class="va">iris_species</span><span class="op">[</span><span class="va">test_idx</span><span class="op">]</span></span>
<span><span class="va">pred_class</span></span>
<span><span class="co">#&gt;  [1] versicolor versicolor virginica  virginica  versicolor virginica </span></span>
<span><span class="co">#&gt;  [7] virginica  virginica  versicolor versicolor versicolor versicolor</span></span>
<span><span class="co">#&gt; [13] versicolor versicolor versicolor versicolor virginica  versicolor</span></span>
<span><span class="co">#&gt; [19] versicolor versicolor virginica  versicolor virginica  versicolor</span></span>
<span><span class="co">#&gt; [25] versicolor virginica  versicolor versicolor versicolor versicolor</span></span>
<span><span class="co">#&gt; Levels: setosa versicolor virginica</span></span>
<span><span class="va">actual_class</span></span>
<span><span class="co">#&gt;  [1] setosa     setosa     versicolor versicolor setosa     setosa    </span></span>
<span><span class="co">#&gt;  [7] setosa     versicolor setosa     setosa     versicolor virginica </span></span>
<span><span class="co">#&gt; [13] virginica  virginica  virginica  virginica  setosa     versicolor</span></span>
<span><span class="co">#&gt; [19] setosa     setosa     setosa     setosa     versicolor setosa    </span></span>
<span><span class="co">#&gt; [25] virginica  versicolor virginica  virginica  virginica  versicolor</span></span>
<span><span class="co">#&gt; Levels: setosa versicolor virginica</span></span></code></pre></div>
<p>Mmmm… maybe we were a bit overconfident. It seems that the model has
ignored <em>I. setosa</em> completely.</p>
<p>We can compute numerically how “good” (or wrong) our model is
according to different performance measures implemented in
<code>kerntools</code>. When dealing with classification, all of them
need a contingency table that contrasts the actual and the predicted
classes (also known as “confusion matrix”). The most simple measure is
the accuracy: number of right predictions divided by the number of
predictions (which is the test set size: in our case, 30).</p>
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">ct</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/table.html" class="external-link">table</a></span><span class="op">(</span><span class="va">actual_class</span>,<span class="va">pred_class</span><span class="op">)</span> <span class="co"># Confusion matrix</span></span>
<span><span class="va">ct</span></span>
<span><span class="co">#&gt;             pred_class</span></span>
<span><span class="co">#&gt; actual_class setosa versicolor virginica</span></span>
<span><span class="co">#&gt;   setosa          0          9         4</span></span>
<span><span class="co">#&gt;   versicolor      0          3         5</span></span>
<span><span class="co">#&gt;   virginica       0          9         0</span></span>
<span><span class="fu"><a href="../reference/Acc.html">Acc</a></span><span class="op">(</span><span class="va">ct</span><span class="op">)</span> <span class="co">## Accuracy</span></span>
<span><span class="co">#&gt; [1] 0.1</span></span>
<span><span class="fu"><a href="../reference/Acc_rnd.html">Acc_rnd</a></span><span class="op">(</span><span class="va">actual_class</span><span class="op">)</span> <span class="co">## Accuracy of the random model</span></span>
<span><span class="co">#&gt; [1] 0.3488889</span></span></code></pre></div>
<p>As expected, our accuracy is overwhelmingly low. We can compare the
result with the accuracy of the random model (according to the class
distribution on the test): 0.35, which means that our model performs a
lot worse than someone classifying at random.</p>
<p>We can explore other measures to infer what are the problems with our
prediction model (for instance, if a species is systematically
missclassified, etc.). For this example, we can compute these measures
by class:</p>
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/Prec.html">Prec</a></span><span class="op">(</span><span class="va">ct</span>,multi.class <span class="op">=</span> <span class="st">"none"</span><span class="op">)</span> <span class="co">## Precision or Positive Predictive Value</span></span>
<span><span class="co">#&gt;     setosa versicolor  virginica </span></span>
<span><span class="co">#&gt;  0.0000000  0.1428571  0.0000000</span></span>
<span><span class="fu"><a href="../reference/Rec.html">Rec</a></span><span class="op">(</span><span class="va">ct</span>,multi.class <span class="op">=</span> <span class="st">"none"</span><span class="op">)</span> <span class="co">## Recall or True Positive Rate</span></span>
<span><span class="co">#&gt;     setosa versicolor  virginica </span></span>
<span><span class="co">#&gt;      0.000      0.375      0.000</span></span>
<span><span class="fu"><a href="../reference/Spe.html">Spe</a></span><span class="op">(</span><span class="va">ct</span>,multi.class <span class="op">=</span> <span class="st">"none"</span><span class="op">)</span> <span class="co">## Specificity or True Negative Rate</span></span>
<span><span class="co">#&gt;     setosa versicolor  virginica </span></span>
<span><span class="co">#&gt;  1.0000000  1.0000000  0.5714286</span></span>
<span><span class="fu"><a href="../reference/F1.html">F1</a></span><span class="op">(</span><span class="va">ct</span>,multi.class <span class="op">=</span> <span class="st">"none"</span><span class="op">)</span> <span class="co">## F1 (harmonic mean of Precision and Recall)</span></span>
<span><span class="co">#&gt;     setosa versicolor  virginica </span></span>
<span><span class="co">#&gt;  0.0000000  0.2068966  0.0000000</span></span></code></pre></div>
<p>(In case we want the overall performance measure, we can compute the
mean of the three classes, or type
<code>multi.class="macro"</code>).</p>
<p>The precision measure tell us that none of the samples predicted to
be “virginica” or “setosa” are correct (in the case of “setosa”, because
none was predicted), and only some (1/7) that were predicted to be
“versicolor” were right. The recall shows that only 3/8 “versicolor”
samples in the test were correctly classified as “versicolor”, while
there is none of “setosa” or “virginica.” F1 is useful because it gives
a “mean” of Precision and Recall. Meanwhile, the low specificity of
“versicolor” points that a lot of samples that were not “versicolor”
were predicted as such.</p>
</div>
<div class="section level3">
<h3 id="a-slightly-more-complicated-example">A (slightly) more complicated example<a class="anchor" aria-label="anchor" href="#a-slightly-more-complicated-example"></a>
</h3>
<p>In the previous section we picked naively the first model we could
train, with not-so-great results. Here, we are going to complicate
things a bit hoping that we will obtain a model that <em>works</em>.</p>
<p>(Note: of course, iris flower classification is a simple task. In
fact, it can be achieved pretty decently with the linear kernel, as can
be deduced from the previous PCA: a linear classifier is enough to
discriminate the flowers using only the 1st and 2nd PCs. However, for
the sake of the example, we will use a different kernel in the present
section).</p>
<p>The radial basis function (RBF) kernel is something like the “gold
standard” among kernels. Unlike the linear kernel (which is the most
simple or “plain” kernel), it is nonlinear: in fact, the RBF kernel is a
universal approximator. We can compute it doing:</p>
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">Krbf</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/RBF.html">RBF</a></span><span class="op">(</span><span class="va">iris_std</span>,g<span class="op">=</span><span class="fl">0.25</span><span class="op">)</span></span>
<span><span class="fu"><a href="../reference/histK.html">histK</a></span><span class="op">(</span><span class="va">Krbf</span>,col<span class="op">=</span><span class="st">"aquamarine"</span>,vn <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span></code></pre></div>
<p><img src="kerntools_files/figure-html/unnamed-chunk-15-1.png" width="672"></p>
<p>Kernel matrix values are typically between 0 and 1. The linear kernel
required only our dataset, but <code><a href="../reference/RBF.html">RBF()</a></code> has a (hyper)parameter
called gamma (<code>g</code> for short). The value of this
hyperparameter should be decided by us, which is an important decision,
because it will affect the decision boundary of the kernel. Fortunately,
some heuristics to estimate a good gamma exist. <code>kerntools</code>
implement three of them, which are available in the function
<code><a href="../reference/estimate_gamma.html">estimate_gamma()</a></code>:</p>
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/estimate_gamma.html">estimate_gamma</a></span><span class="op">(</span><span class="va">iris_std</span><span class="op">)</span></span>
<span><span class="co">#&gt; $d_criterion</span></span>
<span><span class="co">#&gt; [1] 0.25</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $scale_criterion</span></span>
<span><span class="co">#&gt; [1] 0.2512584</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $quantiles_criterion</span></span>
<span><span class="co">#&gt;        90%        50%        10% </span></span>
<span><span class="co">#&gt; 0.05570343 0.16670322 1.73533468</span></span></code></pre></div>
<p>In the previous histogram we visualized the RBF with the gamma given
by the “d_criterion” (and almost the one given by the “scale
criterion”). The third heuristic gives us a distribution of “good” gamma
values. Now, for the sake of comparison, we will compute the RBF kernel
using the median of the “quantiles_criterion”:</p>
<div class="sourceCode" id="cb19"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">Krbf2</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/RBF.html">RBF</a></span><span class="op">(</span><span class="va">iris_std</span>,g<span class="op">=</span><span class="fl">0.1667</span><span class="op">)</span></span>
<span><span class="fu"><a href="../reference/histK.html">histK</a></span><span class="op">(</span><span class="va">Krbf2</span>,col<span class="op">=</span><span class="st">"darkseagreen1"</span>,vn<span class="op">=</span><span class="cn">TRUE</span><span class="op">)</span></span></code></pre></div>
<p><img src="kerntools_files/figure-html/unnamed-chunk-17-1.png" width="672"></p>
<p>Not only the histogram changes: the von Neumann entropy changes as
well. It important to remark that the RBF kernel is very sensitive to
the gamma values. The higher entropy with respect to that of the linear
kernel reflects that, here, we have a higher variability in the kernel
matrix values. (That can be also be deduced comparing both histograms.
Conversely, if we do <code>heatK(Krbf)</code>, we will observe more
extreme values/colors than before). <a href="https://www.mdpi.com/1099-4300/25/1/154" class="external-link">This paper</a> recommends
an entropy between 0.3-0.5, so maybe this will be reflected on the SVM
model’s performance?</p>
<p>Now, we can also do a kernel PCA. Our previous kernel PCA used a
linear kernel so, in reality, it was identical to a “normal” PCA. This
time however we are using a different kernel and now we can actually say
that this is a kernel PCA. The main difference is that the projection of
samples is <em>not</em> going to be linear. Sometimes, this creates
strange patterns that are difficult to interpret.</p>
<p>As later we are going to train a SVM model, it may occur to us that
it would be great to do a PCA only with the training samples, so we can
compare the prediction model with the PCA side by side. To do so, we
will use the same training indices than in the previous section. Even
better: what if we compute a (kernel) PCA with the training samples, and
then project the test samples over them?</p>
<div class="sourceCode" id="cb20"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">Krbf_train</span> <span class="op">&lt;-</span> <span class="va">Krbf2</span><span class="op">[</span><span class="va">train_idx</span>,<span class="va">train_idx</span><span class="op">]</span></span>
<span><span class="va">Krbf_test</span> <span class="op">&lt;-</span> <span class="va">Krbf2</span><span class="op">[</span><span class="va">test_idx</span>,<span class="va">train_idx</span><span class="op">]</span></span>
<span><span class="va">rbf_kpca</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/kPCA.html">kPCA</a></span><span class="op">(</span>K<span class="op">=</span><span class="va">Krbf_train</span>, Ktest<span class="op">=</span><span class="va">Krbf_test</span>, plot <span class="op">=</span> <span class="fl">1</span><span class="op">:</span><span class="fl">2</span>, y <span class="op">=</span> <span class="va">iris_species</span><span class="op">[</span><span class="va">train_idx</span><span class="op">]</span>, title <span class="op">=</span> <span class="st">"RBF PCA"</span><span class="op">)</span></span>
<span><span class="va">rbf_kpca</span><span class="op">$</span><span class="va">plot</span></span></code></pre></div>
<p><img src="kerntools_files/figure-html/unnamed-chunk-18-1.png" width="672"></p>
<p>(Note: remember that, in a real-world problem, the standardization of
the dataset should have been done with the center and std deviation of
the training set.)</p>
<p>Said and done! However, now the patterns on this kernel PCA are a
bit… radial. Still, <em>I. setosa</em> is again on one side, and <em>I.
versicolor</em> and <em>I. virginica</em> on the other. The red, green
and blue samples are the training samples, where the grey samples
correspond to the test samples we projected <em>a posteriori</em> (any
other color can be specified in the <code>kPCA</code> parameter
<code>na_col</code>).</p>
<p>What now? As we have generated more than one kernel matrix from the
same data (thanks to the linear and the RBF kernels), we may compare
these matrices. To do so, we can use a <code>kerntools</code> function
called <code>simK</code>:</p>
<div class="sourceCode" id="cb21"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/simK.html">simK</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>linear<span class="op">=</span><span class="va">KL</span>,rbf_0.166<span class="op">=</span><span class="va">Krbf</span>, rbf_0.25<span class="op">=</span><span class="va">Krbf2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; Remember that Klist should contain only kernel matrices (i.e. squared, symmetric and PSD).</span></span>
<span><span class="co">#&gt;   This function does NOT verify the symmetry and PSD criteria.</span></span>
<span><span class="co">#&gt;              linear rbf_0.166  rbf_0.25</span></span>
<span><span class="co">#&gt; linear    1.0000000 0.5208955 0.4803192</span></span>
<span><span class="co">#&gt; rbf_0.166 0.5208955 1.0000000 0.9898203</span></span>
<span><span class="co">#&gt; rbf_0.25  0.4803192 0.9898203 1.0000000</span></span></code></pre></div>
<p><code>simK</code> will first remind us that a matrix should have
several mathematical properties to be a kernel matrix. When we work with
kernel matrices generated by <code>kerntools</code> (that is:
<code><a href="../reference/Linear.html">Linear()</a></code>, <code><a href="../reference/RBF.html">RBF()</a></code>, etc.) everything will be
alright. However, you can come to <code>kerntools</code> with your
precomputed kernel matrices (as long as they have class “matrix”,
“array”). <code>kerntools</code> implicitly trusts the user knows what
he/she is doing, so remember using proper kernel matrices.</p>
<p><code>simK</code> returns a score between 0 and 1: 1 is complete
similarity, and 0 is complete dissimilarity. We can see that the two RBF
matrices are very similar, while the linear kernel matrix is around a
50% similar to the RBF matrices.</p>
<p>We could also compare the two PCAs. An option to do so is computing
the RV coefficient (Co-Inertia Analysis). However, the RV coefficient
between <code>rbf_kpca</code> and <code>kpca</code> should give the same
result than <code>simK(list(KL,Krbf))</code>. It should be noted that
this equivalence only holds if the dataset is centered beforehand, as
PCAs usually are computed using centered data (for that reason we have
<code>kPCA(..., center=TRUE)</code> by default). If a kernel matrix was
obtained from a non-centered data, it can be centered afterwards with
<code><a href="../reference/centerK.html">centerK()</a></code> (more of this in later sections).</p>
<p>Another way to compare two PCAs is called the Procrustes Analysis.
This analysis compares the correlation between two projections after
“removing” the translation, scale and rotation effects. Although is not
properly a kernel method, <code>kerntools</code> can do a basic
Procrustes Analysis. In our data, we have a moderate Procrustes
correlation: 0.68 (the correlation coefficient is bounded between 0 and
1).</p>
<div class="sourceCode" id="cb22"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">rbf_kpca</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/kPCA.html">kPCA</a></span><span class="op">(</span>K<span class="op">=</span><span class="va">Krbf</span><span class="op">)</span></span>
<span><span class="va">proc</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/Procrustes.html">Procrustes</a></span><span class="op">(</span><span class="va">kpca</span><span class="op">$</span><span class="va">projection</span>,<span class="va">rbf_kpca</span><span class="op">)</span></span>
<span><span class="va">proc</span><span class="op">$</span><span class="va">pro.cor</span> <span class="co"># Procrustes correlation</span></span>
<span><span class="co">#&gt; [1] 0.6862007</span></span></code></pre></div>
<p>(This is a moment as good as any other to show that
<code><a href="../reference/kPCA.html">kPCA()</a></code> can return the kernel PCA projection without
displaying any plot. In that case, all graphical parameters like colors,
labels, etc. are ignored.)</p>
<p>With all of these, we will train a brand new (and hopefully better)
model. We will re-use the same training and test samples:</p>
<div class="sourceCode" id="cb23"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">####### Model (training data)</span></span>
<span><span class="va">model</span> <span class="op">&lt;-</span> <span class="fu">kernlab</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/kernlab/man/ksvm.html" class="external-link">ksvm</a></span><span class="op">(</span>x<span class="op">=</span><span class="va">Krbf_train</span>, y<span class="op">=</span><span class="va">iris_species</span><span class="op">[</span><span class="va">train_idx</span><span class="op">]</span>, kernel<span class="op">=</span><span class="st">"matrix"</span>, C<span class="op">=</span><span class="fl">10</span><span class="op">)</span></span>
<span><span class="va">model</span></span>
<span><span class="co">#&gt; Support Vector Machine object of class "ksvm" </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; SV type: C-svc  (classification) </span></span>
<span><span class="co">#&gt;  parameter : cost C = 10 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [1] " Kernel matrix used as input."</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Number of Support Vectors : 30 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Objective Function Value : -4.1707 -2.7089 -92.348 </span></span>
<span><span class="co">#&gt; Training error : 0.008333</span></span></code></pre></div>
<p>A very low training error, but now we are wiser. What about the
performance in test?</p>
<div class="sourceCode" id="cb24"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">Krbf_test</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/kernlab/man/as.kernelMatrix.html" class="external-link">as.kernelMatrix</a></span><span class="op">(</span><span class="va">Krbf_test</span><span class="op">)</span></span>
<span><span class="co">####### Prediction (test data)</span></span>
<span><span class="va">pred_class</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict</a></span><span class="op">(</span><span class="va">model</span>,<span class="va">Krbf_test</span><span class="op">)</span></span>
<span><span class="va">actual_class</span> <span class="op">&lt;-</span> <span class="va">iris_species</span><span class="op">[</span><span class="va">test_idx</span><span class="op">]</span></span>
<span><span class="va">ct</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/table.html" class="external-link">table</a></span><span class="op">(</span><span class="va">actual_class</span>,<span class="va">pred_class</span><span class="op">)</span> <span class="co"># Confusion matrix</span></span>
<span><span class="fu"><a href="../reference/Acc.html">Acc</a></span><span class="op">(</span><span class="va">ct</span><span class="op">)</span> <span class="co">## Accuracy</span></span>
<span><span class="co">#&gt; [1] 0.5</span></span></code></pre></div>
<p>Wow, that seems a lot better! However, before we get excited, we must
remember that this is a point estimation of accuracy, that comes from a
specific test set (the 30 samples we chose randomly in the previous
section). Another test set surely will deliver a different accuracy.
What if we tried to compute a confidence interval (CI) to have an idea
of how other test sets will behave?</p>
<p><code>kerntools</code> provides two ways to obtain a CI: the Normal
Approximation, and bootstrapping. Normal approximation is quicker, while
bootstrapping is usually considered to be safer (more details: <a href="https://sebastianraschka.com/blog/2022/confidence-intervals-for-ml.html" class="external-link">here</a>):</p>
<div class="sourceCode" id="cb25"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span> <span class="co">## Accuracy CI (95%)</span></span>
<span><span class="fu"><a href="../reference/Normal_CI.html">Normal_CI</a></span><span class="op">(</span>value <span class="op">=</span> <span class="fl">0.5</span>,ntest <span class="op">=</span> <span class="fl">30</span><span class="op">)</span> <span class="co">## Accuracy CI (95%)</span></span>
<span><span class="co">#&gt; [1] 0.3210806 0.6789194</span></span>
<span><span class="fu"><a href="../reference/Boots_CI.html">Boots_CI</a></span><span class="op">(</span>target <span class="op">=</span> <span class="va">actual_class</span>, pred <span class="op">=</span> <span class="va">pred_class</span>, nboots <span class="op">=</span> <span class="fl">2000</span>,index <span class="op">=</span> <span class="st">"acc"</span><span class="op">)</span> </span>
<span><span class="co">#&gt;                2.5%     97.5% </span></span>
<span><span class="co">#&gt; 0.5016667 0.3333333 0.7000000</span></span></code></pre></div>
<p>Both functions default to a 95% CI, but that can be changed via the
<code>confidence</code> parameter. According to the normal
approximation, the accuracy is 0.5 (0.32, 0.68), while according to
bootstrap strategy, it is 0.5 (0.33, 0.66). The CI is wide because the
test is very small (only 30 samples). However, with this test and CI
(with the 95% confidence) we cannot <em>really</em> assure that this
model is really different than the random model, which had an accuracy
of 0.35 (as we computed in the previous section). Useful reminder: next
time, we should choose a larger test set.</p>
<p>Before call it a day, we are going to compute again the other
performance measures. This time, we will not compute them class by
class, but on average (the “macro” approach):</p>
<div class="sourceCode" id="cb26"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/Prec.html">Prec</a></span><span class="op">(</span><span class="va">ct</span><span class="op">)</span> <span class="co">## Precision or Positive Predictive Value</span></span>
<span><span class="co">#&gt; It is identical to weighted Accuracy</span></span>
<span><span class="co">#&gt; [1] 0.4357143</span></span>
<span><span class="fu"><a href="../reference/Rec.html">Rec</a></span><span class="op">(</span><span class="va">ct</span><span class="op">)</span> <span class="co">## Recall or True Positive Rate</span></span>
<span><span class="co">#&gt; [1] 0.4807692</span></span>
<span><span class="fu"><a href="../reference/Spe.html">Spe</a></span><span class="op">(</span><span class="va">ct</span><span class="op">)</span> <span class="co">## Specificity or True Negative Rate</span></span>
<span><span class="co">#&gt; [1] 0.8085477</span></span>
<span><span class="fu"><a href="../reference/F1.html">F1</a></span><span class="op">(</span><span class="va">ct</span><span class="op">)</span> <span class="co">## F1 (harmonic mean of Precision and Recall)</span></span>
<span><span class="co">#&gt; [1] 0.4484848</span></span></code></pre></div>
<p>If we desire to do, we can compute a CI for these values: for
instance, to bootstrap the macro-F1 value, we could simply type
<code>index = "f1"</code> in the <code><a href="../reference/Boots_CI.html">Boots_CI()</a></code> function. In
any case, we should congratulate ourselves because the performance is
clearly higher than last time. Training <em>seriously</em> a machine
learning model involves fine hyperparameter tuning (remember that C in
<code><a href="https://rdrr.io/pkg/kernlab/man/ksvm.html" class="external-link">ksvm()</a></code>?) that we have almost completely skipped. That is:
we should use a strategy like, say, grid search, and compare the
performance measures of each hyperparameter combination via cross
validation, which is far beyond the purposes of this vignette (and
<code>kerntools</code>).</p>
<p>Finally, a warning about computing feature importances in SVM and/or
feature contribution to PCs in PCA. In a few words: don’t do that when
using the RBF kernel. More precisely: of all kernel functions
implemented here, do not ever try to recover the feature contributions
for <code><a href="../reference/RBF.html">RBF()</a></code>, <code><a href="../reference/Laplace.html">Laplace()</a></code>, <code><a href="../reference/Jaccard.html">Jaccard()</a></code>,
<code><a href="../reference/BrayCurtis.html">Ruzicka()</a></code>, <code><a href="../reference/BrayCurtis.html">BrayCurtis()</a></code> and
<code><a href="../reference/Kendall.html">Kendall()</a></code> unless you know <em>very</em> well what you are
doing. If you type something like:</p>
<div class="sourceCode" id="cb27"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">####### RBF versicolor vs virginica model:</span></span>
<span><span class="va">sv_index</span> <span class="op">&lt;-</span> <span class="fu">kernlab</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/kernlab/man/ksvm-class.html" class="external-link">alphaindex</a></span><span class="op">(</span><span class="va">model</span><span class="op">)</span><span class="op">[[</span><span class="fl">3</span><span class="op">]</span><span class="op">]</span></span>
<span><span class="va">sv_coef</span> <span class="op">&lt;-</span> <span class="fu">kernlab</span><span class="fu">::</span><span class="fu">coef</span><span class="op">(</span><span class="va">model</span><span class="op">)</span><span class="op">[[</span><span class="fl">3</span><span class="op">]</span><span class="op">]</span></span>
<span><span class="fu"><a href="../reference/svm_imp.html">svm_imp</a></span><span class="op">(</span>X<span class="op">=</span><span class="va">iris_std</span><span class="op">[</span><span class="va">train_idx</span>,<span class="op">]</span>,svindx<span class="op">=</span><span class="va">sv_index</span>,coeff<span class="op">=</span><span class="va">sv_coef</span><span class="op">)</span></span>
<span><span class="co">#&gt; Do not use this function if the SVM model was created with the RBF,</span></span>
<span><span class="co">#&gt;           Laplacian, Bray-Curtis, Jaccard/Ruzicka, or Kendall's tau kernels</span></span>
<span><span class="co">#&gt; Sepal.Length  Sepal.Width Petal.Length  Petal.Width </span></span>
<span><span class="co">#&gt;    0.8931422    0.7828191   12.3522663   10.0792776</span></span></code></pre></div>
<p>something will be computed, but the result <strong>is wrong</strong>.
Do not ignore the warning raised here. This is not right because,
ultimately, all kernels behave like the linear kernel: they compute the
inner product of some features. But what features? That is the question.
Under the hood, all kernels “send” the original features (the feature
that live in the <em>input space</em>) to other space (usually
higher-dimensional) called the <em>feature space</em>, and they compute
the inner product there. The kernel conflates these two steps into one,
which usually simplifies a lot the calculations and saves a lot of
memory space: this is called the “kernel trick”. But to compute
analytically the feature contributions we need to go that feature space.
To make things worse, the feature space that the kernel implicitly is
using depends on things like the dimensionality of input data, the kind
of kernel, the specific value of its hyperparameters, etc. Going to this
feature space is only trivial for the linear kernel: only then input
space = feature space. Instead, for the RBF kernel, this feature space
is infinite dimensional. Some techniques to estimate it exist (see for
instance: <a href="https://arxiv.org/abs/1109.4603" class="external-link">Explicit
Approximations of the Gaussian Kernel</a>), but they are not yet
implemented in <code>kerntools</code> (and maybe they never are).</p>
</div>
</div>
<div class="section level2">
<h2 id="non-standard-data-exotic-normalizations-and-more-about-feature-spaces">Non-standard data, exotic normalizations, and more about feature
spaces<a class="anchor" aria-label="anchor" href="#non-standard-data-exotic-normalizations-and-more-about-feature-spaces"></a>
</h2>
<p>The natural workflow of this package has been shown (twice) in
previous sections. For that reason, the remainder of this vignette will
deal with more obscure (and interesting) matters concerning kernels for
“non-standard” kinds of data. Also, we will delve deeper in the feature
space and normalization effects.</p>
<div class="section level3">
<h3 id="non-standard-data">Non-standard data<a class="anchor" aria-label="anchor" href="#non-standard-data"></a>
</h3>
<p>Until now, we have worked with kernels for real vectors. That is, we
had a dataset that consisted of several features (four in our case: the
sepal and petal length and width) measured in a set of individuals (the
150 iris flowers). Another way of looking at it is considering that we
had 150 vectors of length 4 (incidentally, this is the way kernel
functions look at data). These were real vectors (or, at least, they
were after we standardized them). Unknowingly, we have also have worked
with kernels for real matrices: we compared three kernel matrices with
<code><a href="../reference/simK.html">simK()</a></code> and the result was… another kernel matrix. In fact,
<code><a href="../reference/simK.html">simK()</a></code> is simply a wrapper of <code><a href="../reference/Frobenius.html">Frobenius()</a></code>. In
the Frobenius kernel, the input of the function (the “objects” they work
with) are not vectors, but numeric <em>matrices</em>.</p>
<p>Most machine learning methods work primarily with real vectors or, in
some cases, matrices. In the case of the kernel methods, they can work
with virtually any kind of data we can think of. This is because what
the kernel method (SVM, kernel PCA, etc) sees is the kernel matrix, not
the original objects. So, as long as we can create a valid
(i.e. symmetric and PSD) kernel function for our objects, everything
will turn just well. The list of kernel functions is endless. Right now,
<code>kerntools</code> can deal effortlessly with the following kinds of
data:</p>
<ul>
<li>Real vectors: Linear, RBF, Laplacian kernels</li>
<li>Real matrices: Frobenius kernel</li>
<li>Counts or Frequencies (non-negative numbers): Bray-Curtis, Ruzicka
(quantitative Jaccard) kernels</li>
<li>Categorical data: Overlap / Dirac kernel</li>
<li>Sets: Intersect, Jaccard kernels</li>
<li>Ordinal data / rankings: Kendall’s tau kernel</li>
<li>Strings / Text: Spectrum kernel</li>
</ul>
<p>All of them are commonly used in different fields. For instace,
categorical data is as widespread as numeric data (or more), text mining
and content analysis is right now a very lively research field, and the
Bray-Curtis and Ruzicka kernels are closely related to famous
beta-diversity indices used in ecological studies.</p>
<p>We will illustrate how <code>kerntools</code> works with categorical
variables. (For the rest of kernel functions, please read in detail
their specific documentation page). <code>kerntools</code> includes a
categorical dataset called <code>showdata</code>:</p>
<div class="sourceCode" id="cb28"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">showdata</span><span class="op">)</span></span>
<span><span class="co">#&gt;    Favorite.color             Favorite.actress        Favorite.actor</span></span>
<span><span class="co">#&gt;  blue     :20     Sophie Turner       :22      Peter Dinklage:20    </span></span>
<span><span class="co">#&gt;  red      :20     Emilia Clarke       :19      Kit Harington :17    </span></span>
<span><span class="co">#&gt;  black    :14     Anya Chalotra       :12      Henry Cavill  :15    </span></span>
<span><span class="co">#&gt;  purple   :12     Freya Allan         :10      Lee Jung-jae  :15    </span></span>
<span><span class="co">#&gt;  green    : 8     Helena Bonham Carter: 8      Hyun Bin      : 8    </span></span>
<span><span class="co">#&gt;  lightblue: 8     Lorraine Ashbourne  : 7      Josh O'Connor : 7    </span></span>
<span><span class="co">#&gt;  (Other)  :18     (Other)             :22      (Other)       :18    </span></span>
<span><span class="co">#&gt;           Favorite.show Liked.new.show</span></span>
<span><span class="co">#&gt;  Game of Thrones :22    No :48        </span></span>
<span><span class="co">#&gt;  The witcher     :17    Yes:52        </span></span>
<span><span class="co">#&gt;  Bridgerton      :14                  </span></span>
<span><span class="co">#&gt;  Squid game      :11                  </span></span>
<span><span class="co">#&gt;  The crown       :11                  </span></span>
<span><span class="co">#&gt;  La casa de papel: 8                  </span></span>
<span><span class="co">#&gt;  (Other)         :17</span></span></code></pre></div>
<p>Here we can see 5 categorical features (class: “factor”). Typical
approaches to this kind of data involve recoding them to “dummy”
variables, so a single categorical variable is transformed to <em>L</em>
dummy variables (where <em>L=number of classes</em>, or using the R
nomenclature, <em>number of levels</em>). Presence of a given class is
marked with a 1 in its corresponding column, while the rest of entries
are 0. This is called one-hot-encoding, and in <code>kerntools</code>
this is done with the <code><a href="../reference/dummy_var.html">dummy_var()</a></code> function:</p>
<div class="sourceCode" id="cb29"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">dummy_showdata</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/dummy_data.html">dummy_data</a></span><span class="op">(</span><span class="va">showdata</span><span class="op">)</span></span>
<span><span class="va">dummy_showdata</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">5</span>,<span class="fl">1</span><span class="op">:</span><span class="fl">3</span><span class="op">]</span></span>
<span><span class="co">#&gt;   Favorite.color_black Favorite.color_blue Favorite.color_green</span></span>
<span><span class="co">#&gt; 1                    0                   0                    0</span></span>
<span><span class="co">#&gt; 2                    1                   0                    0</span></span>
<span><span class="co">#&gt; 3                    0                   0                    0</span></span>
<span><span class="co">#&gt; 4                    0                   1                    0</span></span>
<span><span class="co">#&gt; 5                    0                   0                    0</span></span></code></pre></div>
<p>(sometimes, like in Linear Models, the design matrix contains
<em>L-1</em> dummy variables. This kind of recoding can done with
<code><a href="https://rdrr.io/r/stats/model.matrix.html" class="external-link">model.matrix()</a></code>)</p>
<p>The approach using kernels is a bit different. Here we will work with
the original dataset. The kernel will make the pairwise comparisons of
the <em>N = 100</em> samples and, for every pair of samples, it will
ask: it this class equal in the two samples, or is it different? For
example: “Favorite.color” is “red” in sample 1, “black” in sample 2, and
“red” again in sample 3. The comparison of this categorical feature
between samples 1-2 will return FALSE, while comparing samples 1-3 will
return TRUE.</p>
<div class="sourceCode" id="cb30"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">KD</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/Dirac.html">Dirac</a></span><span class="op">(</span><span class="va">showdata</span>, comp<span class="op">=</span><span class="st">"sum"</span><span class="op">)</span></span>
<span><span class="fu"><a href="../reference/histK.html">histK</a></span><span class="op">(</span><span class="va">KD</span>, col <span class="op">=</span><span class="st">"plum2"</span><span class="op">)</span></span></code></pre></div>
<p><img src="kerntools_files/figure-html/unnamed-chunk-28-1.png" width="672"></p>
<p>As we have not a single categorical variable but <em>D=5</em>, we
should combine the results of this comparison for the <em>D</em>
variables. That’s why we stated <code>comp="sum"</code>: to make the
kernel sum the FALSES and TRUES of pairwise comparing “Favorite.color”,
“Favorite.actress”, “Favorite.actor”, “Favorite.show” and
“Liked.new.show” (we have also the option to average them, or compute a
weighting average, if we consider that some features are more important
than others). The histogram of <code>KD</code> shows there are a few
identical samples, a lot of samples that are completely different, and
most of samples only are equivalent in one of the 5 features.</p>
<p>Now we have our kernel matrix! Now we can do with it whatever we
want, included training a prediction model or computing a kernel PCA.
Yes, exactly: although PCA is a technique that was originally created
for real numeric data, another advantage yet of kernels is that we can
do PCA of everything. For simplicity, here we will not train the
prediction model (but you are free to follow the steps shown in the
previous section), and will only show the kernel PCA As
<code>showdata</code> contains the result of a (fictional) survey with
the idea to predict if people preferences could predict it they liked or
not a new show, this time, we are computing the kernel for features 1:4,
so we can color the kernel PCA with feature 5 (“Did you like the
show?”). Furthermore, for a change, we will also draw ellipses around
the centroids of the “Yes” group and the “No” group:</p>
<div class="sourceCode" id="cb31"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">KD</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/Dirac.html">Dirac</a></span><span class="op">(</span><span class="va">showdata</span><span class="op">[</span>,<span class="fl">1</span><span class="op">:</span><span class="fl">4</span><span class="op">]</span>, comp<span class="op">=</span><span class="st">"sum"</span>,feat_space<span class="op">=</span><span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="va">dirac_kpca</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/kPCA.html">kPCA</a></span><span class="op">(</span><span class="va">KD</span><span class="op">$</span><span class="va">K</span>,plot<span class="op">=</span><span class="fl">1</span><span class="op">:</span><span class="fl">2</span>,y<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/factor.html" class="external-link">factor</a></span><span class="op">(</span><span class="va">showdata</span><span class="op">$</span><span class="va">Liked.new.show</span><span class="op">)</span>,ellipse<span class="op">=</span><span class="fl">0.66</span>,title<span class="op">=</span><span class="st">"Dirac kernel PCA"</span><span class="op">)</span></span>
<span><span class="va">dirac_kpca</span><span class="op">$</span><span class="va">plot</span></span></code></pre></div>
<p><img src="kerntools_files/figure-html/unnamed-chunk-29-1.png" width="672"></p>
<p>It seems that the group that liked the show forms a tight cluster,
while people that do not liked it is scattered along the PC1. Now, we
can study the contributions of each class and see if our intuitions are
confirmed:</p>
<div class="sourceCode" id="cb32"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">pcs</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/kPCA_imp.html">kPCA_imp</a></span><span class="op">(</span><span class="va">KD</span><span class="op">$</span><span class="va">feat_space</span><span class="op">)</span></span>
<span><span class="co">#&gt; Do not use this function if the PCA was created with the RBF,</span></span>
<span><span class="co">#&gt;           Laplacian, Bray-Curtis, Jaccard/Ruzicka, or Kendall's tau kernels</span></span>
<span><span class="va">pc1</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/plotImp.html">plotImp</a></span><span class="op">(</span><span class="va">pcs</span><span class="op">$</span><span class="va">loadings</span><span class="op">[</span><span class="fl">1</span>,<span class="op">]</span>,leftmargin<span class="op">=</span><span class="fl">15</span>,nfeat<span class="op">=</span><span class="fl">10</span>,absolute <span class="op">=</span> <span class="cn">FALSE</span>,  relative <span class="op">=</span> <span class="cn">FALSE</span>,col <span class="op">=</span><span class="st">"bisque"</span><span class="op">)</span></span></code></pre></div>
<p><img src="kerntools_files/figure-html/unnamed-chunk-30-1.png" width="672"></p>
<div class="sourceCode" id="cb33"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">pc2</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/plotImp.html">plotImp</a></span><span class="op">(</span><span class="va">pcs</span><span class="op">$</span><span class="va">loadings</span><span class="op">[</span><span class="fl">2</span>,<span class="op">]</span>,leftmargin<span class="op">=</span><span class="fl">17</span>,nfeat<span class="op">=</span><span class="fl">10</span>,absolute <span class="op">=</span> <span class="cn">FALSE</span>, relative <span class="op">=</span> <span class="cn">FALSE</span>, col<span class="op">=</span><span class="st">"honeydew3"</span><span class="op">)</span></span></code></pre></div>
<p><img src="kerntools_files/figure-html/unnamed-chunk-30-2.png" width="672"></p>
<p>(These plots show only the top 10 features.)</p>
<p>The PC1 seems lead especially by Game of Thrones (and related
actors/actresses) to the right, and The Witcher (and related
actors/actresses) to the left, with a small contribution of the black
color. The PC2 (which seems more relevant than PC1) seems led for The
Witcher on one side, and color blue and the Squid Game on the other
(honorific mention here for Helena Bonham Carter). If we draw the
arrows:</p>
<div class="sourceCode" id="cb34"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">features</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sets.html" class="external-link">union</a></span><span class="op">(</span><span class="va">pc1</span><span class="op">$</span><span class="va">first_features</span>,<span class="va">pc2</span><span class="op">$</span><span class="va">first_features</span><span class="op">)</span></span>
<span><span class="fu"><a href="../reference/kPCA_arrows.html">kPCA_arrows</a></span><span class="op">(</span>plot<span class="op">=</span><span class="va">dirac_kpca</span><span class="op">$</span><span class="va">plot</span>,contributions<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/t.html" class="external-link">t</a></span><span class="op">(</span><span class="va">pcs</span><span class="op">$</span><span class="va">loadings</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">2</span>,<span class="va">features</span><span class="op">]</span><span class="op">)</span>,colour<span class="op">=</span><span class="st">"grey15"</span><span class="op">)</span></span></code></pre></div>
<p><img src="kerntools_files/figure-html/unnamed-chunk-31-1.png" width="672"></p>
<p>In summary, it would seem that a relevant fraction of these dataset
is divided between the Game of Thrones and The Witcher fans, but both of
them have one thing in common: they very clearly did
<strong>not</strong> like the new show.</p>
<p>In the previous code we computed not only the Dirac kernel matrix,
but we also computed its feature space (<code>feat_space=TRUE</code>).
Conveniently, the natural feature space where the Dirac kernel works is
the same generated by one-hot-encoding. That is what allowed us to
compute the contributions of each class (or dummy variable) to the PC1
and 2.</p>
</div>
<div class="section level3">
<h3 id="normalization-techniques">Normalization techniques<a class="anchor" aria-label="anchor" href="#normalization-techniques"></a>
</h3>
<p><code>kerntools</code> provides functions for some normalization
techniques. Several are specific of kernel matrices, other are for data
matrices in general, and some of them are for both.</p>
<p>First we will see the normalization of data matrices or data.frames.
One of the most used techniques is standardization, which we saw in
previous sections. This is already implemented in R base via
<code>scale(X,center=TRUE,scale=TRUE)</code>, which allows standardizing
(by column) or only centering or scaling (by column).
<code>kerntools</code> has the <code><a href="../reference/minmax.html">minmax()</a></code> normalization,
which normalizes the dataset between 0 and 1. The <code><a href="../reference/centerX.html">centerX()</a></code>
function centers a squared dataset by row or column (yes, only squared
datasets: to center by row a <em>NxD</em> dataset, you may use
<code>scale(t(X),center=T,scale=F)</code>). Another useful function is
<code><a href="../reference/TSS.html">TSS()</a></code>, which operates too by row or column, and transforms
absolute frequencies to relative ones, so the row (or column) sums to 1.
In this vein, <code><a href="../reference/cosnormX.html">cosnormX()</a></code> normalizes by the L2 norm by row
(or column): that is, the norm of each row (or column) sums to 1. These
functions usually default to row-normalization, as this is the way
kernel functions look at data (<code><a href="../reference/minmax.html">minmax()</a></code> is an exception).
Finally, normalization for the Frobenius norm is available for data
matrices (and also kernel matrices) via <code><a href="../reference/frobNorm.html">frobNorm()</a></code>.</p>
<p>Apart from the Frobenius normalization, <code>kerntools</code> has
two more normalization functions for kernel matrices:
<code><a href="../reference/cosNorm.html">cosNorm()</a></code> and <code><a href="../reference/centerK.html">centerK()</a></code>. The first one applies
the cosine normalization to a kernel matrix, so its maximum value is 1
(sometimes, this also bound the minimum value around 0). This operation
is related to <code><a href="../reference/cosnormX.html">cosnormX()</a></code>. In fact, when working with the
linear kernel (but only in that case!), these two operations are
equivalent:</p>
<div class="sourceCode" id="cb35"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">KL1</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/Linear.html">Linear</a></span><span class="op">(</span><span class="fu"><a href="../reference/cosnormX.html">cosnormX</a></span><span class="op">(</span><span class="va">iris</span><span class="op">[</span>,<span class="fl">1</span><span class="op">:</span><span class="fl">4</span><span class="op">]</span><span class="op">)</span><span class="op">)</span> <span class="co"># important: by row</span></span>
<span><span class="va">KL2</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/cosNorm.html">cosNorm</a></span><span class="op">(</span><span class="fu"><a href="../reference/Linear.html">Linear</a></span><span class="op">(</span><span class="va">iris</span><span class="op">[</span>,<span class="fl">1</span><span class="op">:</span><span class="fl">4</span><span class="op">]</span><span class="op">)</span><span class="op">)</span> <span class="co"># a third valid option is: Linear(iris[,1:4], cos.norm=TRUE)</span></span>
<span><span class="fu"><a href="../reference/simK.html">simK</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>KL1<span class="op">=</span><span class="va">KL1</span>,KL2<span class="op">=</span><span class="va">KL2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; Remember that Klist should contain only kernel matrices (i.e. squared, symmetric and PSD).</span></span>
<span><span class="co">#&gt;   This function does NOT verify the symmetry and PSD criteria.</span></span>
<span><span class="co">#&gt;     KL1 KL2</span></span>
<span><span class="co">#&gt; KL1   1   1</span></span>
<span><span class="co">#&gt; KL2   1   1</span></span></code></pre></div>
<p>The second function is <code><a href="../reference/centerK.html">centerK()</a></code> (needless to say, this
function is somewhat related to <code><a href="../reference/centerX.html">centerX()</a></code>). Again,
centering the dataset by column and then computing the linear kernel, or
computing the linear kernel and then centering it with
<code><a href="../reference/centerK.html">centerK()</a></code> is equivalent. Then, why have two duplicated ways
for doing the same? Well, apart of speed (which expression is faster
depends on the dataset dimension, that is, if it has more rows or
columns), these expressions are only equivalent when using the linear
kernel. When using another kernel, <code><a href="../reference/cosnormX.html">cosnormX()</a></code> and
<code><a href="../reference/centerK.html">centerK()</a></code> normalize or center the kernel matrix… according
to features in feature space. Not in the input space. For this reason,
this:</p>
<div class="sourceCode" id="cb36"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">center_iris</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/scale.html" class="external-link">scale</a></span><span class="op">(</span><span class="va">iris</span><span class="op">[</span>,<span class="fl">1</span><span class="op">:</span><span class="fl">4</span><span class="op">]</span>,scale<span class="op">=</span><span class="cn">FALSE</span>,center<span class="op">=</span><span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="fu"><a href="../reference/histK.html">histK</a></span><span class="op">(</span><span class="fu"><a href="../reference/RBF.html">RBF</a></span><span class="op">(</span><span class="va">center_iris</span>,g<span class="op">=</span><span class="fl">0.25</span><span class="op">)</span>,col<span class="op">=</span><span class="st">"aquamarine"</span><span class="op">)</span></span></code></pre></div>
<p><img src="kerntools_files/figure-html/unnamed-chunk-33-1.png" width="672"></p>
<p>and this:</p>
<div class="sourceCode" id="cb37"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/histK.html">histK</a></span><span class="op">(</span><span class="fu"><a href="../reference/centerK.html">centerK</a></span><span class="op">(</span><span class="fu"><a href="../reference/RBF.html">RBF</a></span><span class="op">(</span><span class="va">iris</span><span class="op">[</span>,<span class="fl">1</span><span class="op">:</span><span class="fl">4</span><span class="op">]</span>,g<span class="op">=</span><span class="fl">0.25</span><span class="op">)</span><span class="op">)</span>,col<span class="op">=</span><span class="st">"aquamarine3"</span><span class="op">)</span></span></code></pre></div>
<p><img src="kerntools_files/figure-html/unnamed-chunk-34-1.png" width="672"></p>
<p>don’t look alike in the slightest.</p>
<p>(Incidentally, it should be noted that RBF is translation-invariant
with respect to variables in input space. That is: standardization is
useful for this kernel, but simply centering is not.
<code>RBF(iris[,1:4],g=0.25)</code> and using <code>center_iris</code>
is the same.)</p>
<p>In summary: if we want that the two are equivalent (like in the
linear kernel), we should computing the L2 norm or the centering using
the feature space, <em>not</em> the input space. This cannot be done for
the <code><a href="../reference/RBF.html">RBF()</a></code> function, although it is possible for the kernels
stated in the previous subsection.</p>
<p>Up to this moment we have explained normalization for real data. But
what about the other kinds of data? Well, <code>kerntools</code> favors
the approach the other kinds of data are best dealt with kernel
functions. Also remember that some of the <code>kerntools</code> kernel
functions of the previous subsection will also return the feature space
on demand. Still, <code>kerntools</code> offers a basic one-hot-encoding
for categorical data. That is provided by <code><a href="../reference/dummy_data.html">dummy_data()</a></code>,
which converts a categorical dataset to a one-hot-encoded one. (This is
the feature space where the Dirac kernel works). This function requires
that the user specifies the number of levels per factor, but this can be
easily done with another function: <code><a href="../reference/dummy_var.html">dummy_var()</a></code>.</p>
</div>
<div class="section level3">
<h3 id="fusing-data--a-word-about-a-priori-and-a-posteriori-feature-importances-">Fusing data. A word about <em>a priori</em> and <em>a
posteriori</em> feature importances.<a class="anchor" aria-label="anchor" href="#fusing-data--a-word-about-a-priori-and-a-posteriori-feature-importances-"></a>
</h3>
<p>One advantage of using kernel matrices instead of original datasets
is that kernel matrices can be combined very easily. For instance,
imagine that we have two (or more) sources of data for the same
individuals. In our example, dataset1 is numeric and has dimension
<em>NxD<sub>1</sub></em>, while the dataset2 has dimension
<em>NxD<sub>2</sub></em> and contains other kind of data (for example
categorical). You cannot sum or multiply these two datasets; however,
you <em>can</em> sum or multiply their kernel matrices K1 and K2.</p>
<p>Let’s see a very simple illustration with the dataset
<code>mtcars</code>.</p>
<div class="sourceCode" id="cb38"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/dim.html" class="external-link">dim</a></span><span class="op">(</span><span class="va">mtcars</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 32 11</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">head</a></span><span class="op">(</span><span class="va">mtcars</span><span class="op">)</span></span>
<span><span class="co">#&gt;                    mpg cyl disp  hp drat    wt  qsec vs am gear carb</span></span>
<span><span class="co">#&gt; Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4</span></span>
<span><span class="co">#&gt; Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4</span></span>
<span><span class="co">#&gt; Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1</span></span>
<span><span class="co">#&gt; Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1</span></span>
<span><span class="co">#&gt; Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2</span></span>
<span><span class="co">#&gt; Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1</span></span></code></pre></div>
<p><code>mtcars</code> has 11 features: 9 are numeric while 2 can be
though as categorical: vs (engine, which can be V-shaped or straight),
and am (transmission: automatic or manual). We can split mtcars in two
parts: a data.frame with the numeric features and a data.frame with the
categorical ones:</p>
<div class="sourceCode" id="cb39"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">cat_feat_idx</span> <span class="op">&lt;-</span> <span class="fl">8</span><span class="op">:</span><span class="fl">9</span></span>
<span><span class="va">MTCARS</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>num<span class="op">=</span><span class="va">mtcars</span><span class="op">[</span>,<span class="op">-</span><span class="va">cat_feat_idx</span><span class="op">]</span>, cat<span class="op">=</span><span class="va">mtcars</span><span class="op">[</span>,<span class="va">cat_feat_idx</span><span class="op">]</span><span class="op">)</span></span></code></pre></div>
<p>Now we prepare a kernel matrix for each data.frame. For the dataset1
we use the linear kernel and for the dataset2 we use the Dirac
kernel:</p>
<div class="sourceCode" id="cb40"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">K</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/array.html" class="external-link">array</a></span><span class="op">(</span>dim<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">32</span>,<span class="fl">32</span>,<span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">K</span><span class="op">[</span>,,<span class="fl">1</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/Linear.html">Linear</a></span><span class="op">(</span><span class="va">MTCARS</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span><span class="op">)</span> <span class="co">## Kernel for numeric data</span></span>
<span><span class="va">K</span><span class="op">[</span>,,<span class="fl">2</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/Dirac.html">Dirac</a></span><span class="op">(</span><span class="va">MTCARS</span><span class="op">[[</span><span class="fl">2</span><span class="op">]</span><span class="op">]</span><span class="op">)</span> <span class="co">## Kernel for categorical data</span></span></code></pre></div>
<p>We can create a “consensus” kernel from K1 and K2 using a
<code>kerntools</code> function:</p>
<div class="sourceCode" id="cb41"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">Kcons</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/MKC.html">MKC</a></span><span class="op">(</span><span class="va">K</span><span class="op">)</span></span></code></pre></div>
<p>It should be noted that, here, <code>K1</code> has the same weight
than <code>K2</code> when computing the final kernel, although
<code>K1</code> has 9 variables and <code>K2</code> has only 2. If we
want to weight equally each one of the 11 variables in the final kernel,
we can do:</p>
<div class="sourceCode" id="cb42"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">coeff</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/lapply.html" class="external-link">sapply</a></span><span class="op">(</span><span class="va">MTCARS</span>,<span class="va">ncol</span><span class="op">)</span></span>
<span><span class="va">coeff</span> <span class="co">#  K1 will weight 9/11 and K2 2/11.</span></span>
<span><span class="co">#&gt; num cat </span></span>
<span><span class="co">#&gt;   9   2</span></span>
<span><span class="va">Kcons_var</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/MKC.html">MKC</a></span><span class="op">(</span><span class="va">K</span>,coeff<span class="op">=</span><span class="va">coeff</span><span class="op">)</span></span></code></pre></div>
<p>Now, maybe we fancy comparing <code>K1</code> and <code>K2</code> to
our consensus kernel matrices:</p>
<div class="sourceCode" id="cb43"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/simK.html">simK</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>Kcons<span class="op">=</span><span class="va">Kcons</span>,K1<span class="op">=</span><span class="va">K</span><span class="op">[</span>,,<span class="fl">1</span><span class="op">]</span>,K2<span class="op">=</span><span class="va">K</span><span class="op">[</span>,,<span class="fl">2</span><span class="op">]</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; Remember that Klist should contain only kernel matrices (i.e. squared, symmetric and PSD).</span></span>
<span><span class="co">#&gt;   This function does NOT verify the symmetry and PSD criteria.</span></span>
<span><span class="co">#&gt;           Kcons        K1        K2</span></span>
<span><span class="co">#&gt; Kcons 1.0000000 1.0000000 0.7871094</span></span>
<span><span class="co">#&gt; K1    1.0000000 1.0000000 0.7871069</span></span>
<span><span class="co">#&gt; K2    0.7871094 0.7871069 1.0000000</span></span>
<span><span class="fu"><a href="../reference/simK.html">simK</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>Kcons_var<span class="op">=</span><span class="va">Kcons_var</span>,K1<span class="op">=</span><span class="va">K</span><span class="op">[</span>,,<span class="fl">1</span><span class="op">]</span>,K2<span class="op">=</span><span class="va">K</span><span class="op">[</span>,,<span class="fl">2</span><span class="op">]</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; Remember that Klist should contain only kernel matrices (i.e. squared, symmetric and PSD).</span></span>
<span><span class="co">#&gt;   This function does NOT verify the symmetry and PSD criteria.</span></span>
<span><span class="co">#&gt;           Kcons_var        K1        K2</span></span>
<span><span class="co">#&gt; Kcons_var 1.0000000 1.0000000 0.7871074</span></span>
<span><span class="co">#&gt; K1        1.0000000 1.0000000 0.7871069</span></span>
<span><span class="co">#&gt; K2        0.7871074 0.7871069 1.0000000</span></span></code></pre></div>
<p>Mmm… something strange is happening here. Shouldn’t <code>K2</code>
be more similar to the consensus matrix in the former case than in the
latter? This phenomenon is caused because we did not normalize dataset1
nor <code>K1.</code> And then, we averaged <code>K1</code> and
<code>K2</code>, without taking into account that <code>K1</code> has
very large values in comparison to <code>K2</code>:</p>
<div class="sourceCode" id="cb44"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/histK.html">histK</a></span><span class="op">(</span><span class="va">K</span><span class="op">[</span>,,<span class="fl">1</span><span class="op">]</span>, col<span class="op">=</span><span class="st">"khaki1"</span><span class="op">)</span></span></code></pre></div>
<p><img src="kerntools_files/figure-html/unnamed-chunk-41-1.png" width="672"></p>
<div class="sourceCode" id="cb45"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/histK.html">histK</a></span><span class="op">(</span><span class="va">K</span><span class="op">[</span>,,<span class="fl">2</span><span class="op">]</span>, col<span class="op">=</span><span class="st">"hotpink"</span><span class="op">)</span></span></code></pre></div>
<p><img src="kerntools_files/figure-html/unnamed-chunk-41-2.png" width="672"></p>
<p>That is: we though that <code>K2</code> was overrepresented in the
consensus kernel… but actually it was the other way around.</p>
<p>In previous sections we have seen the feature importance given by SVM
models. That can be considered like some kind of <em>a posteriori</em>
feature importance. However, we should be cautious regarding the
implicit weights that we give to the features <em>before</em> training
the model. We can think about this like some kind of <em>a priori</em>
(my apologies to bayesian statisticians for this nomenclature!) feature
importance. It is important to have this implicit weighting in mind
because the SVM (or whatever other method we use) is <em>not</em> seeing
the original data, but our kernel matrices. In fact, this “scale”
problem arises in other (non-kernel) methods: for these reason we are
advised to normalize or standardize numeric datasets.</p>
<p>Now, this time <em>for real</em>, we try to weight equally all our 11
features:</p>
<div class="sourceCode" id="cb46"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">K</span><span class="op">[</span>,,<span class="fl">1</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/Linear.html">Linear</a></span><span class="op">(</span><span class="fu"><a href="../reference/minmax.html">minmax</a></span><span class="op">(</span><span class="va">MTCARS</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span><span class="op">)</span><span class="op">)</span> <span class="co">## Kernel for numeric data</span></span>
<span><span class="va">K</span><span class="op">[</span>,,<span class="fl">2</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/Dirac.html">Dirac</a></span><span class="op">(</span><span class="va">MTCARS</span><span class="op">[[</span><span class="fl">2</span><span class="op">]</span><span class="op">]</span>,comp<span class="op">=</span><span class="st">"sum"</span><span class="op">)</span> <span class="co">## Kernel for categorical data</span></span>
<span><span class="va">Kcons</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/MKC.html">MKC</a></span><span class="op">(</span><span class="va">K</span><span class="op">)</span></span>
<span><span class="va">Kcons_var</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/MKC.html">MKC</a></span><span class="op">(</span><span class="va">K</span>,coeff<span class="op">=</span><span class="va">coeff</span><span class="op">)</span></span>
<span><span class="fu"><a href="../reference/simK.html">simK</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>Kcons<span class="op">=</span><span class="va">Kcons</span>,K1<span class="op">=</span><span class="va">K</span><span class="op">[</span>,,<span class="fl">1</span><span class="op">]</span>,K2<span class="op">=</span><span class="va">K</span><span class="op">[</span>,,<span class="fl">2</span><span class="op">]</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; Remember that Klist should contain only kernel matrices (i.e. squared, symmetric and PSD).</span></span>
<span><span class="co">#&gt;   This function does NOT verify the symmetry and PSD criteria.</span></span>
<span><span class="co">#&gt;           Kcons        K1        K2</span></span>
<span><span class="co">#&gt; Kcons 1.0000000 0.9791777 0.9663366</span></span>
<span><span class="co">#&gt; K1    0.9791777 1.0000000 0.8939859</span></span>
<span><span class="co">#&gt; K2    0.9663366 0.8939859 1.0000000</span></span>
<span><span class="fu"><a href="../reference/simK.html">simK</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>Kcons<span class="op">=</span><span class="va">Kcons_var</span>,K1<span class="op">=</span><span class="va">K</span><span class="op">[</span>,,<span class="fl">1</span><span class="op">]</span>,K2<span class="op">=</span><span class="va">K</span><span class="op">[</span>,,<span class="fl">2</span><span class="op">]</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; Remember that Klist should contain only kernel matrices (i.e. squared, symmetric and PSD).</span></span>
<span><span class="co">#&gt;   This function does NOT verify the symmetry and PSD criteria.</span></span>
<span><span class="co">#&gt;           Kcons        K1        K2</span></span>
<span><span class="co">#&gt; Kcons 1.0000000 0.9977012 0.9222967</span></span>
<span><span class="co">#&gt; K1    0.9977012 1.0000000 0.8939859</span></span>
<span><span class="co">#&gt; K2    0.9222967 0.8939859 1.0000000</span></span></code></pre></div>
<p>The details maybe sound a bit too specific, but the min-max
normalization bounds the numeric data between 0 and 1. The range [0,1]
is the same than the one-hot-encoding for categorical variables (the
feature space related to the Dirac kernel). In addition, we chose
<code>comp=sum</code> because the linear kernel also “sums” each one of
the features. Now, both <code>K1</code> and <code>K2</code> are almost
equally similar to the consensus in the first case, and <code>K2</code>
is less similar in the second case. <code>K1</code> and <code>K2</code>
have still a high similarity, but this is probably caused because
features in <code>mtcars</code> are correlated.</p>
</div>
</div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by Elies Ramon.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.0.</p>
</div>

    </footer>
</div>





  </body>
</html>

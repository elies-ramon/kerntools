[{"path":"https://elies-ramon.github.io/kerntools/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"GNU General Public License","title":"GNU General Public License","text":"Version 3, 29 June 2007Copyright © 2007 Free Software Foundation, Inc. <http://fsf.org/> Everyone permitted copy distribute verbatim copies license document, changing allowed.","code":""},{"path":"https://elies-ramon.github.io/kerntools/LICENSE.html","id":"preamble","dir":"","previous_headings":"","what":"Preamble","title":"GNU General Public License","text":"GNU General Public License free, copyleft license software kinds works. licenses software practical works designed take away freedom share change works. contrast, GNU General Public License intended guarantee freedom share change versions program–make sure remains free software users. , Free Software Foundation, use GNU General Public License software; applies also work released way authors. can apply programs, . speak free software, referring freedom, price. General Public Licenses designed make sure freedom distribute copies free software (charge wish), receive source code can get want , can change software use pieces new free programs, know can things. protect rights, need prevent others denying rights asking surrender rights. Therefore, certain responsibilities distribute copies software, modify : responsibilities respect freedom others. example, distribute copies program, whether gratis fee, must pass recipients freedoms received. must make sure , , receive can get source code. must show terms know rights. Developers use GNU GPL protect rights two steps: (1) assert copyright software, (2) offer License giving legal permission copy, distribute /modify . developers’ authors’ protection, GPL clearly explains warranty free software. users’ authors’ sake, GPL requires modified versions marked changed, problems attributed erroneously authors previous versions. devices designed deny users access install run modified versions software inside , although manufacturer can . fundamentally incompatible aim protecting users’ freedom change software. systematic pattern abuse occurs area products individuals use, precisely unacceptable. Therefore, designed version GPL prohibit practice products. problems arise substantially domains, stand ready extend provision domains future versions GPL, needed protect freedom users. Finally, every program threatened constantly software patents. States allow patents restrict development use software general-purpose computers, , wish avoid special danger patents applied free program make effectively proprietary. prevent , GPL assures patents used render program non-free. precise terms conditions copying, distribution modification follow.","code":""},{"path":[]},{"path":"https://elies-ramon.github.io/kerntools/LICENSE.html","id":"id_0-definitions","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"0. Definitions","title":"GNU General Public License","text":"“License” refers version 3 GNU General Public License. “Copyright” also means copyright-like laws apply kinds works, semiconductor masks. “Program” refers copyrightable work licensed License. licensee addressed “”. “Licensees” “recipients” may individuals organizations. “modify” work means copy adapt part work fashion requiring copyright permission, making exact copy. resulting work called “modified version” earlier work work “based ” earlier work. “covered work” means either unmodified Program work based Program. “propagate” work means anything , without permission, make directly secondarily liable infringement applicable copyright law, except executing computer modifying private copy. Propagation includes copying, distribution (without modification), making available public, countries activities well. “convey” work means kind propagation enables parties make receive copies. Mere interaction user computer network, transfer copy, conveying. interactive user interface displays “Appropriate Legal Notices” extent includes convenient prominently visible feature (1) displays appropriate copyright notice, (2) tells user warranty work (except extent warranties provided), licensees may convey work License, view copy License. interface presents list user commands options, menu, prominent item list meets criterion.","code":""},{"path":"https://elies-ramon.github.io/kerntools/LICENSE.html","id":"id_1-source-code","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"1. Source Code","title":"GNU General Public License","text":"“source code” work means preferred form work making modifications . “Object code” means non-source form work. “Standard Interface” means interface either official standard defined recognized standards body, , case interfaces specified particular programming language, one widely used among developers working language. “System Libraries” executable work include anything, work whole, () included normal form packaging Major Component, part Major Component, (b) serves enable use work Major Component, implement Standard Interface implementation available public source code form. “Major Component”, context, means major essential component (kernel, window system, ) specific operating system () executable work runs, compiler used produce work, object code interpreter used run . “Corresponding Source” work object code form means source code needed generate, install, (executable work) run object code modify work, including scripts control activities. However, include work’s System Libraries, general-purpose tools generally available free programs used unmodified performing activities part work. example, Corresponding Source includes interface definition files associated source files work, source code shared libraries dynamically linked subprograms work specifically designed require, intimate data communication control flow subprograms parts work. Corresponding Source need include anything users can regenerate automatically parts Corresponding Source. Corresponding Source work source code form work.","code":""},{"path":"https://elies-ramon.github.io/kerntools/LICENSE.html","id":"id_2-basic-permissions","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"2. Basic Permissions","title":"GNU General Public License","text":"rights granted License granted term copyright Program, irrevocable provided stated conditions met. License explicitly affirms unlimited permission run unmodified Program. output running covered work covered License output, given content, constitutes covered work. License acknowledges rights fair use equivalent, provided copyright law. may make, run propagate covered works convey, without conditions long license otherwise remains force. may convey covered works others sole purpose make modifications exclusively , provide facilities running works, provided comply terms License conveying material control copyright. thus making running covered works must exclusively behalf, direction control, terms prohibit making copies copyrighted material outside relationship . Conveying circumstances permitted solely conditions stated . Sublicensing allowed; section 10 makes unnecessary.","code":""},{"path":"https://elies-ramon.github.io/kerntools/LICENSE.html","id":"id_3-protecting-users-legal-rights-from-anti-circumvention-law","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"3. Protecting Users’ Legal Rights From Anti-Circumvention Law","title":"GNU General Public License","text":"covered work shall deemed part effective technological measure applicable law fulfilling obligations article 11 WIPO copyright treaty adopted 20 December 1996, similar laws prohibiting restricting circumvention measures. convey covered work, waive legal power forbid circumvention technological measures extent circumvention effected exercising rights License respect covered work, disclaim intention limit operation modification work means enforcing, work’s users, third parties’ legal rights forbid circumvention technological measures.","code":""},{"path":"https://elies-ramon.github.io/kerntools/LICENSE.html","id":"id_4-conveying-verbatim-copies","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"4. Conveying Verbatim Copies","title":"GNU General Public License","text":"may convey verbatim copies Program’s source code receive , medium, provided conspicuously appropriately publish copy appropriate copyright notice; keep intact notices stating License non-permissive terms added accord section 7 apply code; keep intact notices absence warranty; give recipients copy License along Program. may charge price price copy convey, may offer support warranty protection fee.","code":""},{"path":"https://elies-ramon.github.io/kerntools/LICENSE.html","id":"id_5-conveying-modified-source-versions","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"5. Conveying Modified Source Versions","title":"GNU General Public License","text":"may convey work based Program, modifications produce Program, form source code terms section 4, provided also meet conditions: ) work must carry prominent notices stating modified , giving relevant date. b) work must carry prominent notices stating released License conditions added section 7. requirement modifies requirement section 4 “keep intact notices”. c) must license entire work, whole, License anyone comes possession copy. License therefore apply, along applicable section 7 additional terms, whole work, parts, regardless packaged. License gives permission license work way, invalidate permission separately received . d) work interactive user interfaces, must display Appropriate Legal Notices; however, Program interactive interfaces display Appropriate Legal Notices, work need make . compilation covered work separate independent works, nature extensions covered work, combined form larger program, volume storage distribution medium, called “aggregate” compilation resulting copyright used limit access legal rights compilation’s users beyond individual works permit. Inclusion covered work aggregate cause License apply parts aggregate.","code":""},{"path":"https://elies-ramon.github.io/kerntools/LICENSE.html","id":"id_6-conveying-non-source-forms","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"6. Conveying Non-Source Forms","title":"GNU General Public License","text":"may convey covered work object code form terms sections 4 5, provided also convey machine-readable Corresponding Source terms License, one ways: ) Convey object code , embodied , physical product (including physical distribution medium), accompanied Corresponding Source fixed durable physical medium customarily used software interchange. b) Convey object code , embodied , physical product (including physical distribution medium), accompanied written offer, valid least three years valid long offer spare parts customer support product model, give anyone possesses object code either (1) copy Corresponding Source software product covered License, durable physical medium customarily used software interchange, price reasonable cost physically performing conveying source, (2) access copy Corresponding Source network server charge. c) Convey individual copies object code copy written offer provide Corresponding Source. alternative allowed occasionally noncommercially, received object code offer, accord subsection 6b. d) Convey object code offering access designated place (gratis charge), offer equivalent access Corresponding Source way place charge. need require recipients copy Corresponding Source along object code. place copy object code network server, Corresponding Source may different server (operated third party) supports equivalent copying facilities, provided maintain clear directions next object code saying find Corresponding Source. Regardless server hosts Corresponding Source, remain obligated ensure available long needed satisfy requirements. e) Convey object code using peer--peer transmission, provided inform peers object code Corresponding Source work offered general public charge subsection 6d. separable portion object code, whose source code excluded Corresponding Source System Library, need included conveying object code work. “User Product” either (1) “consumer product”, means tangible personal property normally used personal, family, household purposes, (2) anything designed sold incorporation dwelling. determining whether product consumer product, doubtful cases shall resolved favor coverage. particular product received particular user, “normally used” refers typical common use class product, regardless status particular user way particular user actually uses, expects expected use, product. product consumer product regardless whether product substantial commercial, industrial non-consumer uses, unless uses represent significant mode use product. “Installation Information” User Product means methods, procedures, authorization keys, information required install execute modified versions covered work User Product modified version Corresponding Source. information must suffice ensure continued functioning modified object code case prevented interfered solely modification made. convey object code work section , , specifically use , User Product, conveying occurs part transaction right possession use User Product transferred recipient perpetuity fixed term (regardless transaction characterized), Corresponding Source conveyed section must accompanied Installation Information. requirement apply neither third party retains ability install modified object code User Product (example, work installed ROM). requirement provide Installation Information include requirement continue provide support service, warranty, updates work modified installed recipient, User Product modified installed. Access network may denied modification materially adversely affects operation network violates rules protocols communication across network. Corresponding Source conveyed, Installation Information provided, accord section must format publicly documented (implementation available public source code form), must require special password key unpacking, reading copying.","code":""},{"path":"https://elies-ramon.github.io/kerntools/LICENSE.html","id":"id_7-additional-terms","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"7. Additional Terms","title":"GNU General Public License","text":"“Additional permissions” terms supplement terms License making exceptions one conditions. Additional permissions applicable entire Program shall treated though included License, extent valid applicable law. additional permissions apply part Program, part may used separately permissions, entire Program remains governed License without regard additional permissions. convey copy covered work, may option remove additional permissions copy, part . (Additional permissions may written require removal certain cases modify work.) may place additional permissions material, added covered work, can give appropriate copyright permission. Notwithstanding provision License, material add covered work, may (authorized copyright holders material) supplement terms License terms: ) Disclaiming warranty limiting liability differently terms sections 15 16 License; b) Requiring preservation specified reasonable legal notices author attributions material Appropriate Legal Notices displayed works containing ; c) Prohibiting misrepresentation origin material, requiring modified versions material marked reasonable ways different original version; d) Limiting use publicity purposes names licensors authors material; e) Declining grant rights trademark law use trade names, trademarks, service marks; f) Requiring indemnification licensors authors material anyone conveys material (modified versions ) contractual assumptions liability recipient, liability contractual assumptions directly impose licensors authors. non-permissive additional terms considered “restrictions” within meaning section 10. Program received , part , contains notice stating governed License along term restriction, may remove term. license document contains restriction permits relicensing conveying License, may add covered work material governed terms license document, provided restriction survive relicensing conveying. add terms covered work accord section, must place, relevant source files, statement additional terms apply files, notice indicating find applicable terms. Additional terms, permissive non-permissive, may stated form separately written license, stated exceptions; requirements apply either way.","code":""},{"path":"https://elies-ramon.github.io/kerntools/LICENSE.html","id":"id_8-termination","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"8. Termination","title":"GNU General Public License","text":"may propagate modify covered work except expressly provided License. attempt otherwise propagate modify void, automatically terminate rights License (including patent licenses granted third paragraph section 11). However, cease violation License, license particular copyright holder reinstated () provisionally, unless copyright holder explicitly finally terminates license, (b) permanently, copyright holder fails notify violation reasonable means prior 60 days cessation. Moreover, license particular copyright holder reinstated permanently copyright holder notifies violation reasonable means, first time received notice violation License (work) copyright holder, cure violation prior 30 days receipt notice. Termination rights section terminate licenses parties received copies rights License. rights terminated permanently reinstated, qualify receive new licenses material section 10.","code":""},{"path":"https://elies-ramon.github.io/kerntools/LICENSE.html","id":"id_9-acceptance-not-required-for-having-copies","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"9. Acceptance Not Required for Having Copies","title":"GNU General Public License","text":"required accept License order receive run copy Program. Ancillary propagation covered work occurring solely consequence using peer--peer transmission receive copy likewise require acceptance. However, nothing License grants permission propagate modify covered work. actions infringe copyright accept License. Therefore, modifying propagating covered work, indicate acceptance License .","code":""},{"path":"https://elies-ramon.github.io/kerntools/LICENSE.html","id":"id_10-automatic-licensing-of-downstream-recipients","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"10. Automatic Licensing of Downstream Recipients","title":"GNU General Public License","text":"time convey covered work, recipient automatically receives license original licensors, run, modify propagate work, subject License. responsible enforcing compliance third parties License. “entity transaction” transaction transferring control organization, substantially assets one, subdividing organization, merging organizations. propagation covered work results entity transaction, party transaction receives copy work also receives whatever licenses work party’s predecessor interest give previous paragraph, plus right possession Corresponding Source work predecessor interest, predecessor can get reasonable efforts. may impose restrictions exercise rights granted affirmed License. example, may impose license fee, royalty, charge exercise rights granted License, may initiate litigation (including cross-claim counterclaim lawsuit) alleging patent claim infringed making, using, selling, offering sale, importing Program portion .","code":""},{"path":"https://elies-ramon.github.io/kerntools/LICENSE.html","id":"id_11-patents","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"11. Patents","title":"GNU General Public License","text":"“contributor” copyright holder authorizes use License Program work Program based. work thus licensed called contributor’s “contributor version”. contributor’s “essential patent claims” patent claims owned controlled contributor, whether already acquired hereafter acquired, infringed manner, permitted License, making, using, selling contributor version, include claims infringed consequence modification contributor version. purposes definition, “control” includes right grant patent sublicenses manner consistent requirements License. contributor grants non-exclusive, worldwide, royalty-free patent license contributor’s essential patent claims, make, use, sell, offer sale, import otherwise run, modify propagate contents contributor version. following three paragraphs, “patent license” express agreement commitment, however denominated, enforce patent (express permission practice patent covenant sue patent infringement). “grant” patent license party means make agreement commitment enforce patent party. convey covered work, knowingly relying patent license, Corresponding Source work available anyone copy, free charge terms License, publicly available network server readily accessible means, must either (1) cause Corresponding Source available, (2) arrange deprive benefit patent license particular work, (3) arrange, manner consistent requirements License, extend patent license downstream recipients. “Knowingly relying” means actual knowledge , patent license, conveying covered work country, recipient’s use covered work country, infringe one identifiable patents country reason believe valid. , pursuant connection single transaction arrangement, convey, propagate procuring conveyance , covered work, grant patent license parties receiving covered work authorizing use, propagate, modify convey specific copy covered work, patent license grant automatically extended recipients covered work works based . patent license “discriminatory” include within scope coverage, prohibits exercise , conditioned non-exercise one rights specifically granted License. may convey covered work party arrangement third party business distributing software, make payment third party based extent activity conveying work, third party grants, parties receive covered work , discriminatory patent license () connection copies covered work conveyed (copies made copies), (b) primarily connection specific products compilations contain covered work, unless entered arrangement, patent license granted, prior 28 March 2007. Nothing License shall construed excluding limiting implied license defenses infringement may otherwise available applicable patent law.","code":""},{"path":"https://elies-ramon.github.io/kerntools/LICENSE.html","id":"id_12-no-surrender-of-others-freedom","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"12. No Surrender of Others’ Freedom","title":"GNU General Public License","text":"conditions imposed (whether court order, agreement otherwise) contradict conditions License, excuse conditions License. convey covered work satisfy simultaneously obligations License pertinent obligations, consequence may convey . example, agree terms obligate collect royalty conveying convey Program, way satisfy terms License refrain entirely conveying Program.","code":""},{"path":"https://elies-ramon.github.io/kerntools/LICENSE.html","id":"id_13-use-with-the-gnu-affero-general-public-license","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"13. Use with the GNU Affero General Public License","title":"GNU General Public License","text":"Notwithstanding provision License, permission link combine covered work work licensed version 3 GNU Affero General Public License single combined work, convey resulting work. terms License continue apply part covered work, special requirements GNU Affero General Public License, section 13, concerning interaction network apply combination .","code":""},{"path":"https://elies-ramon.github.io/kerntools/LICENSE.html","id":"id_14-revised-versions-of-this-license","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"14. Revised Versions of this License","title":"GNU General Public License","text":"Free Software Foundation may publish revised /new versions GNU General Public License time time. new versions similar spirit present version, may differ detail address new problems concerns. version given distinguishing version number. Program specifies certain numbered version GNU General Public License “later version” applies , option following terms conditions either numbered version later version published Free Software Foundation. Program specify version number GNU General Public License, may choose version ever published Free Software Foundation. Program specifies proxy can decide future versions GNU General Public License can used, proxy’s public statement acceptance version permanently authorizes choose version Program. Later license versions may give additional different permissions. However, additional obligations imposed author copyright holder result choosing follow later version.","code":""},{"path":"https://elies-ramon.github.io/kerntools/LICENSE.html","id":"id_15-disclaimer-of-warranty","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"15. Disclaimer of Warranty","title":"GNU General Public License","text":"WARRANTY PROGRAM, EXTENT PERMITTED APPLICABLE LAW. EXCEPT OTHERWISE STATED WRITING COPYRIGHT HOLDERS /PARTIES PROVIDE PROGRAM “” WITHOUT WARRANTY KIND, EITHER EXPRESSED IMPLIED, INCLUDING, LIMITED , IMPLIED WARRANTIES MERCHANTABILITY FITNESS PARTICULAR PURPOSE. ENTIRE RISK QUALITY PERFORMANCE PROGRAM . PROGRAM PROVE DEFECTIVE, ASSUME COST NECESSARY SERVICING, REPAIR CORRECTION.","code":""},{"path":"https://elies-ramon.github.io/kerntools/LICENSE.html","id":"id_16-limitation-of-liability","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"16. Limitation of Liability","title":"GNU General Public License","text":"EVENT UNLESS REQUIRED APPLICABLE LAW AGREED WRITING COPYRIGHT HOLDER, PARTY MODIFIES /CONVEYS PROGRAM PERMITTED , LIABLE DAMAGES, INCLUDING GENERAL, SPECIAL, INCIDENTAL CONSEQUENTIAL DAMAGES ARISING USE INABILITY USE PROGRAM (INCLUDING LIMITED LOSS DATA DATA RENDERED INACCURATE LOSSES SUSTAINED THIRD PARTIES FAILURE PROGRAM OPERATE PROGRAMS), EVEN HOLDER PARTY ADVISED POSSIBILITY DAMAGES.","code":""},{"path":"https://elies-ramon.github.io/kerntools/LICENSE.html","id":"id_17-interpretation-of-sections-15-and-16","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"17. Interpretation of Sections 15 and 16","title":"GNU General Public License","text":"disclaimer warranty limitation liability provided given local legal effect according terms, reviewing courts shall apply local law closely approximates absolute waiver civil liability connection Program, unless warranty assumption liability accompanies copy Program return fee. END TERMS CONDITIONS","code":""},{"path":"https://elies-ramon.github.io/kerntools/LICENSE.html","id":"how-to-apply-these-terms-to-your-new-programs","dir":"","previous_headings":"","what":"How to Apply These Terms to Your New Programs","title":"GNU General Public License","text":"develop new program, want greatest possible use public, best way achieve make free software everyone can redistribute change terms. , attach following notices program. safest attach start source file effectively state exclusion warranty; file least “copyright” line pointer full notice found. Also add information contact electronic paper mail. program terminal interaction, make output short notice like starts interactive mode: hypothetical commands show w show c show appropriate parts General Public License. course, program’s commands might different; GUI interface, use “box”. also get employer (work programmer) school, , sign “copyright disclaimer” program, necessary. information , apply follow GNU GPL, see <http://www.gnu.org/licenses/>. GNU General Public License permit incorporating program proprietary programs. program subroutine library, may consider useful permit linking proprietary applications library. want , use GNU Lesser General Public License instead License. first, please read <http://www.gnu.org/philosophy/--lgpl.html>.","code":"<one line to give the program's name and a brief idea of what it does.> Copyright (C) <year>  <name of author>  This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.  This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more details.  You should have received a copy of the GNU General Public License along with this program.  If not, see <http://www.gnu.org/licenses/>. <program>  Copyright (C) <year>  <name of author> This program comes with ABSOLUTELY NO WARRANTY; for details type 'show w'. This is free software, and you are welcome to redistribute it under certain conditions; type 'show c' for details."},{"path":"https://elies-ramon.github.io/kerntools/articles/kerntools.html","id":"purpose","dir":"Articles","previous_headings":"","what":"Purpose","title":"kerntools: R tools for kernel methods","text":"kerntools provides R tools working family Machine Learning methods called kernel methods. package implements several kernel functions treating nonnegative real vectors, real matrices, categorical ordinal variables, sets, strings. Several tools studying resulting kernel matrix compare two kernel matrices available. diagnostic tools may used infer kernel(s) matrix(ces) suitability model training. kerntools also provides functions extracting feature importance Support Vector Machines (SVMs) displaying customizable kernel Principal Components Analysis (PCA) plots. convenience, widespread model’s performance measures feature importance visualization available user.","code":""},{"path":"https://elies-ramon.github.io/kerntools/articles/kerntools.html","id":"loading","dir":"Articles","previous_headings":"","what":"Loading","title":"kerntools: R tools for kernel methods","text":"package installed, can loaded anytime typing:","code":"library(kerntools)"},{"path":"https://elies-ramon.github.io/kerntools/articles/kerntools.html","id":"package-overview","dir":"Articles","previous_headings":"","what":"Package Overview","title":"kerntools: R tools for kernel methods","text":"section several examples used illustrate typical workflow kerntools.","code":""},{"path":"https://elies-ramon.github.io/kerntools/articles/kerntools.html","id":"a-simple-example","dir":"Articles","previous_headings":"Package Overview","what":"A simple example","title":"kerntools: R tools for kernel methods","text":"Let’s suppose working well known iris dataset. dataset contains sepal petal measurements N = 150 iris flowers. flowers belong three different species: Iris setosa, Iris versicolor, Iris virginica. D = 4 numeric variables (also called “features”) measured: Sepal.Length, Sepal.Width, Petal.Length Petal.Width. first standardize four variables mean 0 standard deviation 1. places common ground. , compute first kernel: linear kernel. linear kernel simply pairwise inner product N samples (case: flower measurements). result “stored” matrix (kernel matrix) dimensions NxN. Furthermore, always symmetric positive semi-definite (PSD). check kernel two samples (instance, 32 106), can type either KL[32,106] KL[106,32]. noted kernel matrices generated kerntools class “matrix”, “array”. fact, keep things simple, function package requires special classes/objects present base R. Next, examine kernel matrix . Although excessively large, large enough make simply typing KL R unpractical. Instead, may summarize values using histogram:  “almost-gaussian” shape due features’ scaling. parameters, including color, can passed histK() (info, check documentation graphics::hist()). von Neumann entropy shown optional (fact, value can computed separately vonNeumann(KL)). Entropy values close 0 indicate kernel matrix elements similar, values close 1 indicate high variability. Another possibility visualize whole kernel matrix heatmap:  , yellow denotes high similarity samples, red denotes similarity low (colour palette customizable via parameter color). glance see first 50 samples (. setosa) higher intra-group similarity. Also, different samples 101-150 (correspond . virginica). Instead, . versicolor kind intermediate two groups. confirm intuitions (dis)similarity among three species, may proceed widespread ordination method: Principal Components Analysis (PCA). PCAs can computed kernel matrices easily. fact, using kernel matrices expands PCA can , discussed sections. display beautiful PCA plot colors samples species, :  Indeed, can see . setosa . virginica different groups, . versicolor .virginica close. colors can changed desired kPCA(...,colors) parameter. seeing plot, can infer predictive model data work well. Although ton machine learning methods disposal, vignette stick kernel methods. specifically, use famous kernel method: Support Vector Machine (SVM). SVMs implemented kerntools. However, R packages like kernlab e1071. use ksvm() function former package: First foremost: prediction models, mandatory additional test set honest estimation model’s performance can computed (latter). Also, please note real-world machine learning setting, training data preprocessed first exact preprocessing applied test data. case, preprocessing standardize dataset: thus, mean standard deviation computed training data, values used standardize training test sets. said, order interrupt flow vignette, use leave things . Now returning (questionably obtained) model, low training error. support vectors (samples relevant us, rest used define SVM discriminating hyperplane) constitute 22% (approx) samples. bad. jumping test set, may interested another topic: feature importance. means studying variables considered important model discriminating classes. Feature importance important avoiding “black-box models”: prediction models know work well, . Obtaining importances SVM model can somewhat convoluted (discussed later depth) sometimes downright impossible. particular case, problem wanted classify 3 classes (species)… SVM classifiers binary. discriminating 3 classes, kernlab fact builds 3 classifiers: “setosa vs versicolor”, “setosa vs virginica”, “versicolor vs virginica”. 3 classifiers constitute linear_model object prediction class sample done voting scheme. simplify things, features’ importance part, focus third classifier: “versicolor vs virginica”, seen previously two related species. way go obtain index Support Vectors model, coefficients. gracefully provided kernlab. , return kerntools call function svm_imp(): Note need data used compute KL: iris_std. important use version dataset, version pre-processing (“original” without pre-processing). svm_imp() parameters like center, scale cos.norm take widespread normalization techniques account, better play safe. Conveniently, kerntools provides function visualize features’ importance:  can see, model considers petals discriminating sepals, within petals’ measures, petal length. return PCA, also check weight variable first second PCs (, ones displayed). , comes handy kPCA_imp() function. , function requires dataset generated KL matrix: seems first component dominated Petal Length Petal Width also Sepal Length. second component, plays role discriminating . versicolor .virginica, dominated Sepal Width. PCA disagrees bit SVM feature importances, remember latter focused “versicolor vs virginica” problem, former looking ordination three classes. may represent contributions four features 1st PC, make things easier include 2nd PC onto barplot:  used absolute=FALSE contribution variable PC relevant magnitude, also sign. Pink bars represent PC1, black line represents PC2 (parameter y). wanted see order relative magnitude, X axis show relative contribution (plotImp() function, relative=TRUE default). kerntools, can draw contributions PCA plot arrows. need PCA plot (given kPCA()) contributions (given kPCA_imp()):  (Note arrows scaled match original PCA plot. somewhat orientative: directions correct, longer arrows represent greater contribution PC shorter arrows; however, usually arrows’ magnitudes coincide actual magnitudes can computed kPCA_imp()). now, finally, going check performance test set (considering 3 classes). , subset KL suitable test x training matrix, input matrix linear_model, compare predicted species actual species: Mmmm… maybe bit overconfident. seems model ignored . setosa completely. can compute numerically “good” (wrong) model according different performance measures implemented kerntools. dealing classification, need contingency table contrasts actual predicted classes (also known “confusion matrix”). simple measure accuracy: number right predictions divided number predictions (test set size: case, 30). expected, accuracy overwhelmingly low. can compare result accuracy random model (according class distribution test): 0.35, means model performs lot worse someone classifying random. can explore measures infer problems prediction model (instance, species systematically missclassified, etc.). example, can compute measures class: (case want overall performance measure, can compute mean three classes, type multi.class=\"macro\"). precision measure tell us none samples predicted “virginica” “setosa” correct (case “setosa”, none predicted), (1/7) predicted “versicolor” right. recall shows 3/8 “versicolor” samples test correctly classified “versicolor”, none “setosa” “virginica.” F1 useful gives “mean” Precision Recall. Meanwhile, low specificity “versicolor” points lot samples “versicolor” predicted .","code":"iris_std <- scale(iris[,c( \"Sepal.Length\",\"Sepal.Width\",\"Petal.Length\", \"Petal.Width\")]) KL <- Linear(iris_std) dim(KL) #> [1] 150 150 class(KL) #> [1] \"matrix\" \"array\" histK(KL, vn = TRUE) heatK(KL,cos.norm = FALSE) iris_species <- factor(iris$Species) kpca <- kPCA(KL,plot = 1:2,y = iris_species) kpca$plot library(kernlab) set.seed(777) ## Training data test_idx <- sample(1:150)[1:30] # 20% of samples train_idx <- (1:150)[-test_idx] KL_train <- KL[train_idx,train_idx] ## Model (training data) linear_model <- kernlab::ksvm(x=KL_train, y=iris_species[train_idx], kernel=\"matrix\") linear_model #> Support Vector Machine object of class \"ksvm\"  #>  #> SV type: C-svc  (classification)  #>  parameter : cost C = 1  #>  #> [1] \" Kernel matrix used as input.\" #>  #> Number of Support Vectors : 27  #>  #> Objective Function Value : -0.9459 -0.3184 -14.3709  #> Training error : 0.025 ## Third model: Versicolor vs virginica sv_index <- kernlab::alphaindex(linear_model)[[3]] # Vector with the SV indices sv_coef <- kernlab::coef(linear_model)[[3]]  # Vector with the SV coefficients  feat_imp3 <- svm_imp(X=iris_std[train_idx,],svindx=sv_index,coeff=sv_coef) #> Do not use this function if the SVM model was created with the RBF, #>           Laplacian, Bray-Curtis, Jaccard/Ruzicka, or Kendall's tau kernels plotImp(feat_imp3, leftmargin = 7, main=\"3rd model: versicolor vs virginica\", color=\"steelblue1\") #> $first_features #> [1] \"Petal.Length\" \"Petal.Width\"  \"Sepal.Width\"  \"Sepal.Length\" #>  #> $cumsum #> [1] 1 #>  #> $barplot #>      [,1] #> [1,]  0.7 #> [2,]  1.9 #> [3,]  3.1 #> [4,]  4.3 loadings <- kPCA_imp(iris_std) #> Do not use this function if the PCA was created with the RBF, #>           Laplacian, Bray-Curtis, Jaccard/Ruzicka, or Kendall's tau kernels pcs <- loadings$loadings[1:2,] pcs #>     Sepal.Length Sepal.Width Petal.Length Petal.Width #> PC1    0.5210659  -0.2693474   0.58041310  0.56485654 #> PC2    0.3774176   0.9232957   0.02449161  0.06694199 plotImp(pcs[1,], y=pcs[2,], ylegend=\"PC2\",absolute=FALSE, main=\"PC1\", leftmargin = 7,  color=\"rosybrown1\") #> $first_features #> [1] \"Petal.Length\" \"Petal.Width\"  \"Sepal.Length\" \"Sepal.Width\"  #>  #> $cumsum #> [1] 1 #>  #> $barplot #>      [,1] #> [1,]  0.7 #> [2,]  1.9 #> [3,]  3.1 #> [4,]  4.3 kPCA_arrows(plot=kpca$plot,contributions=t(pcs),colour=\"grey15\") KL_test <- as.kernelMatrix(KL[test_idx,train_idx]) ## Prediction (test data) pred_class <- predict(linear_model,KL_test) actual_class <- iris_species[test_idx] pred_class #>  [1] versicolor versicolor virginica  virginica  versicolor virginica  #>  [7] virginica  virginica  versicolor versicolor versicolor versicolor #> [13] versicolor versicolor versicolor versicolor virginica  versicolor #> [19] versicolor versicolor virginica  versicolor virginica  versicolor #> [25] versicolor virginica  versicolor versicolor versicolor versicolor #> Levels: setosa versicolor virginica actual_class #>  [1] setosa     setosa     versicolor versicolor setosa     setosa     #>  [7] setosa     versicolor setosa     setosa     versicolor virginica  #> [13] virginica  virginica  virginica  virginica  setosa     versicolor #> [19] setosa     setosa     setosa     setosa     versicolor setosa     #> [25] virginica  versicolor virginica  virginica  virginica  versicolor #> Levels: setosa versicolor virginica ct <- table(actual_class,pred_class) # Confusion matrix ct #>             pred_class #> actual_class setosa versicolor virginica #>   setosa          0          9         4 #>   versicolor      0          3         5 #>   virginica       0          9         0 Acc(ct) ## Accuracy #> [1] 0.1 Acc_rnd(actual_class) ## Accuracy of the random model #> [1] 0.3488889 Prec(ct,multi.class = \"none\") ## Precision or Positive Predictive Value #>     setosa versicolor  virginica  #>  0.0000000  0.1428571  0.0000000 Rec(ct,multi.class = \"none\") ## Recall or True Positive Rate #>     setosa versicolor  virginica  #>      0.000      0.375      0.000 Spe(ct,multi.class = \"none\") ## Specificity or True Negative Rate #>     setosa versicolor  virginica  #>  1.0000000  1.0000000  0.5714286 F1(ct,multi.class = \"none\") ## F1 (harmonic mean of Precision and Recall) #>     setosa versicolor  virginica  #>  0.0000000  0.2068966  0.0000000"},{"path":"https://elies-ramon.github.io/kerntools/articles/kerntools.html","id":"a-slightly-more-complicated-example","dir":"Articles","previous_headings":"Package Overview","what":"A (slightly) more complicated example","title":"kerntools: R tools for kernel methods","text":"previous section picked naively first model train, --great results. , going complicate things bit hoping obtain model works. (Note: course, iris flower classification simple task. fact, can achieved pretty decently linear kernel, can deduced previous PCA: linear classifier enough discriminate flowers using 1st 2nd PCs. However, sake example, use different kernel present section). radial basis function (RBF) kernel something like “gold standard” among kernels. Unlike linear kernel (simple “plain” kernel), nonlinear: fact, RBF kernel universal approximator. can compute :  Kernel matrix values typically 0 1. linear kernel required dataset, RBF() (hyper)parameter called gamma (g short). value hyperparameter decided us, important decision, affect decision boundary kernel. Fortunately, heuristics estimate good gamma exist. kerntools implement three , available function estimate_gamma(): previous histogram visualized RBF gamma given “d_criterion” (almost one given “scale criterion”). third heuristic gives us distribution “good” gamma values. Now, sake comparison, compute RBF kernel using median “quantiles_criterion”:  histogram changes: von Neumann entropy changes well. important remark RBF kernel sensitive gamma values. higher entropy respect linear kernel reflects , , higher variability kernel matrix values. (can also deduced comparing histograms. Conversely, heatK(Krbf), observe extreme values/colors ). paper recommends entropy 0.3-0.5, maybe reflected SVM model’s performance? Now, can also kernel PCA. previous kernel PCA used linear kernel , reality, identical “normal” PCA. time however using different kernel now can actually say kernel PCA. main difference projection samples going linear. Sometimes, creates strange patterns difficult interpret. later going train SVM model, may occur us great PCA training samples, can compare prediction model PCA side side. , use training indices previous section. Even better: compute (kernel) PCA training samples, project test samples ?  (Note: remember , real-world problem, standardization dataset done center std deviation training set.) Said done! However, now patterns kernel PCA bit… radial. Still, . setosa one side, . versicolor . virginica . red, green blue samples training samples, grey samples correspond test samples projected posteriori (color can specified kPCA parameter na_col). now? generated one kernel matrix data (thanks linear RBF kernels), may compare matrices. , can use kerntools function called simK: simK first remind us matrix several mathematical properties kernel matrix. work kernel matrices generated kerntools (: Linear(), RBF(), etc.) everything alright. However, can come kerntools precomputed kernel matrices (long class “matrix”, “array”). kerntools implicitly trusts user knows /, remember using proper kernel matrices. simK returns score 0 1: 1 complete similarity, 0 complete dissimilarity. can see two RBF matrices similar, linear kernel matrix around 50% similar RBF matrices. also compare two PCAs. option computing RV coefficient (Co-Inertia Analysis). However, RV coefficient rbf_kpca kpca give result simK(list(KL,Krbf)). noted equivalence holds dataset centered beforehand, PCAs usually computed using centered data (reason kPCA(..., center=TRUE) default). kernel matrix obtained non-centered data, can centered afterwards centerK() (later sections). Another way compare two PCAs called Procrustes Analysis. analysis compares correlation two projections “removing” translation, scale rotation effects. Although properly kernel method, kerntools can basic Procrustes Analysis. data, moderate Procrustes correlation: 0.68 (correlation coefficient bounded 0 1). (moment good show kPCA() can return kernel PCA projection without displaying plot. case, graphical parameters like colors, labels, etc. ignored.) , train brand new (hopefully better) model. re-use training test samples: low training error, now wiser. performance test? Wow, seems lot better! However, get excited, must remember point estimation accuracy, comes specific test set (30 samples chose randomly previous section). Another test set surely deliver different accuracy. tried compute confidence interval (CI) idea test sets behave? kerntools provides two ways obtain CI: Normal Approximation, bootstrapping. Normal approximation quicker, bootstrapping usually considered safer (details: ): functions default 95% CI, can changed via confidence parameter. According normal approximation, accuracy 0.5 (0.32, 0.68), according bootstrap strategy, 0.5 (0.33, 0.66). CI wide test small (30 samples). However, test CI (95% confidence) really assure model really different random model, accuracy 0.35 (computed previous section). Useful reminder: next time, choose larger test set. call day, going compute performance measures. time, compute class class, average (“macro” approach): desire , can compute CI values: instance, bootstrap macro-F1 value, simply type index = \"f1\" Boots_CI() function. case, congratulate performance clearly higher last time. Training seriously machine learning model involves fine hyperparameter tuning (remember C ksvm()?) almost completely skipped. : use strategy like, say, grid search, compare performance measures hyperparameter combination via cross validation, far beyond purposes vignette (kerntools). Finally, warning computing feature importances SVM /feature contribution PCs PCA. words: don’t using RBF kernel. precisely: kernel functions implemented , ever try recover feature contributions RBF(), Laplace(), Jaccard(), Ruzicka(), BrayCurtis() Kendall() unless know well . type something like: something computed, result wrong. ignore warning raised . right , ultimately, kernels behave like linear kernel: compute inner product features. features? question. hood, kernels “send” original features (feature live input space) space (usually higher-dimensional) called feature space, compute inner product . kernel conflates two steps one, usually simplifies lot calculations saves lot memory space: called “kernel trick”. compute analytically feature contributions need go feature space. make things worse, feature space kernel implicitly using depends things like dimensionality input data, kind kernel, specific value hyperparameters, etc. Going feature space trivial linear kernel: input space = feature space. Instead, RBF kernel, feature space infinite dimensional. techniques estimate exist (see instance: Explicit Approximations Gaussian Kernel), yet implemented kerntools (maybe never ).","code":"Krbf <- RBF(iris_std,g=0.25) histK(Krbf,col=\"aquamarine\",vn = TRUE) estimate_gamma(iris_std) #> $d_criterion #> [1] 0.25 #>  #> $scale_criterion #> [1] 0.2512584 #>  #> $quantiles_criterion #>        90%        50%        10%  #> 0.05570343 0.16670322 1.73533468 Krbf2 <- RBF(iris_std,g=0.1667) histK(Krbf2,col=\"darkseagreen1\",vn=TRUE) Krbf_train <- Krbf2[train_idx,train_idx] Krbf_test <- Krbf2[test_idx,train_idx] rbf_kpca <- kPCA(K=Krbf_train, Ktest=Krbf_test, plot = 1:2, y = iris_species[train_idx], title = \"RBF PCA\") rbf_kpca$plot simK(list(linear=KL,rbf_0.166=Krbf, rbf_0.25=Krbf2)) #> Remember that Klist should contain only kernel matrices (i.e. squared, symmetric and PSD). #>   This function does NOT verify the symmetry and PSD criteria. #>              linear rbf_0.166  rbf_0.25 #> linear    1.0000000 0.5208955 0.4803192 #> rbf_0.166 0.5208955 1.0000000 0.9898203 #> rbf_0.25  0.4803192 0.9898203 1.0000000 rbf_kpca <- kPCA(K=Krbf) proc <- Procrustes(kpca$projection,rbf_kpca) proc$pro.cor # Procrustes correlation #> [1] 0.6862007 ####### Model (training data) model <- kernlab::ksvm(x=Krbf_train, y=iris_species[train_idx], kernel=\"matrix\", C=10) model #> Support Vector Machine object of class \"ksvm\"  #>  #> SV type: C-svc  (classification)  #>  parameter : cost C = 10  #>  #> [1] \" Kernel matrix used as input.\" #>  #> Number of Support Vectors : 30  #>  #> Objective Function Value : -4.1707 -2.7089 -92.348  #> Training error : 0.008333 Krbf_test <- as.kernelMatrix(Krbf_test) ####### Prediction (test data) pred_class <- predict(model,Krbf_test) actual_class <- iris_species[test_idx] ct <- table(actual_class,pred_class) # Confusion matrix Acc(ct) ## Accuracy #> [1] 0.5 ## Accuracy CI (95%) Normal_CI(value = 0.5,ntest = 30) ## Accuracy CI (95%) #> [1] 0.3210806 0.6789194 Boots_CI(target = actual_class, pred = pred_class, nboots = 2000,index = \"acc\")  #>                2.5%     97.5%  #> 0.5016667 0.3333333 0.7000000 Prec(ct) ## Precision or Positive Predictive Value #> It is identical to weighted Accuracy #> [1] 0.4357143 Rec(ct) ## Recall or True Positive Rate #> [1] 0.4807692 Spe(ct) ## Specificity or True Negative Rate #> [1] 0.8085477 F1(ct) ## F1 (harmonic mean of Precision and Recall) #> [1] 0.4484848 ####### RBF versicolor vs virginica model: sv_index <- kernlab::alphaindex(model)[[3]] sv_coef <- kernlab::coef(model)[[3]] svm_imp(X=iris_std[train_idx,],svindx=sv_index,coeff=sv_coef) #> Do not use this function if the SVM model was created with the RBF, #>           Laplacian, Bray-Curtis, Jaccard/Ruzicka, or Kendall's tau kernels #> Sepal.Length  Sepal.Width Petal.Length  Petal.Width  #>    0.8931422    0.7828191   12.3522663   10.0792776"},{"path":"https://elies-ramon.github.io/kerntools/articles/kerntools.html","id":"non-standard-data-exotic-normalizations-and-more-about-feature-spaces","dir":"Articles","previous_headings":"","what":"Non-standard data, exotic normalizations, and more about feature spaces","title":"kerntools: R tools for kernel methods","text":"natural workflow package shown (twice) previous sections. reason, remainder vignette deal obscure (interesting) matters concerning kernels “non-standard” kinds data. Also, delve deeper feature space normalization effects.","code":""},{"path":"https://elies-ramon.github.io/kerntools/articles/kerntools.html","id":"non-standard-data","dir":"Articles","previous_headings":"Non-standard data, exotic normalizations, and more about feature spaces","what":"Non-standard data","title":"kerntools: R tools for kernel methods","text":"now, worked kernels real vectors. , dataset consisted several features (four case: sepal petal length width) measured set individuals (150 iris flowers). Another way looking considering 150 vectors length 4 (incidentally, way kernel functions look data). real vectors (, least, standardized ). Unknowingly, also worked kernels real matrices: compared three kernel matrices simK() result … another kernel matrix. fact, simK() simply wrapper Frobenius(). Frobenius kernel, input function (“objects” work ) vectors, numeric matrices. machine learning methods work primarily real vectors , cases, matrices. case kernel methods, can work virtually kind data can think . kernel method (SVM, kernel PCA, etc) sees kernel matrix, original objects. , long can create valid (.e. symmetric PSD) kernel function objects, everything turn just well. list kernel functions endless. Right now, kerntools can deal effortlessly following kinds data: Real vectors: Linear, RBF, Laplacian kernels Real matrices: Frobenius kernel Counts Frequencies (non-negative numbers): Bray-Curtis, Ruzicka (quantitative Jaccard) kernels Categorical data: Overlap / Dirac kernel Sets: Intersect, Jaccard kernels Ordinal data / rankings: Kendall’s tau kernel Strings / Text: Spectrum kernel commonly used different fields. instace, categorical data widespread numeric data (), text mining content analysis right now lively research field, Bray-Curtis Ruzicka kernels closely related famous beta-diversity indices used ecological studies. illustrate kerntools works categorical variables. (rest kernel functions, please read detail specific documentation page). kerntools includes categorical dataset called showdata: can see 5 categorical features (class: “factor”). Typical approaches kind data involve recoding “dummy” variables, single categorical variable transformed L dummy variables (L=number classes, using R nomenclature, number levels). Presence given class marked 1 corresponding column, rest entries 0. called one-hot-encoding, kerntools done dummy_var() function: (sometimes, like Linear Models, design matrix contains L-1 dummy variables. kind recoding can done model.matrix()) approach using kernels bit different. work original dataset. kernel make pairwise comparisons N = 100 samples , every pair samples, ask: class equal two samples, different? example: “Favorite.color” “red” sample 1, “black” sample 2, “red” sample 3. comparison categorical feature samples 1-2 return FALSE, comparing samples 1-3 return TRUE.  single categorical variable D=5, combine results comparison D variables. ’s stated comp=\"sum\": make kernel sum FALSES TRUES pairwise comparing “Favorite.color”, “Favorite.actress”, “Favorite.actor”, “Favorite.show” “Liked.new.show” (also option average , compute weighting average, consider features important others). histogram KD shows identical samples, lot samples completely different, samples equivalent one 5 features. Now kernel matrix! Now can whatever want, included training prediction model computing kernel PCA. Yes, exactly: although PCA technique originally created real numeric data, another advantage yet kernels can PCA everything. simplicity, train prediction model (free follow steps shown previous section), show kernel PCA showdata contains result (fictional) survey idea predict people preferences predict liked new show, time, computing kernel features 1:4, can color kernel PCA feature 5 (“like show?”). Furthermore, change, also draw ellipses around centroids “Yes” group “” group:  seems group liked show forms tight cluster, people liked scattered along PC1. Now, can study contributions class see intuitions confirmed:   (plots show top 10 features.) PC1 seems lead especially Game Thrones (related actors/actresses) right, Witcher (related actors/actresses) left, small contribution black color. PC2 (seems relevant PC1) seems led Witcher one side, color blue Squid Game (honorific mention Helena Bonham Carter). draw arrows:  summary, seem relevant fraction dataset divided Game Thrones Witcher fans, one thing common: clearly like new show. previous code computed Dirac kernel matrix, also computed feature space (feat_space=TRUE). Conveniently, natural feature space Dirac kernel works generated one-hot-encoding. allowed us compute contributions class (dummy variable) PC1 2.","code":"summary(showdata) #>    Favorite.color             Favorite.actress        Favorite.actor #>  blue     :20     Sophie Turner       :22      Peter Dinklage:20     #>  red      :20     Emilia Clarke       :19      Kit Harington :17     #>  black    :14     Anya Chalotra       :12      Henry Cavill  :15     #>  purple   :12     Freya Allan         :10      Lee Jung-jae  :15     #>  green    : 8     Helena Bonham Carter: 8      Hyun Bin      : 8     #>  lightblue: 8     Lorraine Ashbourne  : 7      Josh O'Connor : 7     #>  (Other)  :18     (Other)             :22      (Other)       :18     #>           Favorite.show Liked.new.show #>  Game of Thrones :22    No :48         #>  The witcher     :17    Yes:52         #>  Bridgerton      :14                   #>  Squid game      :11                   #>  The crown       :11                   #>  La casa de papel: 8                   #>  (Other)         :17 dummy_showdata <- dummy_data(showdata) dummy_showdata[1:5,1:3] #>   Favorite.color_black Favorite.color_blue Favorite.color_green #> 1                    0                   0                    0 #> 2                    1                   0                    0 #> 3                    0                   0                    0 #> 4                    0                   1                    0 #> 5                    0                   0                    0 KD <- Dirac(showdata, comp=\"sum\") histK(KD, col =\"plum2\") KD <- Dirac(showdata[,1:4], comp=\"sum\",feat_space=TRUE) dirac_kpca <- kPCA(KD$K,plot=1:2,y=factor(showdata$Liked.new.show),ellipse=0.66,title=\"Dirac kernel PCA\") dirac_kpca$plot pcs <- kPCA_imp(KD$feat_space) #> Do not use this function if the PCA was created with the RBF, #>           Laplacian, Bray-Curtis, Jaccard/Ruzicka, or Kendall's tau kernels pc1 <- plotImp(pcs$loadings[1,],leftmargin=15,nfeat=10,absolute = FALSE,  relative = FALSE,col =\"bisque\") pc2 <- plotImp(pcs$loadings[2,],leftmargin=17,nfeat=10,absolute = FALSE, relative = FALSE, col=\"honeydew3\") features <- union(pc1$first_features,pc2$first_features) kPCA_arrows(plot=dirac_kpca$plot,contributions=t(pcs$loadings[1:2,features]),colour=\"grey15\")"},{"path":"https://elies-ramon.github.io/kerntools/articles/kerntools.html","id":"normalization-techniques","dir":"Articles","previous_headings":"Non-standard data, exotic normalizations, and more about feature spaces","what":"Normalization techniques","title":"kerntools: R tools for kernel methods","text":"kerntools provides functions normalization techniques. Several specific kernel matrices, data matrices general, . First see normalization data matrices data.frames. One used techniques standardization, saw previous sections. already implemented R base via scale(X,center=TRUE,scale=TRUE), allows standardizing (column) centering scaling (column). kerntools minmax() normalization, normalizes dataset 0 1. centerX() function centers squared dataset row column (yes, squared datasets: center row NxD dataset, may use scale(t(X),center=T,scale=F)). Another useful function TSS(), operates row column, transforms absolute frequencies relative ones, row (column) sums 1. vein, cosnormX() normalizes L2 norm row (column): , norm row (column) sums 1. functions usually default row-normalization, way kernel functions look data (minmax() exception). Finally, normalization Frobenius norm available data matrices (also kernel matrices) via frobNorm(). Apart Frobenius normalization, kerntools two normalization functions kernel matrices: cosNorm() centerK(). first one applies cosine normalization kernel matrix, maximum value 1 (sometimes, also bound minimum value around 0). operation related cosnormX(). fact, working linear kernel (case!), two operations equivalent: second function centerK() (needless say, function somewhat related centerX()). , centering dataset column computing linear kernel, computing linear kernel centering centerK() equivalent. , two duplicated ways ? Well, apart speed (expression faster depends dataset dimension, , rows columns), expressions equivalent using linear kernel. using another kernel, cosnormX() centerK() normalize center kernel matrix… according features feature space. input space. reason, :  :  don’t look alike slightest. (Incidentally, noted RBF translation-invariant respect variables input space. : standardization useful kernel, simply centering . RBF(iris[,1:4],g=0.25) using center_iris .) summary: want two equivalent (like linear kernel), computing L2 norm centering using feature space, input space. done RBF() function, although possible kernels stated previous subsection. moment explained normalization real data. kinds data? Well, kerntools favors approach kinds data best dealt kernel functions. Also remember kerntools kernel functions previous subsection also return feature space demand. Still, kerntools offers basic one-hot-encoding categorical data. provided dummy_data(), converts categorical dataset one-hot-encoded one. (feature space Dirac kernel works). function requires user specifies number levels per factor, can easily done another function: dummy_var().","code":"KL1 <- Linear(cosnormX(iris[,1:4])) # important: by row KL2 <- cosNorm(Linear(iris[,1:4])) # a third valid option is: Linear(iris[,1:4], cos.norm=TRUE) simK(list(KL1=KL1,KL2=KL2)) #> Remember that Klist should contain only kernel matrices (i.e. squared, symmetric and PSD). #>   This function does NOT verify the symmetry and PSD criteria. #>     KL1 KL2 #> KL1   1   1 #> KL2   1   1 center_iris <- scale(iris[,1:4],scale=FALSE,center=TRUE) histK(RBF(center_iris,g=0.25),col=\"aquamarine\") histK(centerK(RBF(iris[,1:4],g=0.25)),col=\"aquamarine3\")"},{"path":"https://elies-ramon.github.io/kerntools/articles/kerntools.html","id":"fusing-data--a-word-about-a-priori-and-a-posteriori-feature-importances-","dir":"Articles","previous_headings":"Non-standard data, exotic normalizations, and more about feature spaces","what":"Fusing data. A word about a priori and a posteriori feature importances.","title":"kerntools: R tools for kernel methods","text":"One advantage using kernel matrices instead original datasets kernel matrices can combined easily. instance, imagine two () sources data individuals. example, dataset1 numeric dimension NxD1, dataset2 dimension NxD2 contains kind data (example categorical). sum multiply two datasets; however, can sum multiply kernel matrices K1 K2. Let’s see simple illustration dataset mtcars. mtcars 11 features: 9 numeric 2 can though categorical: vs (engine, can V-shaped straight), (transmission: automatic manual). can split mtcars two parts: data.frame numeric features data.frame categorical ones: Now prepare kernel matrix data.frame. dataset1 use linear kernel dataset2 use Dirac kernel: can create “consensus” kernel K1 K2 using kerntools function: noted , , K1 weight K2 computing final kernel, although K1 9 variables K2 2. want weight equally one 11 variables final kernel, can : Now, maybe fancy comparing K1 K2 consensus kernel matrices: Mmm… something strange happening . Shouldn’t K2 similar consensus matrix former case latter? phenomenon caused normalize dataset1 K1. , averaged K1 K2, without taking account K1 large values comparison K2:   : though K2 overrepresented consensus kernel… actually way around. previous sections seen feature importance given SVM models. can considered like kind posteriori feature importance. However, cautious regarding implicit weights give features training model. can think like kind priori (apologies bayesian statisticians nomenclature!) feature importance. important implicit weighting mind SVM (whatever method use) seeing original data, kernel matrices. fact, “scale” problem arises (non-kernel) methods: reason advised normalize standardize numeric datasets. Now, time real, try weight equally 11 features: details maybe sound bit specific, min-max normalization bounds numeric data 0 1. range [0,1] one-hot-encoding categorical variables (feature space related Dirac kernel). addition, chose comp=sum linear kernel also “sums” one features. Now, K1 K2 almost equally similar consensus first case, K2 less similar second case. K1 K2 still high similarity, probably caused features mtcars correlated.","code":"dim(mtcars) #> [1] 32 11 head(mtcars) #>                    mpg cyl disp  hp drat    wt  qsec vs am gear carb #> Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4 #> Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4 #> Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1 #> Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1 #> Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2 #> Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1 cat_feat_idx <- 8:9 MTCARS <- list(num=mtcars[,-cat_feat_idx], cat=mtcars[,cat_feat_idx]) K <- array(dim=c(32,32,2)) K[,,1] <- Linear(MTCARS[[1]]) ## Kernel for numeric data K[,,2] <- Dirac(MTCARS[[2]]) ## Kernel for categorical data Kcons <- MKC(K) coeff <- sapply(MTCARS,ncol) coeff #  K1 will weight 9/11 and K2 2/11. #> num cat  #>   9   2 Kcons_var <- MKC(K,coeff=coeff) simK(list(Kcons=Kcons,K1=K[,,1],K2=K[,,2])) #> Remember that Klist should contain only kernel matrices (i.e. squared, symmetric and PSD). #>   This function does NOT verify the symmetry and PSD criteria. #>           Kcons        K1        K2 #> Kcons 1.0000000 1.0000000 0.7871094 #> K1    1.0000000 1.0000000 0.7871069 #> K2    0.7871094 0.7871069 1.0000000 simK(list(Kcons_var=Kcons_var,K1=K[,,1],K2=K[,,2])) #> Remember that Klist should contain only kernel matrices (i.e. squared, symmetric and PSD). #>   This function does NOT verify the symmetry and PSD criteria. #>           Kcons_var        K1        K2 #> Kcons_var 1.0000000 1.0000000 0.7871074 #> K1        1.0000000 1.0000000 0.7871069 #> K2        0.7871074 0.7871069 1.0000000 histK(K[,,1], col=\"khaki1\") histK(K[,,2], col=\"hotpink\") K[,,1] <- Linear(minmax(MTCARS[[1]])) ## Kernel for numeric data K[,,2] <- Dirac(MTCARS[[2]],comp=\"sum\") ## Kernel for categorical data Kcons <- MKC(K) Kcons_var <- MKC(K,coeff=coeff) simK(list(Kcons=Kcons,K1=K[,,1],K2=K[,,2])) #> Remember that Klist should contain only kernel matrices (i.e. squared, symmetric and PSD). #>   This function does NOT verify the symmetry and PSD criteria. #>           Kcons        K1        K2 #> Kcons 1.0000000 0.9791777 0.9663366 #> K1    0.9791777 1.0000000 0.8939859 #> K2    0.9663366 0.8939859 1.0000000 simK(list(Kcons=Kcons_var,K1=K[,,1],K2=K[,,2])) #> Remember that Klist should contain only kernel matrices (i.e. squared, symmetric and PSD). #>   This function does NOT verify the symmetry and PSD criteria. #>           Kcons        K1        K2 #> Kcons 1.0000000 0.9977012 0.9222967 #> K1    0.9977012 1.0000000 0.8939859 #> K2    0.9222967 0.8939859 1.0000000"},{"path":"https://elies-ramon.github.io/kerntools/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Elies Ramon. Author, maintainer, copyright holder.","code":""},{"path":"https://elies-ramon.github.io/kerntools/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Ramon E (2024). kerntools: Kernel Functions Tools Machine Learning Applications. R package version 1.0.2, https://elies-ramon.github.io/kerntools/, https://github.com/elies-ramon/kerntools.","code":"@Manual{,   title = {kerntools: Kernel Functions and Tools for Machine Learning Applications},   author = {Elies Ramon},   year = {2024},   note = {R package version 1.0.2, https://elies-ramon.github.io/kerntools/},   url = {https://github.com/elies-ramon/kerntools}, }"},{"path":"https://elies-ramon.github.io/kerntools/index.html","id":"kerntools","dir":"","previous_headings":"","what":"Kernel Functions and Tools for Machine Learning Applications","title":"Kernel Functions and Tools for Machine Learning Applications","text":"goal kerntools provide R tools working family Machine Learning methods called kernel methods. can used complement R packages like kernlab. Right now, kerntools implements several kernel functions treating non-negative real vectors, real matrices, categorical ordinal variables, sets, strings. Several tools studying resulting kernel matrix compare two kernel matrices available. diagnostic tools may used infer kernel(s) matrix(ces) suitability training models. package also provides functions computing feature importance Support Vector Machines (SVMs) models, display customizable kernel Principal Components Analysis (PCA) plots. convenience, widespread performance measures feature importance barplots available user.","code":""},{"path":[]},{"path":"https://elies-ramon.github.io/kerntools/index.html","id":"installation-and-loading","dir":"","previous_headings":"Installation","what":"Installation and loading","title":"Kernel Functions and Tools for Machine Learning Applications","text":"Installing kerntools easy. R console: package installed, can loaded anytime typing:","code":"install.packages(\"kerntools\") library(kerntools)"},{"path":"https://elies-ramon.github.io/kerntools/index.html","id":"dependencies","dir":"","previous_headings":"Installation","what":"Dependencies","title":"Kernel Functions and Tools for Machine Learning Applications","text":"kerntools requires R (>= 2.10). Currently, also relies following packages: dplyr ggplot2 kernlab methods reshape2 stringi Usually, packages missing library, installed automatically kerntools installed.","code":""},{"path":"https://elies-ramon.github.io/kerntools/index.html","id":"a-quick-example-kernel-pca","dir":"","previous_headings":"","what":"A quick example: kernel PCA","title":"Kernel Functions and Tools for Machine Learning Applications","text":"Imagine want perform (kernel) PCA plot dataset consist categorical variables. can done easily kerntools! First, chose appropriate kernel data (example, Dirac kernel categorical variables), pass output Dirac() function kPCA() function.  can customize kernel PCA plot: apart picking principal components want display (example: PC1 PC2), may want add title, legend, use different colors represent additional variable interest, can check patterns data. see detail customize kPCA() plot, please refer documentation. projection matrix also returned (dirac_kpca$projection), may use analyses /creating plot.","code":"head(showdata) #>   Favorite.color   Favorite.actress     Favorite.actor    Favorite.show #> 1            red      Sophie Turner      Josh O'Connor        The crown #> 2          black         Soo Ye-jin           Hyun Bin       Bridgerton #> 3            red Lorraine Ashbourne       Henry Cavill       Bridgerton #> 4           blue      Sophie Turner       Alvaro Morte La casa de papel #> 5            red      Sophie Turner Michael K Williams         The wire #> 6         yellow      Sophie Turner      Kit Harington  Game of Thrones #>   Liked.new.show #> 1            Yes #> 2             No #> 3            Yes #> 4             No #> 5            Yes #> 6             No KD <- Dirac(showdata[,1:4]) dirac_kpca <- kPCA(KD,plot=c(1,2),title=\"Survey\", name_leg = \"Liked the show?\",                     y=showdata$Liked.new.show, ellipse=0.66) dirac_kpca$plot"},{"path":"https://elies-ramon.github.io/kerntools/index.html","id":"main-kerntools-features","dir":"","previous_headings":"","what":"Main kerntools features","title":"Kernel Functions and Tools for Machine Learning Applications","text":"Right now, kerntools can deal effortlessly following kinds data: Real vectors: Linear, RBF Laplacian kernels. Real matrices: Frobenius kernel. Counts Frequencies (non-negative numbers): Bray-Curtis Ruzicka (quantitative Jaccard) kernels. Categorical data: Overlap / Dirac kernel. Sets: Intersect Jaccard kernels. Ordinal data rankings: Kendall’s tau kernel. Strings Text: Spectrum kernel. Several tools visualizing comparing kernel matrices provided. Regarding kernel PCA, kerntools allows user : Compute kernel PCA kernel matrix, computed kerntools provided user. Display customizable PCA plots (possible) Compute display contribution variables principal component. Compare two PCAs generated set samples using Co-inertia Procrustes analysis. using specific kernels, kerntools computes importance variable feature Support Vector Machine (SVM) model. kerntools train SVMs prediction models, can recover feature importance models fitted packages (instance kernlab). importances can sorted summarized customizable barplot. Finally, following performance measures regression, binary multi-class classification implemented: Regression: Normalized Mean Squared Error Classification: accuracy, specificity, sensitivity, precision F1 (optional) confidence intervals, computed using normal approximation bootstrapping.","code":""},{"path":"https://elies-ramon.github.io/kerntools/index.html","id":"example-data","dir":"","previous_headings":"","what":"Example data","title":"Kernel Functions and Tools for Machine Learning Applications","text":"kerntools contains categorical toy dataset called showdata.","code":""},{"path":[]},{"path":"https://elies-ramon.github.io/kerntools/index.html","id":"vignette","dir":"","previous_headings":"Documentation","what":"Vignette","title":"Kernel Functions and Tools for Machine Learning Applications","text":"see detailed step--step examples illustrate main cases use kerntools, please look vignette:","code":"browseVignettes(kerntools)"},{"path":"https://elies-ramon.github.io/kerntools/index.html","id":"additional-help","dir":"","previous_headings":"Documentation","what":"Additional help","title":"Kernel Functions and Tools for Machine Learning Applications","text":"Remember detailed, argument--argument documentation available function: documentation example dataset available analogous way, typing:","code":"help(kPCA) ## or the specific name of the function ?kPCA help(showdata) ?showdata"},{"path":"https://elies-ramon.github.io/kerntools/index.html","id":"more-about-kernels","dir":"","previous_headings":"Documentation","what":"More about kernels","title":"Kernel Functions and Tools for Machine Learning Applications","text":"know kernel functions, matrices methods, can consult following reference materials: Bishop, C. M., & Nasrabadi, N. M. (2006). Pattern recognition machine learning (Vol. 4, . 4, p. 738). Chapter 6, pp. 291-323. New York: springer. Müller, K. R., Mika, S., Tsuda, K., & Schölkopf, K. (2018) introduction kernel-based learning algorithms. Handbook neural network signal processing (pp. 4-1). CRC Press. Shawe-Taylor, J., & Cristianini, N. (2004). Kernel methods pattern analysis. Cambridge university press.","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/Acc.html","id":null,"dir":"Reference","previous_headings":"","what":"Accuracy — Acc","title":"Accuracy — Acc","text":"`Acc()` computes accuracy output classification model actual values target. can also compute weighted accuracy, useful imbalanced classification problems. weighting applied according class frequencies target. balanced problems, weighted Acc = Acc.","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/Acc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Accuracy — Acc","text":"","code":"Acc(ct, weighted = FALSE)"},{"path":"https://elies-ramon.github.io/kerntools/reference/Acc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Accuracy — Acc","text":"ct Confusion Matrix. weighted TRUE, weighted accuracy returned. (Defaults: FALSE).","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/Acc.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Accuracy — Acc","text":"Accuracy model (single value).","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/Acc.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Accuracy — Acc","text":"","code":"y <- c(rep(\"a\",3),rep(\"b\",2)) y_pred <- c(rep(\"a\",2),rep(\"b\",3)) ct <- table(y,y_pred) Acc(ct) #> [1] 0.8 Acc(ct,weighted=TRUE) #> [1] 0.8333333"},{"path":"https://elies-ramon.github.io/kerntools/reference/Acc_rnd.html","id":null,"dir":"Reference","previous_headings":"","what":"Accuracy of a random model — Acc_rnd","title":"Accuracy of a random model — Acc_rnd","text":"`Acc_rnd()` computes expected accuracy random classifier based class frequencies target. measure can used benchmark contrasted accuracy (test) given prediction model.","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/Acc_rnd.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Accuracy of a random model — Acc_rnd","text":"","code":"Acc_rnd(target, freq = FALSE)"},{"path":"https://elies-ramon.github.io/kerntools/reference/Acc_rnd.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Accuracy of a random model — Acc_rnd","text":"target character vector factor. Alternatively, numeric vector (see ). freq TRUE `target` contains frequencies classes (case, `target` numeric), FALSE otherwise. (Defaults: FALSE).","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/Acc_rnd.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Accuracy of a random model — Acc_rnd","text":"Expected accuracy random classification model (single value).","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/Acc_rnd.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Accuracy of a random model — Acc_rnd","text":"","code":"# Expected accuracy of a random model: target <- c(rep(\"a\",5),rep(\"b\",2)) Acc_rnd(target) #> [1] 0.5918367 # This is the same than: freqs <- c(5/7,2/7) Acc_rnd(freqs,freq=TRUE) #> [1] 0.5918367"},{"path":"https://elies-ramon.github.io/kerntools/reference/Boots_CI.html","id":null,"dir":"Reference","previous_headings":"","what":"Confidence Interval using Bootstrap — Boots_CI","title":"Confidence Interval using Bootstrap — Boots_CI","text":"`Boots_CI()` computes Confidence Interval (CI) performance measure (instance, accuracy) via bootstrapping.","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/Boots_CI.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Confidence Interval using Bootstrap — Boots_CI","text":"","code":"Boots_CI(target, pred, index = \"acc\", nboots, confidence = 95, ...)"},{"path":"https://elies-ramon.github.io/kerntools/reference/Boots_CI.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Confidence Interval using Bootstrap — Boots_CI","text":"target Numeric vector containing actual values. pred Numeric vector containing predicted values. (order target's). index Performance measure name, lowercase. (Defaults: \"acc\"). nboots Number bootstrapping replicas. confidence Confidence level; instance, 95% 99%. (Defaults: 95). ... arguments passed performance measures functions; notably, multi.class=\"macro\" multi.class=\"micro\" macro micro performance measures. (Defaults: \"macro\").","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/Boots_CI.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Confidence Interval using Bootstrap — Boots_CI","text":"vector containing bootstrap estimate performance CI.","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/Boots_CI.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Confidence Interval using Bootstrap — Boots_CI","text":"","code":"y <- c(rep(\"a\",30),rep(\"b\",20)) y_pred <- c(rep(\"a\",20),rep(\"b\",30)) # Computing Accuracy with their 95%CI Boots_CI(target=y, pred=y_pred, index=\"acc\", nboots=1000, confidence=95) #>          2.5%  97.5%  #> 0.7989 0.6800 0.9000"},{"path":"https://elies-ramon.github.io/kerntools/reference/BrayCurtis.html","id":null,"dir":"Reference","previous_headings":"","what":"Kernels for count data — BrayCurtis","title":"Kernels for count data — BrayCurtis","text":"Ruzicka Bray-Curtis kernel functions absolute relative frequencies count data. kernels input matrix data.frame dimension NxD N>1, D>1, containing strictly non-negative real numbers. Samples rows. Thus, working relative frequencies, `rowSums(X)` 1 (100, another arbitrary number) rows (samples) dataset.","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/BrayCurtis.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Kernels for count data — BrayCurtis","text":"","code":"BrayCurtis(X)  Ruzicka(X)"},{"path":"https://elies-ramon.github.io/kerntools/reference/BrayCurtis.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Kernels for count data — BrayCurtis","text":"X Matrix data.frame contains absolute relative frequencies.","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/BrayCurtis.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Kernels for count data — BrayCurtis","text":"Kernel matrix (dimension: NxN).","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/BrayCurtis.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Kernels for count data — BrayCurtis","text":"info measures, please check Details ?vegan::vegdist(). Note , vegan help page, \"Ruzicka\" corresponds \"quantitative Jaccard\". `BrayCurtis(X)` gives result  `1-vegan::vegdist(X,method=\"bray\")`, `Ruzicka(data)`  `1-vegan::vegdist(data,method=\"jaccard\")`.","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/BrayCurtis.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Kernels for count data — BrayCurtis","text":"","code":"data <- matrix(rpois(5000,lambda=3),ncol=100,nrow=50) Kruz <- Ruzicka(data) Kbray <- BrayCurtis(data) Kruz[1:5,1:5] #>           1         2         3         4         5 #> 1 1.0000000 0.5468354 0.5073171 0.5111663 0.4796163 #> 2 0.5468354 1.0000000 0.5207254 0.4974093 0.5181347 #> 3 0.5073171 0.5207254 1.0000000 0.4885496 0.5089059 #> 4 0.5111663 0.4974093 0.4885496 1.0000000 0.4897959 #> 5 0.4796163 0.5181347 0.5089059 0.4897959 1.0000000 Kbray[1:5,1:5] #>           1         2         3         4         5 #> 1 1.0000000 0.7070376 0.6731392 0.6765189 0.6482982 #> 2 0.7070376 1.0000000 0.6848382 0.6643599 0.6825939 #> 3 0.6731392 0.6848382 1.0000000 0.6564103 0.6745363 #> 4 0.6765189 0.6643599 0.6564103 1.0000000 0.6575342 #> 5 0.6482982 0.6825939 0.6745363 0.6575342 1.0000000"},{"path":"https://elies-ramon.github.io/kerntools/reference/Dirac.html","id":null,"dir":"Reference","previous_headings":"","what":"Kernels for categorical variables — Dirac","title":"Kernels for categorical variables — Dirac","text":"matrix data.frame dimension NxD, N>1, D>0, `Dirac()` computes simplest kernel categorical data. Samples rows features columns. single feature, `Dirac()` returns 1 category (class, level) two given samples, 0 otherwise. Instead, D>1, results D features combined sum, mean, weighted mean.","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/Dirac.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Kernels for categorical variables — Dirac","text":"","code":"Dirac(X, comp = \"mean\", coeff = NULL, feat_space = FALSE)"},{"path":"https://elies-ramon.github.io/kerntools/reference/Dirac.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Kernels for categorical variables — Dirac","text":"X Matrix (class \"character\") data.frame (class \"character\", columns = \"factor\"). elements X assumed categorical nature. comp D>1, argument indicates variables dataset combined. Options : \"mean\", \"sum\" \"weighted\". (Defaults: \"mean\") \"sum\" gives importance variables, returns   unnormalized kernel matrix. \"mean\" gives importance variables, returns   normalized kernel matrix (elements range 0 1). \"weighted\" weights variable according `coeff` parameter, returns   normalized kernel matrix. coeff (optional) vector weights length D. feat_space FALSE, kernel matrix returned. Otherwise, feature space also returned. (Defaults: FALSE).","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/Dirac.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Kernels for categorical variables — Dirac","text":"Kernel matrix (dimension: NxN), list kernel matrix feature space.","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/Dirac.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Kernels for categorical variables — Dirac","text":"Belanche, L. ., Villegas, M. . (2013). Kernel functions categorical variables application problems life sciences. Artificial Intelligence Research Development (pp. 171-180). IOS Press. Link","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/Dirac.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Kernels for categorical variables — Dirac","text":"","code":"# Categorical data summary(CO2) #>      Plant             Type         Treatment       conc          uptake      #>  Qn1    : 7   Quebec     :42   nonchilled:42   Min.   :  95   Min.   : 7.70   #>  Qn2    : 7   Mississippi:42   chilled   :42   1st Qu.: 175   1st Qu.:17.90   #>  Qn3    : 7                                    Median : 350   Median :28.30   #>  Qc1    : 7                                    Mean   : 435   Mean   :27.21   #>  Qc3    : 7                                    3rd Qu.: 675   3rd Qu.:37.12   #>  Qc2    : 7                                    Max.   :1000   Max.   :45.50   #>  (Other):42                                                                   Kdirac <- Dirac(CO2[,1:3]) ## Display a subset of the kernel matrix: Kdirac[c(1,15,50,65),c(1,15,50,65)] #>            1        15        50        65 #> 1  1.0000000 0.6666667 0.3333333 0.0000000 #> 15 0.6666667 1.0000000 0.3333333 0.0000000 #> 50 0.3333333 0.3333333 1.0000000 0.3333333 #> 65 0.0000000 0.0000000 0.3333333 1.0000000"},{"path":"https://elies-ramon.github.io/kerntools/reference/F1.html","id":null,"dir":"Reference","previous_headings":"","what":"F1 score — F1","title":"F1 score — F1","text":"`F1()` computes F1 score output classification prediction model actual values target.","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/F1.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"F1 score — F1","text":"","code":"F1(ct, multi.class = \"macro\")"},{"path":"https://elies-ramon.github.io/kerntools/reference/F1.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"F1 score — F1","text":"ct Confusion Matrix. multi.class results class aggregated, ? Options: \"none\", \"macro\", \"micro\". (Defaults: \"macro\").","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/F1.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"F1 score — F1","text":"F1 (single value).","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/F1.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"F1 score — F1","text":"F1 corresponds harmonic mean Precision Recall.","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/F1.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"F1 score — F1","text":"","code":"y <- c(rep(\"a\",3),rep(\"b\",2)) y_pred <- c(rep(\"a\",2),rep(\"b\",3)) ct <- table(y,y_pred) F1(ct) #> [1] 0.8"},{"path":"https://elies-ramon.github.io/kerntools/reference/Frobenius.html","id":null,"dir":"Reference","previous_headings":"","what":"Frobenius kernel — Frobenius","title":"Frobenius kernel — Frobenius","text":"`Frobenius()` computes Frobenius kernel numeric matrices.","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/Frobenius.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Frobenius kernel — Frobenius","text":"","code":"Frobenius(DATA, cos.norm = FALSE, feat_space = FALSE)"},{"path":"https://elies-ramon.github.io/kerntools/reference/Frobenius.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Frobenius kernel — Frobenius","text":"DATA list M matrices data.frames containing real numbers (class \"integer\", \"float\" \"double\"). matrices data.frames number rows columns. cos.norm resulting kernel matrix cosine normalized? (Defaults: FALSE). feat_space FALSE, kernel matrix returned. Otherwise, feature space also returned. (Defaults: FALSE).","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/Frobenius.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Frobenius kernel — Frobenius","text":"Kernel matrix (dimension:NxN), list kernel matrix feature space.","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/Frobenius.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Frobenius kernel — Frobenius","text":"Frobenius kernel Frobenius inner product matrices.","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/Frobenius.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Frobenius kernel — Frobenius","text":"","code":"data1 <- matrix(rnorm(250000),ncol=500,nrow=500) data2 <- matrix(rnorm(250000),ncol=500,nrow=500) data3 <- matrix(rnorm(250000),ncol=500,nrow=500)  Frobenius(list(data1,data2,data3)) #>              [,1]        [,2]         [,3] #> [1,] 250006.28548   -345.2440     27.80452 #> [2,]   -345.24405 249780.1340   -845.03543 #> [3,]     27.80452   -845.0354 249859.33523"},{"path":"https://elies-ramon.github.io/kerntools/reference/Jaccard.html","id":null,"dir":"Reference","previous_headings":"","what":"Kernels for sets — Jaccard","title":"Kernels for sets — Jaccard","text":"`Intersect()` `Jaccard()` compute kernel functions name, useful set data. input matrix data.frame dimension NxD, N>1, D>0. Samples rows features columns. single feature, `Jaccard()` returns 1 elements set exactly two given samples, 0 completely different (see Details). Instead, multivariate case (D>1), results (`Intersect()` `Jaccard()`) D features combined sum, mean, weighted mean.","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/Jaccard.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Kernels for sets — Jaccard","text":"","code":"Jaccard(X, elements = LETTERS, comp = \"mean\", coeff = NULL)  Intersect(   X,   elements = LETTERS,   comp = \"mean\",   coeff = NULL,   feat_space = FALSE )"},{"path":"https://elies-ramon.github.io/kerntools/reference/Jaccard.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Kernels for sets — Jaccard","text":"X Matrix (class \"character\") data.frame (class \"character\", columns = \"factor\"). elements X assumed categorical nature. elements potential elements (symbols) can appear sets. elements interest, can excluded taken account kernels. (Defaults: LETTERS). comp D>1, argument indicates variables dataset combined. Options : \"mean\", \"sum\" \"weighted\". (Defaults: \"mean\") \"sum\" gives importance variables, returns   unnormalized kernel matrix. \"mean\" gives importance variables, returns   normalized kernel matrix (elements range 0 1). \"weighted\" weights variable according `coeff` parameter, returns   normalized kernel matrix. coeff (optional) vector weights length D. feat_space (available Jaccard kernel). FALSE, kernel matrix returned. Otherwise, feature space returned . (Defaults: FALSE).","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/Jaccard.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Kernels for sets — Jaccard","text":"Kernel matrix (dimension: NxN), list kernel matrix feature space.","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/Jaccard.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Kernels for sets — Jaccard","text":"Let \\(,B\\) two sets. , Intersect kernel defined : $$K_{Intersect}(,B)=|\\cap B| $$ Jaccard kernel defined : $$K_{Jaccard}(,B)=|\\cap B| / |\\cup B|$$ specific implementation Intersect Jaccard kernels expects set members (elements) character symbols (length=1). case set data multivariate (D>1 columns, one contains set feature), elements D sets come domain (universe). instance, dataset two variables, elements first one colors c(\"green\",\"black\",\"white\",\"red\") second names c(\"Anna\",\"Elsa\",\"Maria\") allowed. case, set factors recoded colors c(\"g\",\"b\",\"w\",\"r\") names c(\"\",\"E\",\"M\") , necessary, 'Intersect()' (`Jaccard()`) called twice.","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/Jaccard.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Kernels for sets — Jaccard","text":"Bouchard, M., Jousselme, . L., Doré, P. E. (2013). proof positive definiteness Jaccard index matrix. International Journal Approximate Reasoning, 54(5), 615-626. Ruiz, F., Angulo, C., Agell, N. (2008). Intersection Signed-Intersection Kernels Intervals. Frontiers Artificial Intelligence Applications. 184. 262-270. doi: 10.3233/978-1-58603-925-7-262.","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/Jaccard.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Kernels for sets — Jaccard","text":"","code":"# Sets data ## Generating a dataset with sets containing uppercase letters random_set <- function(x)paste(sort(sample(LETTERS,x,FALSE)),sep=\"\",collapse = \"\") max_setsize <- 4 setsdata <- matrix(replicate(20,random_set(sample(2:max_setsize,1))),nrow=4,ncol=5)  ## Computing the Intersect kernel: Intersect(setsdata,elements=LETTERS,comp=\"sum\") #>    1  2  3  4 #> 1 15  2  0  3 #> 2  2 13  0  0 #> 3  0  0 13  2 #> 4  3  0  2 14  ## Computing the Jaccard kernel weighting the variables: coeffs <- c(0.1,0.15,0.15,0.4,0.20) Jaccard(setsdata,elements=LETTERS,comp=\"weighted\",coeff=coeffs) #>      1    2         3         4 #> 1 1.00 0.08 0.0000000 0.1300000 #> 2 0.08 1.00 0.0000000 0.0000000 #> 3 0.00 0.00 1.0000000 0.1285714 #> 4 0.13 0.00 0.1285714 1.0000000"},{"path":"https://elies-ramon.github.io/kerntools/reference/KTA.html","id":null,"dir":"Reference","previous_headings":"","what":"Kernel-target alignment — KTA","title":"Kernel-target alignment — KTA","text":"`KTA()` computes alignment kernel matrix target variable.","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/KTA.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Kernel-target alignment — KTA","text":"","code":"KTA(K, y)"},{"path":"https://elies-ramon.github.io/kerntools/reference/KTA.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Kernel-target alignment — KTA","text":"K kernel matrix (class: \"matrix\"). y target variable. numeric vector factor two levels.","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/KTA.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Kernel-target alignment — KTA","text":"Alignment value.","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/KTA.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Kernel-target alignment — KTA","text":"","code":"K1 <- RBF(iris[1:100,1:4],g=0.1) y <- factor(iris[1:100,5]) KTA(K1,y) #> Remember that Klist should contain only kernel matrices (i.e. squared, symmetric and PSD). #>   This function does NOT verify the symmetry and PSD criteria. #> [1] 0.4058431"},{"path":"https://elies-ramon.github.io/kerntools/reference/Kendall.html","id":null,"dir":"Reference","previous_headings":"","what":"Kendall's tau kernel — Kendall","title":"Kendall's tau kernel — Kendall","text":"`Kendall()` computes Kendall's tau, happens kernel function ordinal variables, ranks permutations.","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/Kendall.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Kendall's tau kernel — Kendall","text":"","code":"Kendall(X, NA.as.0 = TRUE, samples.in.rows = FALSE, comp = \"mean\")"},{"path":"https://elies-ramon.github.io/kerntools/reference/Kendall.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Kendall's tau kernel — Kendall","text":"X evaluating single ordinal feature, X numeric matrix data.frame. data multivariate, X list, ordinal/ranking feature placed different element list (see Examples). NA..0 NAs converted 0s? (Defaults: TRUE). samples..rows TRUE, samples considered rows. Otherwise, assumed columns. (Defaults: FALSE). comp X list, argument indicates ordinal/ranking variables combined. Options : \"mean\" \"sum\". (Defaults: \"mean\").","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/Kendall.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Kendall's tau kernel — Kendall","text":"Kernel matrix (dimension: NxN).","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/Kendall.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Kendall's tau kernel — Kendall","text":"Jiao, Y. Vert, J.P. Kendall Mallows kernels permutations. International Conference Machine Learning. PMLR, 2015. Link","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/Kendall.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Kendall's tau kernel — Kendall","text":"","code":"# 3 people are given a list of 10 colors. They rank them from most (1) to least # (10) favorite color_list <-  c(\"black\",\"blue\",\"green\",\"grey\",\"lightblue\",\"orange\",\"purple\", \"red\",\"white\",\"yellow\") survey1 <- 1:10 survey2 <- 10:1 survey3 <- sample(10) color <- cbind(survey1,survey2,survey3) # Samples in columns rownames(color) <- color_list Kendall(color) #>             survey1     survey2     survey3 #> survey1  1.00000000 -1.00000000 -0.02222222 #> survey2 -1.00000000  1.00000000  0.02222222 #> survey3 -0.02222222  0.02222222  1.00000000  # The same 3 people are asked the number of times they ate 5 different kinds of # food during the last month: food <- matrix(c(10, 1,18, 25,30, 7, 5,20, 5, 12, 7,20, 20, 3,22),ncol=5,nrow=3) rownames(food) <- colnames(color) colnames(food) <- c(\"spinach\", \"chicken\", \"beef\" , \"salad\",\"lentils\") # (we can observe that, for person 2, vegetables << meat, while for person 3 # is the other way around) Kendall(food,samples.in.rows=TRUE) #>         survey1 survey2 survey3 #> survey1     1.0     0.2     0.4 #> survey2     0.2     1.0    -0.4 #> survey3     0.4    -0.4     1.0  # We can combine this results: dataset <- list(color=color,food=t(food)) #All samples in columns Kendall(dataset) #> Composition: Mean #>            [,1]       [,2]       [,3] #> [1,]  1.0000000 -0.4000000  0.1888889 #> [2,] -0.4000000  1.0000000 -0.1888889 #> [3,]  0.1888889 -0.1888889  1.0000000"},{"path":"https://elies-ramon.github.io/kerntools/reference/Laplace.html","id":null,"dir":"Reference","previous_headings":"","what":"Laplacian kernel — Laplace","title":"Laplacian kernel — Laplace","text":"`Laplace()` computes laplacian kernel possible pairs rows matrix data.frame dimension NxD.","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/Laplace.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Laplacian kernel — Laplace","text":"","code":"Laplace(X, g = NULL)"},{"path":"https://elies-ramon.github.io/kerntools/reference/Laplace.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Laplacian kernel — Laplace","text":"X Matrix data.frame contains real numbers (\"integer\", \"float\" \"double\"). g Gamma hyperparameter. g=0 NULL, `Laplace()` returns Manhattan distance (L1 norm two vectors).","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/Laplace.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Laplacian kernel — Laplace","text":"Kernel matrix (dimension: NxN).","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/Laplace.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Laplacian kernel — Laplace","text":"Let \\(x_i,x_j\\) two real vectors. , laplacian kernel defined : $$K_{Lapl}(x_i,x_j)=\\exp(-\\gamma \\|x_i - x_j \\|_1)$$","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/Laplace.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Laplacian kernel — Laplace","text":"","code":"dat <- matrix(rnorm(250),ncol=50,nrow=5) Laplace(dat,g=0.1) #>             1           2           3           4           5 #> 1 1.000000000 0.005222016 0.002596732 0.013192989 0.011030047 #> 2 0.005222016 1.000000000 0.002682465 0.003775454 0.002542276 #> 3 0.002596732 0.002682465 1.000000000 0.001086852 0.002064650 #> 4 0.013192989 0.003775454 0.001086852 1.000000000 0.006260778 #> 5 0.011030047 0.002542276 0.002064650 0.006260778 1.000000000"},{"path":"https://elies-ramon.github.io/kerntools/reference/Linear.html","id":null,"dir":"Reference","previous_headings":"","what":"Linear kernel — Linear","title":"Linear kernel — Linear","text":"`Linear()` computes inner product possible pairs rows matrix data.frame dimension NxD.","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/Linear.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Linear kernel — Linear","text":"","code":"Linear(X, cos.norm = FALSE, coeff = NULL)"},{"path":"https://elies-ramon.github.io/kerntools/reference/Linear.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Linear kernel — Linear","text":"X Matrix data.frame contains real numbers (\"integer\", \"float\" \"double\"). cos.norm resulting kernel matrix cosine normalized? (Defaults: FALSE). coeff (optional) vector length D weights one features (columns). cos.norm=TRUE, `Linear()` first weighting cosine-normalization.","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/Linear.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Linear kernel — Linear","text":"Kernel matrix (dimension: NxN).","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/Linear.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Linear kernel — Linear","text":"","code":"dat <- matrix(rnorm(250),ncol=50,nrow=5) Linear(dat) #>            [,1]      [,2]      [,3]      [,4]       [,5] #> [1,] 56.3533820 -1.902548  6.634868  5.378836 -0.6782112 #> [2,] -1.9025476 52.316577  8.875222 -2.050903  5.1256125 #> [3,]  6.6348682  8.875222 57.110595 -8.071251  4.1534043 #> [4,]  5.3788362 -2.050903 -8.071251 36.896757  2.7138702 #> [5,] -0.6782112  5.125612  4.153404  2.713870 39.8589741"},{"path":"https://elies-ramon.github.io/kerntools/reference/MKC.html","id":null,"dir":"Reference","previous_headings":"","what":"Multiple Kernel (Matrices) Combination — MKC","title":"Multiple Kernel (Matrices) Combination — MKC","text":"Combination kernel matrices coming different datasets / feature types single kernel matrix.","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/MKC.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Multiple Kernel (Matrices) Combination — MKC","text":"","code":"MKC(K, coeff = NULL)"},{"path":"https://elies-ramon.github.io/kerntools/reference/MKC.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Multiple Kernel (Matrices) Combination — MKC","text":"K three-dimensional NxDxM array containing M kernel matrices. coeff vector length M weight kernel matrix. NULL, kernel matrices weight. (Defaults: NULL)","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/MKC.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Multiple Kernel (Matrices) Combination — MKC","text":"kernel matrix.","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/MKC.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Multiple Kernel (Matrices) Combination — MKC","text":"","code":"# For illustrating a possible use of this function, we work with a dataset # that contains numeric and categorical features.  summary(mtcars) #>       mpg             cyl             disp             hp        #>  Min.   :10.40   Min.   :4.000   Min.   : 71.1   Min.   : 52.0   #>  1st Qu.:15.43   1st Qu.:4.000   1st Qu.:120.8   1st Qu.: 96.5   #>  Median :19.20   Median :6.000   Median :196.3   Median :123.0   #>  Mean   :20.09   Mean   :6.188   Mean   :230.7   Mean   :146.7   #>  3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0   3rd Qu.:180.0   #>  Max.   :33.90   Max.   :8.000   Max.   :472.0   Max.   :335.0   #>       drat             wt             qsec             vs         #>  Min.   :2.760   Min.   :1.513   Min.   :14.50   Min.   :0.0000   #>  1st Qu.:3.080   1st Qu.:2.581   1st Qu.:16.89   1st Qu.:0.0000   #>  Median :3.695   Median :3.325   Median :17.71   Median :0.0000   #>  Mean   :3.597   Mean   :3.217   Mean   :17.85   Mean   :0.4375   #>  3rd Qu.:3.920   3rd Qu.:3.610   3rd Qu.:18.90   3rd Qu.:1.0000   #>  Max.   :4.930   Max.   :5.424   Max.   :22.90   Max.   :1.0000   #>        am              gear            carb       #>  Min.   :0.0000   Min.   :3.000   Min.   :1.000   #>  1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:2.000   #>  Median :0.0000   Median :4.000   Median :2.000   #>  Mean   :0.4062   Mean   :3.688   Mean   :2.812   #>  3rd Qu.:1.0000   3rd Qu.:4.000   3rd Qu.:4.000   #>  Max.   :1.0000   Max.   :5.000   Max.   :8.000   cat_feat_idx <- which(colnames(mtcars) %in% c(\"vs\", \"am\"))  # vs and am are categorical variables. We make a list, with the numeric features # in the first element and the categorical features in the second: DATA <- list(num=mtcars[,-cat_feat_idx], cat=mtcars[,cat_feat_idx]) # Our N, D and M dimensions are: N <- nrow(mtcars); D <- ncol(mtcars); M <- length(DATA)  # Now we prepare a kernel matrix: K <- array(dim=c(N,N,M)) K[,,1] <- Linear(DATA[[1]],cos.norm = TRUE) ## Kernel for numeric data K[,,2] <- Dirac(DATA[[2]]) ## Kernel for categorical data  # Here, K1 has the same weight than K2 when computing the final kernel, although # K1 has 9 variables and K2 has only 2. Kconsensus <- MKC(K) Kconsensus[1:5,1:5] #>           [,1]      [,2]      [,3]      [,4]      [,5] #> [1,] 1.0000000 0.9999976 0.7459507 0.4898233 0.7429409 #> [2,] 0.9999976 1.0000000 0.7460135 0.4898009 0.7428780 #> [3,] 0.7459507 0.7460135 1.0000000 0.7244304 0.4786498 #> [4,] 0.4898233 0.4898009 0.7244304 1.0000000 0.7489934 #> [5,] 0.7429409 0.7428780 0.4786498 0.7489934 1.0000000  # If we want to weight equally each one of the 11 variables in the final # kernel, K1 will weight 9/11 and K2 2/11. coeff <- sapply(DATA,ncol) coeff #> num cat  #>   9   2  Kweighted <- MKC(K,coeff=coeff) Kweighted[1:5,1:5] #>           [,1]      [,2]      [,3]      [,4]      [,5] #> [1,] 1.0000000 0.9999960 0.9024648 0.8015291 0.8975396 #> [2,] 0.9999960 1.0000000 0.9025676 0.8014924 0.8974368 #> [3,] 0.9024648 0.9025676 1.0000000 0.8672498 0.7832451 #> [4,] 0.8015291 0.8014924 0.8672498 1.0000000 0.9074437 #> [5,] 0.8975396 0.8974368 0.7832451 0.9074437 1.0000000"},{"path":"https://elies-ramon.github.io/kerntools/reference/Normal_CI.html","id":null,"dir":"Reference","previous_headings":"","what":"Confidence Interval using Normal Approximation — Normal_CI","title":"Confidence Interval using Normal Approximation — Normal_CI","text":"`Normal_CI()` computes Confidence Interval (CI) performance measure (instance, accuracy) using normal approximation. Thus, advisable test size , least, 30 instances.","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/Normal_CI.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Confidence Interval using Normal Approximation — Normal_CI","text":"","code":"Normal_CI(value, ntest, confidence = 95)"},{"path":"https://elies-ramon.github.io/kerntools/reference/Normal_CI.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Confidence Interval using Normal Approximation — Normal_CI","text":"value Performance value (single value). ntest Test set size (single value). confidence Confidence level; instance, 95% 99%. (Defaults: 95).","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/Normal_CI.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Confidence Interval using Normal Approximation — Normal_CI","text":"vector containing CI.","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/Normal_CI.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Confidence Interval using Normal Approximation — Normal_CI","text":"","code":"# Computing accuracy y <- c(rep(\"a\",30),rep(\"b\",20)) y_pred <- c(rep(\"a\",20),rep(\"b\",30)) ct <- table(y,y_pred) accuracy <- Acc(ct) # Computing 95%CI Normal_CI(accuracy, ntest=length(y), confidence=95) #> [1] 0.6891277 0.9108723"},{"path":"https://elies-ramon.github.io/kerntools/reference/Prec.html","id":null,"dir":"Reference","previous_headings":"","what":"Precision or PPV — Prec","title":"Precision or PPV — Prec","text":"`Prec()` computes Precision PPV (Positive Predictive Value) output classification model actual values target. precision class can aggregated. Macro-precision average precision classes. Micro-precision weighted average.","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/Prec.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Precision or PPV — Prec","text":"","code":"Prec(ct, multi.class = \"macro\")"},{"path":"https://elies-ramon.github.io/kerntools/reference/Prec.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Precision or PPV — Prec","text":"ct Confusion Matrix. multi.class results class aggregated, ? Options: \"none\", \"macro\", \"micro\". (Defaults: \"macro\").","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/Prec.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Precision or PPV — Prec","text":"PPV (single value).","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/Prec.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Precision or PPV — Prec","text":"","code":"y <- c(rep(\"a\",3),rep(\"b\",2)) y_pred <- c(rep(\"a\",2),rep(\"b\",3)) ct <- table(y,y_pred) Prec(ct) #> It is identical to weighted Accuracy #> [1] 0.8333333"},{"path":"https://elies-ramon.github.io/kerntools/reference/Procrustes.html","id":null,"dir":"Reference","previous_headings":"","what":"Procrustes Analysis — Procrustes","title":"Procrustes Analysis — Procrustes","text":"Procrustes Analysis compares two PCA/PCoA/MDS/ordination methods' projections \"removing\" translation, scaling rotation effects. Thus, compared configuration \"maximum similarity\". Samples two projections related. similarity projections X1 X2 quantified using correlation-like statistic derived symmetric Procrustes sum squared differences X1 X2.","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/Procrustes.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Procrustes Analysis — Procrustes","text":"","code":"Procrustes(X1, X2, plot = NULL, labels = NULL)"},{"path":"https://elies-ramon.github.io/kerntools/reference/Procrustes.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Procrustes Analysis — Procrustes","text":"X1 matrix data.frame containing PCA/PCoA/MDS projection. X2 second matrix data.frame containing different PCA/PCoA/MDS projection, number rows X1. plot (optional) `ggplot2` displayed. input vector integers length 2, corresponding two Principal Components displayed plot. labels (optional) vector length nrow(X1), instead, nrow(X1)+nrow(X2). name displayed next point.","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/Procrustes.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Procrustes Analysis — Procrustes","text":"list containing: * X1 (zero-centered scaled). * X2 superimposed X1 (translating, scaling rotating X2). * Procrustes correlation X1 X2. * (optional) `ggplot2` plot.","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/Procrustes.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Procrustes Analysis — Procrustes","text":"`Procrustes()` performs Procrustes Analysis equivalent `vegan::procrustes(X,Y,scale=FALSE,symmetric=TRUE)`.","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/Procrustes.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Procrustes Analysis — Procrustes","text":"","code":"data1 <- matrix(rnorm(900),ncol=30,nrow=30) data2 <- matrix(rnorm(900),ncol=30,nrow=30) pca1 <- kPCA(Linear(data1),center=TRUE) pca2 <- kPCA(Linear(data2),center=TRUE) procr <- Procrustes(pca1,pca2) # Procrustean correlation between pca1 and pca2: procr$pro.cor #> [1] 0.7147493 # With plot (first two axes): procr <- Procrustes(pca1,pca2,plot=1:2,labels=1:30) procr$plot"},{"path":"https://elies-ramon.github.io/kerntools/reference/RBF.html","id":null,"dir":"Reference","previous_headings":"","what":"Gaussian RBF (Radial Basis Function) kernel — RBF","title":"Gaussian RBF (Radial Basis Function) kernel — RBF","text":"`RBF()` computes RBF kernel possible pairs rows matrix data.frame dimension NxD.","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/RBF.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Gaussian RBF (Radial Basis Function) kernel — RBF","text":"","code":"RBF(X, g = NULL)"},{"path":"https://elies-ramon.github.io/kerntools/reference/RBF.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Gaussian RBF (Radial Basis Function) kernel — RBF","text":"X Matrix data.frame contains real numbers (\"integer\", \"float\" \"double\"). g Gamma hyperparameter. g=0 NULL, `RBF()` returns matrix squared Euclidean distances instead RBF kernel matrix.","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/RBF.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Gaussian RBF (Radial Basis Function) kernel — RBF","text":"Kernel matrix (dimension: NxN).","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/RBF.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Gaussian RBF (Radial Basis Function) kernel — RBF","text":"Let \\(x_i,x_j\\) two real vectors. , RBF kernel defined : $$K_{RBF}(x_i,x_j)=\\exp(-\\gamma \\|x_i - x_j \\|^2)$$ Sometimes RBF kernel given hyperparameter called sigma. case: \\(\\gamma = 1/\\sigma^2\\).","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/RBF.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Gaussian RBF (Radial Basis Function) kernel — RBF","text":"","code":"dat <- matrix(rnorm(250),ncol=50,nrow=5) RBF(dat,g=0.1) #>              [,1]         [,2]         [,3]         [,4]         [,5] #> [1,] 1.000000e+00 6.290053e-05 5.819938e-04 7.689408e-04 1.082033e-03 #> [2,] 6.290053e-05 1.000000e+00 1.523218e-05 1.273325e-05 2.561708e-05 #> [3,] 5.819938e-04 1.523218e-05 1.000000e+00 6.333544e-05 3.105290e-04 #> [4,] 7.689408e-04 1.273325e-05 6.333544e-05 1.000000e+00 1.805479e-05 #> [5,] 1.082033e-03 2.561708e-05 3.105290e-04 1.805479e-05 1.000000e+00"},{"path":"https://elies-ramon.github.io/kerntools/reference/Rec.html","id":null,"dir":"Reference","previous_headings":"","what":"Recall or Sensitivity or TPR — Rec","title":"Recall or Sensitivity or TPR — Rec","text":"`Rec()` computes Recall, also known Sensitivity TPR (True Positive Rate),  output classification model actual values target.","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/Rec.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Recall or Sensitivity or TPR — Rec","text":"","code":"Rec(ct, multi.class = \"macro\")"},{"path":"https://elies-ramon.github.io/kerntools/reference/Rec.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Recall or Sensitivity or TPR — Rec","text":"ct Confusion Matrix. multi.class results class aggregated, ? Options: \"none\", \"macro\", \"micro\". (Defaults: \"macro\").","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/Rec.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Recall or Sensitivity or TPR — Rec","text":"TPR (single value).","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/Rec.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Recall or Sensitivity or TPR — Rec","text":"","code":"y <- c(rep(\"a\",3),rep(\"b\",2)) y_pred <- c(rep(\"a\",2),rep(\"b\",3)) ct <- table(y,y_pred) Rec(ct) #> [1] 0.8333333"},{"path":"https://elies-ramon.github.io/kerntools/reference/Spe.html","id":null,"dir":"Reference","previous_headings":"","what":"Specificity or TNR — Spe","title":"Specificity or TNR — Spe","text":"`Spe()` computes Specificity TNR (True Negative Rate) output classification prediction model actual values target.","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/Spe.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Specificity or TNR — Spe","text":"","code":"Spe(ct, multi.class = \"macro\")"},{"path":"https://elies-ramon.github.io/kerntools/reference/Spe.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Specificity or TNR — Spe","text":"ct Confusion Matrix. multi.class results class aggregated, ? Options: \"none\", \"macro\", \"micro\". (Defaults: \"macro\").","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/Spe.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Specificity or TNR — Spe","text":"TNR (single value).","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/Spe.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Specificity or TNR — Spe","text":"","code":"y <- c(rep(\"a\",3),rep(\"b\",2)) y_pred <- c(rep(\"a\",2),rep(\"b\",3)) ct <- table(y,y_pred) Spe(ct) #> [1] 0.8333333"},{"path":"https://elies-ramon.github.io/kerntools/reference/Spectrum.html","id":null,"dir":"Reference","previous_headings":"","what":"Spectrum kernel — Spectrum","title":"Spectrum kernel — Spectrum","text":"`Spectrum()` computes basic Spectrum kernel strings. kernel computes similarity two strings counting many matching substrings length l present one.","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/Spectrum.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Spectrum kernel — Spectrum","text":"","code":"Spectrum(   x,   alphabet,   l = 1,   group.ids = NULL,   weights = NULL,   feat_space = FALSE,   cos.norm = FALSE )"},{"path":"https://elies-ramon.github.io/kerntools/reference/Spectrum.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Spectrum kernel — Spectrum","text":"x Vector strings (length N). alphabet Alphabet reference. l Length substrings. group.ids (optional) vector ids. allows compute kernel groups strings within x, instead individual strings. weights (optional) numeric vector long x. allows weight differently one strings. feat_space FALSE, kernel matrix returned. Otherwise, feature space (.e. table number times substring length l appears string) also returned (Defaults: FALSE). cos.norm resulting kernel matrix cosine normalized? (Defaults: FALSE).","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/Spectrum.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Spectrum kernel — Spectrum","text":"Kernel matrix (dimension: NxN), list kernel matrix feature space.","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/Spectrum.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Spectrum kernel — Spectrum","text":"large datasets function may slow. case, may use `stringdot()` function `kernlab` package, `spectrumKernel()` function `kebabs` package.","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/Spectrum.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Spectrum kernel — Spectrum","text":"Leslie, C., Eskin, E., Noble, W.S. spectrum kernel: string kernel SVM protein classification. Pac Symp Biocomput. 2002:564-75. PMID: 11928508. Link","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/Spectrum.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Spectrum kernel — Spectrum","text":"","code":"## Examples of alphabets. _ stands for a blank space, a gap, or the ## start or the end of sequence) NT <- c(\"A\",\"C\",\"G\",\"T\",\"_\") # DNA nucleotides AA <- c(\"A\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"K\",\"L\",\"M\",\"N\",\"P\",\"Q\",\"R\",\"S\",\"T\", \"V\",\"W\",\"Y\",\"_\") ##canonical aminoacids letters_ <- c(letters,\"_\") ## Example of data strings <- c(\"hello_world\",\"hello_word\",\"hola_mon\",\"kaixo_mundua\", \"saluton_mondo\",\"ola_mundo\", \"bonjour_le_monde\") names(strings) <- c(\"english1\",\"english_typo\",\"catalan\",\"basque\", \"esperanto\",\"galician\",\"french\") ## Computing the kernel: Spectrum(strings,alphabet=letters_,l=2) #>              english1 english_typo catalan basque esperanto galician french #> english1           10            8       0      1         0        0      0 #> english_typo        8            9       0      1         0        0      0 #> catalan             0            0       7      1         4        4      4 #> basque              1            1       1     11         2        4      2 #> esperanto           0            0       4      2        14        3      7 #> galician            0            0       4      4         3        8      2 #> french              0            0       4      2         7        2     17"},{"path":"https://elies-ramon.github.io/kerntools/reference/TSS.html","id":null,"dir":"Reference","previous_headings":"","what":"Total Sum Scaling — TSS","title":"Total Sum Scaling — TSS","text":"function transforms dataset absolute relative frequencies (row column).","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/TSS.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Total Sum Scaling — TSS","text":"","code":"TSS(X, rows = TRUE)"},{"path":"https://elies-ramon.github.io/kerntools/reference/TSS.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Total Sum Scaling — TSS","text":"X Numeric matrix data.frame size containing absolute frequencies. rows TRUE, operation done row; otherwise, done column. (Defaults: TRUE).","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/TSS.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Total Sum Scaling — TSS","text":"relative frequency matrix data.frame dimension X.","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/TSS.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Total Sum Scaling — TSS","text":"","code":"dat <- matrix(rnorm(50),ncol=5,nrow=10) TSS(dat) #It can be checked that, after scaling, the sum of each row is equal to 1. #>             [,1]       [,2]       [,3]        [,4]         [,5] #>  [1,]  0.1792313  1.7862013 -0.4447070  -0.3588527 -0.161872910 #>  [2,]  0.9857786 -0.4544859  0.9839674  -0.2878203 -0.227439758 #>  [3,]  0.5751775  0.7584452 -0.5491987  -0.1692078  0.384783829 #>  [4,]  0.2274961  0.3439082  2.8405000  -1.3568577 -1.055046643 #>  [5,]  0.2667752  0.3638267  0.1160319   0.2544706 -0.001104359 #>  [6,]  0.3659695 -0.5283007  0.6352615   0.5236386  0.003430973 #>  [7,]  0.9700962 -0.2188070  0.3686820   0.9110148 -1.030985981 #>  [8,]  0.1384819  0.3878949  0.3454320  -0.2364140  0.364605206 #>  [9,] 44.7377927 22.3769142 16.4918532 -76.7229752 -5.883584865 #> [10,]  0.4804605 -1.7435773  2.2010283   1.1689823 -1.106893868"},{"path":"https://elies-ramon.github.io/kerntools/reference/centerK.html","id":null,"dir":"Reference","previous_headings":"","what":"Centering a kernel matrix — centerK","title":"Centering a kernel matrix — centerK","text":"equivalent compute `K` centered data (.e. mean column subtracted) Feature Space.","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/centerK.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Centering a kernel matrix — centerK","text":"","code":"centerK(K)"},{"path":"https://elies-ramon.github.io/kerntools/reference/centerK.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Centering a kernel matrix — centerK","text":"K Kernel matrix (class \"matrix\").","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/centerK.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Centering a kernel matrix — centerK","text":"Centered `K` (class \"matrix\").","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/centerK.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Centering a kernel matrix — centerK","text":"","code":"dat <- matrix(rnorm(250),ncol=50,nrow=5) K <- Linear(dat) centerK(K) #>             [,1]       [,2]      [,3]        [,4]       [,5] #> [1,]  38.8555517 -15.393520 -5.259264   0.4375577 -18.640325 #> [2,] -15.3935202  48.151695 -9.891882 -17.1895187  -5.676774 #> [3,]  -5.2592643  -9.891882 25.865268  -3.3674645  -7.346657 #> [4,]   0.4375577 -17.189519 -3.367465  38.0499034 -17.930478 #> [5,] -18.6403249  -5.676774 -7.346657 -17.9304779  49.594234"},{"path":"https://elies-ramon.github.io/kerntools/reference/centerX.html","id":null,"dir":"Reference","previous_headings":"","what":"Centering a squared matrix by row or column — centerX","title":"Centering a squared matrix by row or column — centerX","text":"centers numeric matrix dimension N x N row (rows=TRUE) column  (rows=FALSE).","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/centerX.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Centering a squared matrix by row or column — centerX","text":"","code":"centerX(X, rows = TRUE)"},{"path":"https://elies-ramon.github.io/kerntools/reference/centerX.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Centering a squared matrix by row or column — centerX","text":"X Numeric matrix data.frame size. rows TRUE, operation done row; otherwise, done column. (Defaults: TRUE).","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/centerX.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Centering a squared matrix by row or column — centerX","text":"Centered X (class \"matrix\").","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/centerX.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Centering a squared matrix by row or column — centerX","text":"","code":"dat <- matrix(rnorm(25),ncol=5,nrow=5) centerX(dat) #>              [,1]       [,2]        [,3]       [,4]       [,5] #> [1,]  0.331954608 -0.3144212  1.04114063 -0.4818102 -0.5768639 #> [2,] -1.281884885  0.1001558 -0.19252746  0.9114199  0.4628367 #> [3,]  2.566438778  0.8216270  0.07588762 -0.2238742 -3.2400792 #> [4,]  0.685493509 -0.5345828  0.25920442  0.7817238 -1.1918390 #> [5,]  0.002971348  0.3466128  0.17936177  1.2822336 -1.8111796"},{"path":"https://elies-ramon.github.io/kerntools/reference/cosNorm.html","id":null,"dir":"Reference","previous_headings":"","what":"Cosine normalization of a kernel matrix — cosNorm","title":"Cosine normalization of a kernel matrix — cosNorm","text":"equivalent compute K using normalization `X/sqrt(sum(X^2))` Feature Space.","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/cosNorm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cosine normalization of a kernel matrix — cosNorm","text":"","code":"cosNorm(K)"},{"path":"https://elies-ramon.github.io/kerntools/reference/cosNorm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cosine normalization of a kernel matrix — cosNorm","text":"K Kernel matrix (class \"matrix\").","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/cosNorm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Cosine normalization of a kernel matrix — cosNorm","text":"Cosine-normalized K (class \"matrix\").","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/cosNorm.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Cosine normalization of a kernel matrix — cosNorm","text":"Ah-Pine, J. (2010). Normalized kernels similarity indices. Advances Knowledge Discovery Data Mining: 14th Pacific-Asia Conference, PAKDD 2010, Hyderabad, India, June 21-24, 2010. Proceedings. Part II 14 (pp. 362-373). Springer Berlin Heidelberg. Link","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/cosNorm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Cosine normalization of a kernel matrix — cosNorm","text":"","code":"dat <- matrix(rnorm(250),ncol=50,nrow=5) K <- Linear(dat) cosNorm(K) #>             [,1]        [,2]        [,3]        [,4]        [,5] #> [1,]  1.00000000 -0.09393048 -0.21186492 -0.04942704 -0.09977431 #> [2,] -0.09393048  1.00000000  0.07301721 -0.13194280 -0.13218538 #> [3,] -0.21186492  0.07301721  1.00000000 -0.01952813  0.28006966 #> [4,] -0.04942704 -0.13194280 -0.01952813  1.00000000 -0.06079162 #> [5,] -0.09977431 -0.13218538  0.28006966 -0.06079162  1.00000000"},{"path":"https://elies-ramon.github.io/kerntools/reference/cosnormX.html","id":null,"dir":"Reference","previous_headings":"","what":"Cosine normalization of a matrix — cosnormX","title":"Cosine normalization of a matrix — cosnormX","text":"Normalizes numeric matrix dividing row (rows=TRUE) column (rows=FALSE) L2 norm. Thus, row (column) unit norm.","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/cosnormX.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cosine normalization of a matrix — cosnormX","text":"","code":"cosnormX(X, rows = TRUE)"},{"path":"https://elies-ramon.github.io/kerntools/reference/cosnormX.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cosine normalization of a matrix — cosnormX","text":"X Numeric matrix data.frame size. rows TRUE, operation done row; otherwise, done column. (Defaults: TRUE).","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/cosnormX.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Cosine normalization of a matrix — cosnormX","text":"Cosine-normalized X.","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/cosnormX.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Cosine normalization of a matrix — cosnormX","text":"","code":"dat <- matrix(rnorm(50),ncol=5,nrow=10) cosnormX(dat) #>                [,1]        [,2]        [,3]         [,4]        [,5] #>  [1,] -1.862894e-01 -0.30809428 -0.58164128  0.707277006  0.17840076 #>  [2,]  1.761182e-01  0.77365469 -0.47950849  0.374104606 -0.02362507 #>  [3,] -1.317544e-02  0.92501733 -0.32979123 -0.179326581  0.05700053 #>  [4,] -3.813744e-01 -0.06494686 -0.15870194  0.895916293  0.14994379 #>  [5,]  4.133313e-03 -0.32203422 -0.82245328 -0.434568702 -0.17606113 #>  [6,]  2.420227e-01 -0.16326397  0.46996629  0.369146113  0.74674812 #>  [7,] -1.242098e-02  0.53559500 -0.62098790  0.009702709 -0.57206958 #>  [8,] -8.802303e-01 -0.33924432  0.02350475 -0.241192483 -0.22667519 #>  [9,]  7.880297e-05  0.36295155  0.72182910  0.587940556  0.03943124 #> [10,]  2.076936e-01 -0.19759876 -0.35527559 -0.788907563  0.41136627"},{"path":"https://elies-ramon.github.io/kerntools/reference/desparsify.html","id":null,"dir":"Reference","previous_headings":"","what":"This function deletes those columns and/or rows in a matrix/data.frame that only contain 0s. — desparsify","title":"This function deletes those columns and/or rows in a matrix/data.frame that only contain 0s. — desparsify","text":"function deletes columns /rows matrix/data.frame contain 0s.","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/desparsify.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"This function deletes those columns and/or rows in a matrix/data.frame that only contain 0s. — desparsify","text":"","code":"desparsify(X, dim = 2)"},{"path":"https://elies-ramon.github.io/kerntools/reference/desparsify.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"This function deletes those columns and/or rows in a matrix/data.frame that only contain 0s. — desparsify","text":"X Numeric matrix data.frame size. dim numeric vector. 1 indicates function applied rows, 2 columns, c(1, 2) indicates rows columns. (Defaults: 2).","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/desparsify.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"This function deletes those columns and/or rows in a matrix/data.frame that only contain 0s. — desparsify","text":"X less rows columns. (Class: X).","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/desparsify.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"This function deletes those columns and/or rows in a matrix/data.frame that only contain 0s. — desparsify","text":"","code":"dat <- matrix(rnorm(150),ncol=50,nrow=30) dat[c(2,6,12),] <- 0 dat[,c(30,40,50)] <- 0 dim(desparsify(dat)) #> [1] 30 47 dim(desparsify(dat,dim=c(1,2))) #> [1] 27 47"},{"path":"https://elies-ramon.github.io/kerntools/reference/dummy_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert categorical data to dummies. — dummy_data","title":"Convert categorical data to dummies. — dummy_data","text":"Given matrix data.frame containing character/factors, function performs one-hot-encoding.","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/dummy_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert categorical data to dummies. — dummy_data","text":"","code":"dummy_data(X, lev = NULL)"},{"path":"https://elies-ramon.github.io/kerntools/reference/dummy_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert categorical data to dummies. — dummy_data","text":"X matrix, data.frame containing factors. (columns class, coerced factors anyway). lev (optional) vector categories (\"levels\") factor.","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/dummy_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert categorical data to dummies. — dummy_data","text":"X (class: \"matrix\") performing one-hot-encoding.","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/dummy_data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert categorical data to dummies. — dummy_data","text":"","code":"summary(CO2) #>      Plant             Type         Treatment       conc          uptake      #>  Qn1    : 7   Quebec     :42   nonchilled:42   Min.   :  95   Min.   : 7.70   #>  Qn2    : 7   Mississippi:42   chilled   :42   1st Qu.: 175   1st Qu.:17.90   #>  Qn3    : 7                                    Median : 350   Median :28.30   #>  Qc1    : 7                                    Mean   : 435   Mean   :27.21   #>  Qc3    : 7                                    3rd Qu.: 675   3rd Qu.:37.12   #>  Qc2    : 7                                    Max.   :1000   Max.   :45.50   #>  (Other):42                                                                   CO2_dummy <- dummy_data(CO2[,1:3],lev=dummy_var(CO2[,1:3])) CO2_dummy[1:10,1:5] #>    Plant_Qn1 Plant_Qn2 Plant_Qn3 Plant_Qc1 Plant_Qc3 #> 1          1         0         0         0         0 #> 2          1         0         0         0         0 #> 3          1         0         0         0         0 #> 4          1         0         0         0         0 #> 5          1         0         0         0         0 #> 6          1         0         0         0         0 #> 7          1         0         0         0         0 #> 8          0         1         0         0         0 #> 9          0         1         0         0         0 #> 10         0         1         0         0         0"},{"path":"https://elies-ramon.github.io/kerntools/reference/dummy_var.html","id":null,"dir":"Reference","previous_headings":"","what":"Levels per factor variable — dummy_var","title":"Levels per factor variable — dummy_var","text":"function gives categories (\"levels\") per categorical variable (\"factor\").","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/dummy_var.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Levels per factor variable — dummy_var","text":"","code":"dummy_var(X)"},{"path":"https://elies-ramon.github.io/kerntools/reference/dummy_var.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Levels per factor variable — dummy_var","text":"X matrix, data.frame containing factors. (columns class, coerced factors anyway).","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/dummy_var.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Levels per factor variable — dummy_var","text":"list levels.","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/dummy_var.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Levels per factor variable — dummy_var","text":"","code":"summary(showdata) #>    Favorite.color             Favorite.actress        Favorite.actor #>  blue     :20     Sophie Turner       :22      Peter Dinklage:20     #>  red      :20     Emilia Clarke       :19      Kit Harington :17     #>  black    :14     Anya Chalotra       :12      Henry Cavill  :15     #>  purple   :12     Freya Allan         :10      Lee Jung-jae  :15     #>  green    : 8     Helena Bonham Carter: 8      Hyun Bin      : 8     #>  lightblue: 8     Lorraine Ashbourne  : 7      Josh O'Connor : 7     #>  (Other)  :18     (Other)             :22      (Other)       :18     #>           Favorite.show Liked.new.show #>  Game of Thrones :22    No :48         #>  The witcher     :17    Yes:52         #>  Bridgerton      :14                   #>  Squid game      :11                   #>  The crown       :11                   #>  La casa de papel: 8                   #>  (Other)         :17                   dummy_var(showdata) #> $Favorite.color #>  [1] \"black\"     \"blue\"      \"green\"     \"grey\"      \"lightblue\" \"orange\"    #>  [7] \"purple\"    \"red\"       \"white\"     \"yellow\"    #>  #> $Favorite.actress #>  [1] \"Alba Flores\"          \"Anya Chalotra\"        \"Diana Rigg\"           #>  [4] \"Elisabet Casanovas\"   \"Emilia Clarke\"        \"Freya Allan\"          #>  [7] \"Helena Bonham Carter\" \"Lorraine Ashbourne\"   \"Mj Rodriguez\"         #> [10] \"Soo Ye-jin\"           \"Sophie Turner\"        #>  #> $Favorite.actor #>  [1] \"Alvaro Morte\"       \"David Solans\"       \"Francesc Orella\"    #>  [4] \"Henry Cavill\"       \"Hyun Bin\"           \"Jonathan Bailey\"    #>  [7] \"Josh O'Connor\"      \"Kit Harington\"      \"Lee Jung-jae\"       #> [10] \"Michael K Williams\" \"Peter Dinklage\"     #>  #> $Favorite.show #> [1] \"Bridgerton\"       \"Game of Thrones\"  \"La casa de papel\" \"Merli\"            #> [5] \"Pose\"             \"Squid game\"       \"The crown\"        \"The wire\"         #> [9] \"The witcher\"      #>  #> $Liked.new.show #> [1] \"No\"  \"Yes\" #>"},{"path":"https://elies-ramon.github.io/kerntools/reference/estimate_gamma.html","id":null,"dir":"Reference","previous_headings":"","what":"Gamma hyperparameter estimation (RBF kernel) — estimate_gamma","title":"Gamma hyperparameter estimation (RBF kernel) — estimate_gamma","text":"function returns estimation optimum value gamma hyperparameter (required RBF kernel function) using different heuristics: D criterion returns inverse number features X. Scale criterion returns inverse number features,   normalized total variance X. Quantiles criterion range values, computed function   `kernlab::sigest()`.","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/estimate_gamma.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Gamma hyperparameter estimation (RBF kernel) — estimate_gamma","text":"","code":"estimate_gamma(X)"},{"path":"https://elies-ramon.github.io/kerntools/reference/estimate_gamma.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Gamma hyperparameter estimation (RBF kernel) — estimate_gamma","text":"X Matrix data.frame contains real numbers (\"integer\", \"float\" \"double\").","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/estimate_gamma.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Gamma hyperparameter estimation (RBF kernel) — estimate_gamma","text":"list gamma value estimation according different criteria.","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/estimate_gamma.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Gamma hyperparameter estimation (RBF kernel) — estimate_gamma","text":"","code":"data <- matrix(rnorm(150),ncol=50,nrow=30) gamma <- estimate_gamma(data) gamma #> $d_criterion #> [1] 0.02 #>  #> $scale_criterion #> [1] 0.02303655 #>  #> $quantiles_criterion #>         90%         50%         10%  #> 0.006738926 0.010638543 0.026341814  #>  K <- RBF(data, g = gamma$scale_criterion) K[1:5,1:5] #>            [,1]      [,2]      [,3]        [,4]        [,5] #> [1,] 1.00000000 0.1038933 0.1121367 0.031411920 0.019236010 #> [2,] 0.10389328 1.0000000 0.2662183 0.150640220 0.292283136 #> [3,] 0.11213673 0.2662183 1.0000000 0.280657307 0.111923228 #> [4,] 0.03141192 0.1506402 0.2806573 1.000000000 0.009912131 #> [5,] 0.01923601 0.2922831 0.1119232 0.009912131 1.000000000"},{"path":"https://elies-ramon.github.io/kerntools/reference/frobNorm.html","id":null,"dir":"Reference","previous_headings":"","what":"Frobenius normalization — frobNorm","title":"Frobenius normalization — frobNorm","text":"function computes Frobenius normalization matrix.","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/frobNorm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Frobenius normalization — frobNorm","text":"","code":"frobNorm(X)"},{"path":"https://elies-ramon.github.io/kerntools/reference/frobNorm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Frobenius normalization — frobNorm","text":"X Numeric matrix size. may kernel matrix.","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/frobNorm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Frobenius normalization — frobNorm","text":"Frobenius-normalized X (class: \"matrix\").","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/frobNorm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Frobenius normalization — frobNorm","text":"","code":"dat <- matrix(rnorm(50),ncol=5,nrow=10) frobNorm(dat) #>              [,1]        [,2]        [,3]        [,4]         [,5] #>  [1,] -0.17950115  0.09741912 -0.12926209  0.11081521  0.023787870 #>  [2,]  0.05972679  0.12270525 -0.11045850  0.23828370  0.196333575 #>  [3,] -0.02348416  0.11438400  0.07598117  0.02074774 -0.131957336 #>  [4,] -0.10635057 -0.07725894  0.04294729  0.14483213 -0.145851488 #>  [5,]  0.04735028 -0.07426241  0.13850323  0.12015496 -0.006378478 #>  [6,] -0.12042047  0.38378659  0.06563148 -0.17061251  0.051396769 #>  [7,]  0.12275937 -0.22208435  0.01782928  0.20106857 -0.080136333 #>  [8,]  0.01102490  0.08081502 -0.28671392 -0.04038845  0.070205573 #>  [9,]  0.15868728 -0.01163804  0.08361512  0.20052225 -0.173707749 #> [10,] -0.01934217  0.02086693 -0.20604122  0.33012838 -0.052663055"},{"path":"https://elies-ramon.github.io/kerntools/reference/heatK.html","id":null,"dir":"Reference","previous_headings":"","what":"Kernel matrix heatmap — heatK","title":"Kernel matrix heatmap — heatK","text":"`heatK()` plots heatmap kernel matrix.","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/heatK.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Kernel matrix heatmap — heatK","text":"","code":"heatK(   K,   cos.norm = FALSE,   title = NULL,   color = c(\"red\", \"yellow\"),   raster = FALSE )"},{"path":"https://elies-ramon.github.io/kerntools/reference/heatK.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Kernel matrix heatmap — heatK","text":"K Kernel matrix (class \"matrix\"). cos.norm TRUE, cosine normalization applied kernel matrix elements maximum value 1. (Defaults: FALSE). title Heatmap title (optional). color vector length 2 containing two colors. first color used represent minimum value second maximum value kernel matrix. raster large kernel matrices, raster = TRUE draw quicker better-looking heatmaps. (Defaults=FALSE).","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/heatK.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Kernel matrix heatmap — heatK","text":"`ggplot2` heatmap.","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/heatK.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Kernel matrix heatmap — heatK","text":"","code":"data <- matrix(rnorm(150),ncol=50,nrow=30) K <- Linear(data) heatK(K)"},{"path":"https://elies-ramon.github.io/kerntools/reference/histK.html","id":null,"dir":"Reference","previous_headings":"","what":"Kernel matrix histogram — histK","title":"Kernel matrix histogram — histK","text":"`histK()` plots histogram kernel matrix.","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/histK.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Kernel matrix histogram — histK","text":"","code":"histK(K, main = \"Histogram of K\", vn = FALSE, ...)"},{"path":"https://elies-ramon.github.io/kerntools/reference/histK.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Kernel matrix histogram — histK","text":"K Kernel matrix (class \"matrix\"). main Plot title. vn TRUE, value von Neumann entropy shown plot. (Defaults: FALSE). ... arguments graphical parameters passed `plot.histogram`.","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/histK.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Kernel matrix histogram — histK","text":"object class \"histogram\".","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/histK.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Kernel matrix histogram — histK","text":"Information von Neumann entropy can found '?vonNeumann()'.","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/histK.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Kernel matrix histogram — histK","text":"","code":"data <- matrix(rnorm(150),ncol=50,nrow=30) K <- RBF(data,g=0.01) histK(K)"},{"path":"https://elies-ramon.github.io/kerntools/reference/kPCA.html","id":null,"dir":"Reference","previous_headings":"","what":"Kernel PCA — kPCA","title":"Kernel PCA — kPCA","text":"`kPCA()` computes kernel PCA kernel matrix , desired, produces plot. contribution original variables Principal Components (PCs), sometimes referred \"loadings\", returned (, go `kPCA_imp()`).","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/kPCA.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Kernel PCA — kPCA","text":"","code":"kPCA(   K,   center = TRUE,   Ktest = NULL,   plot = NULL,   y = NULL,   colors = \"black\",   na_col = \"grey70\",   title = \"Kernel PCA\",   pos_leg = \"right\",   name_leg = \"\",   labels = NULL,   ellipse = NULL )"},{"path":"https://elies-ramon.github.io/kerntools/reference/kPCA.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Kernel PCA — kPCA","text":"K Kernel matrix (class \"matrix\"). center logical value. TRUE, variables zero-centered PCA. (Defaults: TRUE). Ktest (optional) additional kernel matrix corresponding test samples, dimension Ntest x Ntraining. new samples projected (using color defined `na_col`) kernel PCA computed K. Remember data generated `Ktest` centered beforehand, using values used centering `K`. plot (optional) `ggplot2` displayed. input vector integers length 2, corresponding two Principal Components displayed plot. y (optional) factor, numeric vector, length equal `nrow(K)` (number samples). parameter allows paint points different colors. colors single color, vector colors. `y` numeric, gradient colors first second entry used paint points. (Defaults: \"black\"). na_col Color entries NA parameter `y`, entries corresponding `Ktest` (`Ktest` NULL). Otherwise, parameter ignored. title Plot title. pos_leg Position legend. name_leg Title legend. (Defaults: blank) labels (optional) vector length nrow(K). name displayed next point. ellipse (optional) float 0 1. ellipse drawn group points defined `y`. `y` class \"factor.\" parameter indicate spread ellipse.","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/kPCA.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Kernel PCA — kPCA","text":"list two objects: * PCA projection (class \"matrix\"). Please note K computed NxD table N > D, first N-D PCs may useful. * (optional) `ggplot2` plot selected PCs.","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/kPCA.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Kernel PCA — kPCA","text":"ordinary PCA, kernel PCA can used summarize, visualize /create new features dataset. Data can projected linear nonlinear way, depending kernel used. kernel `Linear()`, kernel PCA equivalent ordinary PCA.","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/kPCA.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Kernel PCA — kPCA","text":"","code":"dat <- matrix(rnorm(150),ncol=50,nrow=30) K <- Linear(dat)  ## Projection's coordinates only: pca <- kPCA(K)  ## Coordinates + plot of the two first principal components (PC1 and PC2): pca <- kPCA(K,plot=1:2, colors = \"coral2\") pca$plot"},{"path":"https://elies-ramon.github.io/kerntools/reference/kPCA_arrows.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot the original variables' contribution to a PCA plot — kPCA_arrows","title":"Plot the original variables' contribution to a PCA plot — kPCA_arrows","text":"`kPCA_arrows()` draws arrows (kernel) PCA plot represent contribution original variables two displayed Principal Components (PCs).","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/kPCA_arrows.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot the original variables' contribution to a PCA plot — kPCA_arrows","text":"","code":"kPCA_arrows(plot, contributions, colour = \"steelblue\", size = 4, ...)"},{"path":"https://elies-ramon.github.io/kerntools/reference/kPCA_arrows.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot the original variables' contribution to a PCA plot — kPCA_arrows","text":"plot kernel PCA plot generated `kPCA()`. contributions variables contributions, instance obtained via `kPCA_imp()`. mandatory draw original variables; subset interest can passed argument. colour Color arrows labels. (Defaults: \"steelblue\"). size Size labels. (Defaults: 4). ... Additional parameters passed geom_segments() geom_text().","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/kPCA_arrows.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot the original variables' contribution to a PCA plot — kPCA_arrows","text":"PCA plot arrows (`ggplot2` object).","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/kPCA_arrows.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Plot the original variables' contribution to a PCA plot — kPCA_arrows","text":"important note arrows scaled match samples' projection plot. Thus, arrows' directions correct, expect magnitudes match output `kPCA_imp()` functions(`prcomp`, `princomp...`). (Nevertheless, least proportional real magnitudes.)","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/kPCA_arrows.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot the original variables' contribution to a PCA plot — kPCA_arrows","text":"","code":"dat <- matrix(rnorm(500),ncol=10,nrow=50) K <- Linear(dat)  ## Computing the kernel PCA. The plot represents PC1 and PC2: kpca <- kPCA(K,plot=1:2)  ## Computing the contributions to all the PCS: pcs <- kPCA_imp(dat,secure=FALSE) #> Do not use this function if the PCA was created with the RBF, #>           Laplacian, Bray-Curtis, Jaccard/Ruzicka, or Kendall's tau kernels  ## We will draw the arrows for PC1 and PC2. contributions <- t(pcs$loadings[1:2,]) rownames(contributions) <- 1:10 kPCA_arrows(plot=kpca$plot,contributions=contributions)"},{"path":"https://elies-ramon.github.io/kerntools/reference/kPCA_imp.html","id":null,"dir":"Reference","previous_headings":"","what":"Contributions of the variables to the Principal Components (","title":"Contributions of the variables to the Principal Components (","text":"`kPCA_imp()` performs PCA kernel PCA simultaneously returns contributions variables Principal Components (sometimes, contributions called \"loadings\") Feature Space. Optionally, can also return samples' projection (cropped relevant PCs) values used centering variables Feature Space. return plot, projects test data. , please use `kPCA()`.","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/kPCA_imp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Contributions of the variables to the Principal Components (","text":"","code":"kPCA_imp(DATA, center = TRUE, projected = NULL, secure = FALSE)"},{"path":"https://elies-ramon.github.io/kerntools/reference/kPCA_imp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Contributions of the variables to the Principal Components (","text":"DATA matrix data.frame (kernel matrix) containing data feature space. Please note nrow(DATA) higher ncol(DATA). Linear kernel used, feature space simply original space. center logical value. TRUE, variables zero-centered. (Defaults: TRUE). projected (optional) desired, PCA projection (generated, example, `kPCA()`) can included. DATA big (especially number rows) may save computation time. secure (optional) TRUE, tests quality loadings may slow. (Defaults: FALSE).","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/kPCA_imp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Contributions of the variables to the Principal Components (","text":"list three objects: * PCA projection (class \"matrix\") using relevant Principal Components. * loadings. * values used center variable Feature Space.","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/kPCA_imp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Contributions of the variables to the Principal Components (","text":"function may valid kernels. use RBF, Laplacian, Bray-Curtis, Jaccard/Ruzicka, Kendall's tau kernels unless know exactly .","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/kPCA_imp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Contributions of the variables to the Principal Components (","text":"","code":"dat <- matrix(rnorm(150),ncol=30,nrow=50) contributions <- kPCA_imp(dat) #> Do not use this function if the PCA was created with the RBF, #>           Laplacian, Bray-Curtis, Jaccard/Ruzicka, or Kendall's tau kernels contributions$loadings[c(\"PC1\",\"PC2\"),1:5] #>           [,1]        [,2]        [,3]       [,4]        [,5] #> PC1 0.30133440  0.06199908  0.07316892 0.30133440  0.06199908 #> PC2 0.08032387 -0.29497520 -0.08085609 0.08032387 -0.29497520"},{"path":"https://elies-ramon.github.io/kerntools/reference/kerntools-package.html","id":null,"dir":"Reference","previous_headings":"","what":"kerntools: Kernel Functions and Tools for Machine Learning Applications — kerntools-package","title":"kerntools: Kernel Functions and Tools for Machine Learning Applications — kerntools-package","text":"Kernel functions diverse types data (including, restricted : nonnegative real vectors, real matrices, categorical ordinal variables, sets, strings), plus utilities like kernel similarity, kernel Principal Components Analysis (PCA) features' importance Support Vector Machines (SVMs), expand 'R' packages like 'kernlab'.","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/kerntools-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"kerntools: Kernel Functions and Tools for Machine Learning Applications — kerntools-package","text":"Maintainer: Elies Ramon eramon@everlyrusher.com (ORCID) [copyright holder]","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/minmax.html","id":null,"dir":"Reference","previous_headings":"","what":"Minmax normalization — minmax","title":"Minmax normalization — minmax","text":"Minmax normalization. Custom min/max values may passed function.","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/minmax.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Minmax normalization — minmax","text":"","code":"minmax(X, rows = FALSE, values = NULL)"},{"path":"https://elies-ramon.github.io/kerntools/reference/minmax.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Minmax normalization — minmax","text":"X Numeric matrix data.frame size. rows TRUE, minmax normalization done row; otherwise, done column. (Defaults: FALSE) values (optional) list containing two elements, \"max\" values \"min\" values. value passed, typical minmax normalization (normalizes dataset 0 1) computed observed maximum minimum value column (row) X.","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/minmax.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Minmax normalization — minmax","text":"Minmax-normalized X.","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/minmax.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Minmax normalization — minmax","text":"","code":"dat <- matrix(rnorm(100),ncol=10,nrow=10) dat_minmax <- minmax(dat) apply(dat_minmax,2,min) ## Min values = 0 #>  [1] 0 0 0 0 0 0 0 0 0 0 apply(dat_minmax,2,max) ## Max values = 1 #>  [1] 1 1 1 1 1 1 1 1 1 1 # We can also explicitly state the max and min values: values <- list(min=apply(dat,2,min),max=apply(dat,2,max)) dat_minmax <- minmax(dat,values=values)"},{"path":"https://elies-ramon.github.io/kerntools/reference/nmse.html","id":null,"dir":"Reference","previous_headings":"","what":"NMSE (Normalized Mean Squared Error) — nmse","title":"NMSE (Normalized Mean Squared Error) — nmse","text":"`nmse()` computes Normalized Mean Squared Error output regression model actual values target.","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/nmse.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"NMSE (Normalized Mean Squared Error) — nmse","text":"","code":"nmse(target, pred)"},{"path":"https://elies-ramon.github.io/kerntools/reference/nmse.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"NMSE (Normalized Mean Squared Error) — nmse","text":"target Numeric vector containing actual values. pred Numeric vector containing predicted values. (order target)","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/nmse.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"NMSE (Normalized Mean Squared Error) — nmse","text":"normalized mean squared error (single value).","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/nmse.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"NMSE (Normalized Mean Squared Error) — nmse","text":"Normalized Mean Squared error defined : $$NMSE=MSE/((N-1)*var(target))$$ MSE Mean Squared Error.","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/nmse.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"NMSE (Normalized Mean Squared Error) — nmse","text":"","code":"y <- 1:10 y_pred <- y+rnorm(10) nmse(y,y_pred) #> [1] 0.05025904"},{"path":"https://elies-ramon.github.io/kerntools/reference/plotImp.html","id":null,"dir":"Reference","previous_headings":"","what":"Importance barplot — plotImp","title":"Importance barplot — plotImp","text":"`plotImp()` displays barplot numeric vector, assumed contain features importance (prediction model) contribution original variable Principal Component (PCA). barplot, features/PCs sorted decreasing importance.","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/plotImp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Importance barplot — plotImp","text":"","code":"plotImp(   x,   y = NULL,   relative = TRUE,   absolute = TRUE,   nfeat = NULL,   names = NULL,   main = NULL,   xlim = NULL,   color = \"grey\",   leftmargin = NULL,   ylegend = NULL,   leg_pos = \"right\",   ... )"},{"path":"https://elies-ramon.github.io/kerntools/reference/plotImp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Importance barplot — plotImp","text":"x Numeric vector containing importances. y (optional) Numeric vector containing different, independent variable contrasted feature importances. length order `x`. relative TRUE, barplot display relative importances. (Defaults: TRUE). absolute FALSE, bars may positive negative, affect order features Otherwise, absolute value `x` taken (Defaults: TRUE). nfeat (optional) number top (important) features displayed plot. names (optional) names features, order `x`. main (optional) Plot title. xlim (optional) numeric vector. absent, minimum maximum value `x` used establish axis' range. color Color(s) chosen bars. single value vector. (Defaults: \"grey\"). leftmargin (optional) Left margin space plot. ylegend (optional) allows add text explaining `y` (`y` NULL). leg_pos `ylegend` TRUE, position legend. (Defaults: \"right\"). ... (optional) Additional arguments (`axes`, `asp`,...) graphical parameters (`par`). See `?graphics::barplot()`.","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/plotImp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Importance barplot — plotImp","text":"list containing: * vector importances decreasing order. `nfeat` NULL, top `nfeat` returned. * cumulative sum (absolute) importances. * numeric vector giving coordinates drawn bars' midpoints.","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/plotImp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Importance barplot — plotImp","text":"","code":"importances <- rnorm(30) names_imp <- paste0(\"Feat\",1:length(importances))  plot1 <- plotImp(x=importances,names=names_imp,main=\"Barplot\")  plot2 <- plotImp(x=importances,names=names_imp,relative=FALSE, main=\"Barplot\",nfeat=10)  plot3 <- plotImp(x=importances,names=names_imp,absolute=FALSE, main=\"Barplot\",color=\"coral2\")"},{"path":"https://elies-ramon.github.io/kerntools/reference/showdata.html","id":null,"dir":"Reference","previous_headings":"","what":"Showdata — showdata","title":"Showdata — showdata","text":"toy dataset contains results (fictional) survey commissioned well-known streaming platform. platform invited 100 people watch footage new show premiere. , participants asked pick favorite color, actress, actors shows list. Finally, asked disclose liked new show.","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/showdata.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Showdata — showdata","text":"","code":"showdata"},{"path":"https://elies-ramon.github.io/kerntools/reference/showdata.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Showdata — showdata","text":"data.frame 100 rows 5 factor variables: Favorite.color Favorite color Favorite.actress Favorite actress Favorite.actor Favorite actor Favorite.show Favorite show Liked.new.show like new show?","code":""},{"path":[]},{"path":"https://elies-ramon.github.io/kerntools/reference/simK.html","id":null,"dir":"Reference","previous_headings":"","what":"Kernel matrix similarity — simK","title":"Kernel matrix similarity — simK","text":"`simK()` computes similarity kernel matrices.","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/simK.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Kernel matrix similarity — simK","text":"","code":"simK(Klist)"},{"path":"https://elies-ramon.github.io/kerntools/reference/simK.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Kernel matrix similarity — simK","text":"Klist list M kernel matrices identical NxN dimension.","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/simK.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Kernel matrix similarity — simK","text":"Kernel matrix (dimension: MxM).","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/simK.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Kernel matrix similarity — simK","text":"wrapper `Frobenius()`.","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/simK.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Kernel matrix similarity — simK","text":"","code":"K1 <- Linear(matrix(rnorm(7500),ncol=150,nrow=50)) K2 <- Linear(matrix(rnorm(7500),ncol=150,nrow=50)) K3 <- Linear(matrix(rnorm(7500),ncol=150,nrow=50))  simK(list(K1,K2,K3)) #> Remember that Klist should contain only kernel matrices (i.e. squared, symmetric and PSD). #>   This function does NOT verify the symmetry and PSD criteria. #>           [,1]      [,2]      [,3] #> [1,] 1.0000000 0.7477411 0.7472335 #> [2,] 0.7477411 1.0000000 0.7422804 #> [3,] 0.7472335 0.7422804 1.0000000"},{"path":"https://elies-ramon.github.io/kerntools/reference/svm_imp.html","id":null,"dir":"Reference","previous_headings":"","what":"SVM feature importance — svm_imp","title":"SVM feature importance — svm_imp","text":"Recovering features importances SVM model.","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/svm_imp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"SVM feature importance — svm_imp","text":"","code":"svm_imp(   X,   svindx,   coeff,   result = \"absolute\",   cos.norm = FALSE,   center = FALSE,   scale = FALSE )"},{"path":"https://elies-ramon.github.io/kerntools/reference/svm_imp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"SVM feature importance — svm_imp","text":"X Matrix data.frame contains real numbers (\"integer\", \"float\" \"double\"). X kernel matrix, original dataset used compute kernel matrix. svindx Indices support vectors. coeff target * alpha. result string. \"absolute\", absolute values importances returned. \"squared\", squared values returned. input result original (positive /negative) importance values (see Details). (Defaults: \"absolute\"). cos.norm Boolean. data cosine normalized prior training model? (Defaults: FALSE). center Boolean. data centered prior training model? (Defaults: FALSE). scale Boolean. data scaled prior training model? (Defaults: FALSE).","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/svm_imp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"SVM feature importance — svm_imp","text":"importance feature (vector).","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/svm_imp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"SVM feature importance — svm_imp","text":"function may valid kernels. use RBF, Laplacian, Bray-Curtis, Jaccard/Ruzicka, Kendall's tau kernels unless know exactly . Usually sign importances irrelevant, thus justifying working absolute squared values; see instance Guyon et al. (2002). classification tasks exception , can demonstrated feature space strictly nonnegative. case, positive importance implies feature  contributes \"positive\" class, negative importance  \"negative\" class.","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/svm_imp.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"SVM feature importance — svm_imp","text":"Guyon, ., Weston, J., Barnhill, S., Vapnik, V. (2002) Gene selection cancer classification using support vector machines. Machine learning, 46, 389-422. Link","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/svm_imp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"SVM feature importance — svm_imp","text":"","code":"data1 <- iris[1:100,] sv_index <- c( 24, 42, 58, 99) coefficients <- c(-0.2670988, -0.3582848,  0.2129282,  0.4124554) # This SV and coefficients were obtained from a model generated with kernlab: # model <- kernlab::ksvm(Species ~ .,data=data1, kernel=\"vanilladot\",scaled = TRUE) # sv_index <- unlist(kernlab::alphaindex(model)) # coefficients <- kernlab::unlist(coef(model)) # Now we compute the importances: svm_imp(X=data1[,-5],svindx=sv_index,coeff=coefficients,center=TRUE,scale=TRUE) #> Do not use this function if the SVM model was created with the RBF, #>           Laplacian, Bray-Curtis, Jaccard/Ruzicka, or Kendall's tau kernels #> Sepal.Length  Sepal.Width Petal.Length  Petal.Width  #>    0.2686391    0.3411357    0.7037992    0.7530602"},{"path":"https://elies-ramon.github.io/kerntools/reference/vonNeumann.html","id":null,"dir":"Reference","previous_headings":"","what":"Von Neumann entropy — vonNeumann","title":"Von Neumann entropy — vonNeumann","text":"`vonNeumann()` computes von Neumann entropy kernel matrix. Entropy values close 0 indicate elements similar, may result underfitting training prediction model. Instead, values close 1 indicate high variability may produce overfitting.","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/vonNeumann.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Von Neumann entropy — vonNeumann","text":"","code":"vonNeumann(K)"},{"path":"https://elies-ramon.github.io/kerntools/reference/vonNeumann.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Von Neumann entropy — vonNeumann","text":"K Kernel matrix (class \"matrix\").","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/vonNeumann.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Von Neumann entropy — vonNeumann","text":"Von Neumann entropy (single value).","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/vonNeumann.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Von Neumann entropy — vonNeumann","text":"Belanche-Muñoz, L.. Wiejacha, M. (2023) Analysis Kernel Matrices via von Neumann Entropy Relation RVM Performances. Entropy, 25, 154. doi:10.3390/e25010154. Link","code":""},{"path":"https://elies-ramon.github.io/kerntools/reference/vonNeumann.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Von Neumann entropy — vonNeumann","text":"","code":"data <- matrix(rnorm(150),ncol=50,nrow=30) K <- Linear(data) vonNeumann(K) #> [1] 0.4478621"},{"path":"https://elies-ramon.github.io/kerntools/news/index.html","id":"kerntools-102","dir":"Changelog","previous_headings":"","what":"kerntools 1.0.2","title":"kerntools 1.0.2","text":"CRAN release: 2024-09-04 Fixed errors arose CRAN Package Check using alternative BLAS/LAPACK implementations.","code":""},{"path":"https://elies-ramon.github.io/kerntools/news/index.html","id":"kerntools-101","dir":"Changelog","previous_headings":"","what":"kerntools 1.0.1","title":"kerntools 1.0.1","text":"CRAN release: 2024-08-27 Fixed typos DESCRIPTION file ‘Acc()’ function.","code":""},{"path":"https://elies-ramon.github.io/kerntools/news/index.html","id":"kerntools-100","dir":"Changelog","previous_headings":"","what":"kerntools 1.0.0","title":"kerntools 1.0.0","text":"Initial CRAN submission.","code":""}]
